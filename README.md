# Daily Papers - DoA Estimation
Automatically fetches the latest arXiv papers on DoA estimation.

Last update: 2025-12-23

## Array DOA estimation
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Partition Function Estimation Using Analog Quantum Processors](https://arxiv.org/abs/2512.19685v1)** | 2025-12-22 | <details><summary>Show</summary><p>We evaluate using programmable superconducting flux qubit D-Wave quantum annealers to approximate the partition function of Ising models. We propose the use of two distinct quantum annealer sampling methods: chains of Monte Carlo-like reverse quantum anneals, and standard linear-ramp quantum annealing. The control parameters used to attenuate the quality of the simulations are the effective analog energy scale of the J coupling, the total annealing time, and for the case of reverse annealing the anneal-pause. The core estimation technique is to sample across the energy spectrum of the classical Hamiltonian of interest, and therefore obtain a density of states estimate for each energy level, which in turn can be used to compute an estimate of the partition function with some sampling error. This estimation technique is powerful because once the distribution is sampled it allows thermodynamic quantity computation at arbitrary temperatures. On a $25$ spin $\pm J$ hardware graph native Ising model we find parameter regimes of the D-Wave processors that provide comparable result quality to two standard classical Monte Carlo methods, Multiple Histogram Reweighting and Wang-Landau. Remarkably, we find that fast quench-like anneals can quickly generate ensemble distributions that are very good estimates of the true partition function of the classical Ising model; on a Pegasus graph-structured QPU we report a logarithmic relative error of $7.6 \times 10^{-6}$, from $171,000$ samples generated using $0.2$ seconds of QPU time with an anneal time of $8$ nanoseconds per sample which is interestingly within the closed system dynamics timescale of the superconducting qubits.</p></details> |  |
| **[An Adaptive Graphical Lasso Approach to Modeling Symptom Networks of Common Mental Disorders in Eritrean Refugee Population](https://arxiv.org/abs/2512.19681v1)** | 2025-12-22 | <details><summary>Show</summary><p>Despite the significant public health burden of common mental disorders (CMDs) among refugee populations, their underlying symptom structures remain underexplored. This study uses Gaussian graphical modeling to examine the symptom network of post-traumatic stress disorder (PTSD), depression, anxiety, and somatic distress among Eritrean refugees in the Greater Washington, DC area. Given the small sample size (n) and high-dimensional symptom space (p), we propose a novel extension of the standard graphical LASSO by incorporating adaptive penalization, which improves sparsity selection and network estimation stability under n < p conditions. To evaluate the reliability of the network, we apply bootstrap resampling and use centrality measures to identify the most influential symptoms. Our analysis identifies six distinct symptom clusters, with somatic-anxiety symptoms forming the most interconnected group. Notably, symptoms such as nausea and reliving past experiences emerge as central symptoms linking PTSD, anxiety, depression, and somatic distress. Additionally, we identify symptoms like feeling fearful, sleep problems, and loss of interest in activities as key symptoms, either being closely positioned to many others or acting as important bridges that help maintain the overall network connectivity, thereby highlighting their potential importance as possible intervention targets.</p></details> | 34 pages, 7 figures |
| **[FMCW Radar Principles and Human Activity Recognition Systems: Foundations, Techniques, and Applications](https://arxiv.org/abs/2410.08483v2)** | 2025-12-22 | <details><summary>Show</summary><p>This book introduces the theoretical foundations of FMCW radar systems, including range and velocity estimation, signal processing techniques, and the generation of radar point clouds. A detailed discussion of Python and MATLAB as the primary programming tools for radar signal processing is provided, including the integration of libraries like NumPy, Matplotlib, and SciPy for data analysis and visualization. In addition, the book covers advanced techniques such as deep learning applications for radar signal processing, focusing on Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks, and Transformers for analyzing radar data. Furthermore, it highlights state-of-the-art methods for human activity recognition using radar, leveraging a combination of traditional signal processing techniques and machine learning models. The book is designed to cater to both beginners and experts in radar signal processing, offering practical examples, code implementations, and insights into the future of radar technology in various domains, including autonomous systems and security applications.</p></details> | 203pages |
| **[Deep Legendre Transform](https://arxiv.org/abs/2512.19649v1)** | 2025-12-22 | <details><summary>Show</summary><p>We introduce a novel deep learning algorithm for computing convex conjugates of differentiable convex functions, a fundamental operation in convex analysis with various applications in different fields such as optimization, control theory, physics and economics. While traditional numerical methods suffer from the curse of dimensionality and become computationally intractable in high dimensions, more recent neural network-based approaches scale better, but have mostly been studied with the aim of solving optimal transport problems and require the solution of complicated optimization or max-min problems. Using an implicit Fenchel formulation of convex conjugation, our approach facilitates an efficient gradient-based framework for the minimization of approximation errors and, as a byproduct, also provides a posteriori error estimates for the approximation quality. Numerical experiments demonstrate our method's ability to deliver accurate results across different high-dimensional examples. Moreover, by employing symbolic regression with Kolmogorov--Arnold networks, it is able to obtain the exact convex conjugates of specific convex functions.</p></details> | <details><summary>Accep...</summary><p>Accepted at NeurIPS 2025 (poster). NeurIPS page: https://neurips.cc/virtual/2025/loc/san-diego/poster/120307</p></details> |
| **[On the class of exponential statistical structures of type B](https://arxiv.org/abs/2510.26863v2)** | 2025-12-22 | <details><summary>Show</summary><p>The article is devoted to the study of exponential statistical structures of type B, which constitute a subclass of exponential families of probability distributions. This class is characterized by a number of analytical and probabilistic properties that make it a convenient tool for solving both theoretical and applied problems in statistics. The relevance of this research lies in the need to generalize known classes of distributions and to develop a unified framework for their analysis, which is essential for applications in stochastic modeling, machine learning, financial mathematics. The paper proposes a formal definition of type B. Necessary and sufficient conditions for a statistical structure to belong to class B are established, and it is proved that such structures can be represented through a dominating measure with an explicit Laplace transform. The obtained results make it possible to describe a wide range of well-known one-dimensional and multivariate distributions, including the Binomial, Poisson, Normal, Gamma, Polynomial, Logarithmic distributions, as well as specific cases such as the Borel-Tanner and Random Walk distributions. Particular attention is given to the proof of structural theorems that determine the stability of class B under linear transformations and the addition of independent random vectors. Recursive relations for initial and central moments as well as for semi-invariants are obtained, providing an efficient analytical and computational framework for their evaluation. Furthermore, the tails of type B distributions are investigated using the properties of the Laplace transform. New exponential inequalities for estimating the probabilities of large deviations are derived. The obtained results can be applied in theoretical studies and in practical problems of stochastic modeling.</p></details> |  |
| **[Milstein-type Schemes for Hyperbolic SPDEs](https://arxiv.org/abs/2512.19647v1)** | 2025-12-22 | <details><summary>Show</summary><p>This article studies the temporal approximation of hyperbolic semilinear stochastic evolution equations with multiplicative Gaussian noise by Milstein-type schemes. We take the term hyperbolic to mean that the leading operator generates a contractive, not necessarily analytic $C_0$-semigroup. Optimal convergence rates are derived for the pathwise uniform strong error \[ E_h^\infty := \Big(\mathbb{E}\max_{1\le j \le M}\|U_{t_j}-u_j\|_X^p\Big)^{1/p} \] on a Hilbert space $X$ for $p\in [2,\infty)$. Here, $U$ is the mild solution and $u_j$ its Milstein approximation at time $t_j=jh$ with step size $h>0$ and final time $T=Mh>0$. For sufficiently regular nonlinearity and noise, we establish strong convergence of order one, with the error satisfying $E_h^\infty\lesssim h\sqrt{\log(T/h)}$ for rational Milstein schemes and $E_h^\infty \lesssim h$ for exponential Milstein schemes. This extends previous results from parabolic to hyperbolic SPDEs and from exponential to rational Milstein schemes. Root-mean-square error estimates are strengthened to pathwise uniform estimates. Numerical experiments validate the convergence rates for the stochastic Schrödinger equation. Further applications to Maxwell's and transport equations are included.</p></details> | <details><summary>47 pa...</summary><p>47 pages, 3 figure, 4 tables. Comments are welcome!</p></details> |
| **[The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference](https://arxiv.org/abs/2512.19643v1)** | 2025-12-22 | <details><summary>Show</summary><p>Numerical simulation of time-dependent partial differential equations (PDEs) is central to scientific and engineering applications, but high-fidelity solvers are often prohibitively expensive for long-horizon or time-critical settings. Neural operator (NO) surrogates offer fast inference across parametric and functional inputs; however, most autoregressive NO frameworks remain vulnerable to compounding errors, and ensemble-averaged metrics provide limited guarantees for individual inference trajectories. In practice, error accumulation can become unacceptable beyond the training horizon, and existing methods lack mechanisms for online monitoring or correction. To address this gap, we propose ANCHOR (Adaptive Numerical Correction for High-fidelity Operator Rollouts), an online, instance-aware hybrid inference framework for stable long-horizon prediction of nonlinear, time-dependent PDEs. ANCHOR treats a pretrained NO as the primary inference engine and adaptively couples it with a classical numerical solver using a physics-informed, residual-based error estimator. Inspired by adaptive time-stepping in numerical analysis, ANCHOR monitors an exponential moving average (EMA) of the normalized PDE residual to detect accumulating error and trigger corrective solver interventions without requiring access to ground-truth solutions. We show that the EMA-based estimator correlates strongly with the true relative L2 error, enabling data-free, instance-aware error control during inference. Evaluations on four canonical PDEs: 1D and 2D Burgers', 2D Allen-Cahn, and 3D heat conduction, demonstrate that ANCHOR reliably bounds long-horizon error growth, stabilizes extrapolative rollouts, and significantly improves robustness over standalone neural operators, while remaining substantially more efficient than high-fidelity numerical solvers.</p></details> | 18 pages, 7 figures |
| **[Quantum Imaging of Birefringent Samples using Hong-Ou-Mandel Interference](https://arxiv.org/abs/2512.19637v1)** | 2025-12-22 | <details><summary>Show</summary><p>Two-photon interference in a Hong-Ou-Mandel (HOM) interferometer can be used as a quantum sensing mechanism due to the sensitivity of the interference dip to perturbations of the photon indistinguishability. In particular, recent works have generalized this concept to microscopy setups, but the sensitivity to optical path differences constrains its application to samples with thickness variation typically below a few micrometers if tracking changes in the coincidences at a fixed delay. Extending the concept to polarization microscopy and circumventing this limitation, this manuscript explores the use of a narrowband photon pair source with coherence length >1 mm to broaden the HOM dip. Thus, realistic sample-thickness variations introduce negligible temporal distinguishability, and changes in coincidence rate at the dip centre are then dominated by sample-induced polarization effects. To compute the polarization rotation, we develop a statistical model for the interferometer, derive the Fisher information, and establish a maximum-likelihood estimator for the local fast-axis angle. Recording dip and baseline frames at each sample position via raster scanning, the experimental results validate the framework, agreeing with classical polarized-intensity images while demonstrating operation at a maximum-precision regime and insensitiveness to layer thickness. Overall, the approach enclosed provides a quantum-based quantitative imaging of birefringent structures, which can motivate further advantageous applications, including enhanced signal-to-noise ratio and lower damage imaging of photosensitive samples.</p></details> |  |
| **[Approximate co-sufficient sampling with regularization](https://arxiv.org/abs/2309.08063v3)** | 2025-12-22 | <details><summary>Show</summary><p>In this work, we consider the problem of goodness-of-fit (GoF) testing for parametric models. This testing problem involves a composite null hypothesis, due to the unknown values of the model parameters. In some special cases, co-sufficient sampling (CSS) can remove the influence of these unknown parameters via conditioning on a sufficient statistic -- often, the maximum likelihood estimator (MLE) of the unknown parameters. However, many common parametric settings do not permit this approach, since conditioning on a sufficient statistic leads to a powerless test. The recent approximate co-sufficient sampling (aCSS) framework of Barber and Janson (2022) offers an alternative, replacing sufficiency with an approximately sufficient statistic (namely, a noisy version of the MLE). This approach recovers power in a range of settings where CSS cannot be applied, but can only be applied in settings where the unconstrained MLE is well-defined and well-behaved, which implicitly assumes a low-dimensional regime. In this work, we extend aCSS to the setting of constrained and penalized maximum likelihood estimation, so that more complex estimation problems can now be handled within the aCSS framework, including examples such as mixtures-of-Gaussians (where the unconstrained MLE is not well-defined due to degeneracy) and high-dimensional Gaussian linear models (where the MLE can perform well under regularization, such as an $\ell_1$ penalty or a shape constraint).</p></details> |  |
| **[FFTArray: A Python Library for the Implementation of Discretized Multi-Dimensional Fourier Transforms](https://arxiv.org/abs/2508.03697v3)** | 2025-12-22 | <details><summary>Show</summary><p>Partial differential equations describing the dynamics of physical systems rarely have closed-form solutions. Fourier spectral methods, which use Fast Fourier Transforms (FFTs) to approximate solutions, are a common approach to solving these equations. However, mapping Fourier integrals to discrete FFTs is not straightforward, as the selection of the grid as well as the coordinate-dependent phase and scaling factors require special care. Moreover, most software packages that deal with this step integrate it tightly into their full-stack implementations. Such an integrated design sacrifices generality, making it difficult to adapt to new coordinate systems, boundary conditions, or problem-specific requirements. To address these challenges, we present FFTArray, a Python library that automates the general discretization of Fourier transforms. Its purpose is to reduce the barriers to developing high-performance, maintainable code for pseudo-spectral Fourier methods. Its interface enables the direct translation of textbook equations and complex research problems into code, and its modular design scales naturally to multiple dimensions. This makes the definition of valid coordinate grids straightforward, while coordinate grid specific corrections are applied with minimal impact on computational performance. Built on the Python Array API Standard, FFTArray integrates seamlessly with array backends like NumPy, JAX and PyTorch and supports Graphics Processing Unit acceleration. The code is openly available at https://github.com/QSTheory/fftarray under Apache-2.0 license.</p></details> | <details><summary>Rewor...</summary><p>Reworked introduction to chapter 2 and subsection 2.1 to address referee report</p></details> |
| **[LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry](https://arxiv.org/abs/2512.19629v1)** | 2025-12-22 | <details><summary>Show</summary><p>Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the \href{https://steinate.github.io/logoplanner.github.io/}{project page}.</p></details> | <details><summary>Proje...</summary><p>Project page:https://steinate.github.io/logoplanner.github.io/</p></details> |
| **[Global bifurcation of hollow vortex streets](https://arxiv.org/abs/2512.19619v1)** | 2025-12-22 | <details><summary>Show</summary><p>Vortex streets are periodic configurations of vortices propagating through an irrotational flow. In this paper, we study streets of hollow vortices, which are solutions to the free boundary $2$-d irrotational incompressible Euler equations. Each vortex core is a region of constant pressure in the complement of the fluid domain with a nonzero circulation around it. We prove that any non-degenerate singly-periodic point vortex configuration can be ``desingularized'' to create a global curve of solutions to the steady hollow vortex street problem, and we further characterize the types of singular behavior that can develop as one transverses the curve to its extreme. As specific examples, we study von Kármán vortex streets, translating vortex arrays, and a two-pair (2P) configuration. Our method is based on analytic global bifurcation theory and adapts the desingularization technique of Chen, Walsh, and Wheeler to the periodic setting.</p></details> | 35 pages, 3 figures |
| **[Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior](https://arxiv.org/abs/2512.19584v1)** | 2025-12-22 | <details><summary>Show</summary><p>Dynamic PET enables the quantitative estimation of physiology-related parameters and is widely utilized in research and increasingly adopted in clinical settings. Parametric imaging in dynamic PET requires kinetic modeling to estimate voxel-wise physiological parameters based on specific kinetic models. However, parametric images estimated through kinetic model fitting often suffer from low image quality due to the inherently ill-posed nature of the fitting process and the limited counts resulting from non-continuous data acquisition across multiple bed positions in whole-body PET. In this work, we proposed a diffusion model-based kinetic modeling framework for parametric image estimation, using the Patlak model as an example. The score function of the diffusion model was pre-trained on static total-body PET images and served as a prior for both Patlak slope and intercept images by leveraging their patch-wise similarity. During inference, the kinetic model was incorporated as a data-consistency constraint to guide the parametric image estimation. The proposed framework was evaluated on total-body dynamic PET datasets with different dose levels, demonstrating the feasibility and promising performance of the proposed framework in improving parametric image quality.</p></details> | 10 pages, 9 figures |
| **[A modified Brinkman penalization fictitious domain method for the unsteady Navier-Stokes equations](https://arxiv.org/abs/2512.19580v1)** | 2025-12-22 | <details><summary>Show</summary><p>This paper investigates a modification of the fictitious domain method with continuation in the lower-order coefficients for the unsteady Navier-Stokes equations governing the motion of an incompressible homogeneous fluid in a bounded 2D or 3D domain. The modification enables {a solution-dependent} choice of the critical parameter. Global-in-time existence and convergence of a weak solution to the auxiliary problem are proved, and local-in-time existence and convergence of a unique strong solution are established. For the strong solution, a new higher-order convergence rate estimate in the penalization parameter is obtained. The introduced framework allows us to apply a pointwise divergence free finite element method as a discretization technique, leading to strongly mass conservative discrete fictitious domain method. A numerical example illustrates the performance of the method.</p></details> |  |
| **[Deep Learning for Primordial $B$-mode Extraction](https://arxiv.org/abs/2512.19577v1)** | 2025-12-22 | <details><summary>Show</summary><p>The search for primordial gravitational waves is a central goal of cosmic microwave background (CMB) surveys. Isolating the characteristic $B$-mode polarization signal sourced by primordial gravitational waves is challenging for several reasons: the amplitude of the signal is inherently small; astrophysical foregrounds produce $B$-mode polarization contaminating the signal; and secondary $B$-mode polarization fluctuations are produced via the conversion of $E$ modes. Current and future low-noise, multi-frequency observations enable sufficient precision to address the first two of these challenges such that secondary $B$ modes will become the bottleneck for improved constraints on the amplitude of primordial gravitational waves. The dominant source of secondary $B$-mode polarization is gravitational lensing by large scale structure. Various strategies have been developed to estimate the lensing deflection and to reverse its effects the CMB, thus reducing confusion from lensing $B$ modes in the search for primordial gravitational waves. However, a few complications remain. First, there may be additional sources of secondary $B$-mode polarization, for example from patchy reionization or from cosmic polarization rotation. Second, the statistics of delensed CMB maps can become complicated and non-Gaussian, especially when advanced lensing reconstruction techniques are applied. We previously demonstrated how a deep learning network, ResUNet-CMB, can provide nearly optimal simultaneous estimates of multiple sources of secondary $B$-mode polarization. In this paper, we show how deep learning can be applied to estimate and remove multiple sources of secondary $B$-mode polarization, and we further show how this technique can be used in a likelihood analysis to produce nearly optimal, unbiased estimates of the amplitude of primordial gravitational waves.</p></details> | <details><summary>12 pa...</summary><p>12 pages, 8 figures. Code available from https://github.com/EEmGuzman/resunet-cmb</p></details> |
| **[Degeneration of Riemann surfaces and small eigenvalues of the Laplacian](https://arxiv.org/abs/2509.06151v2)** | 2025-12-22 | <details><summary>Show</summary><p>For a one-parameter degeneration of compact Riemann surfaces endowed with the Kähler metric induced from the Kähler metric on the total space of the family, we determine the exact magnitude of the small eigenvalues of the Laplacian as a function on the parameter space, under the assumption that the singular fiber is reduced. The novelty in our approach is that we compute the asymptotic behavior of certain difference of (logarithm of) analytic torsions in the degeneration in two ways. On the one hand, via heat kernel estimates, it is shown that the leading asymptotic is determined by the product of the small eigenvalues. On the other hand, using Quillen metrics, the leading asymptotic is connected with the period integrals, which we explicitly evaluate.</p></details> | 45 pages |
| **[Optimal Uncertainty Quantification under General Moment Constraints on Input Subdomains](https://arxiv.org/abs/2512.19572v1)** | 2025-12-22 | <details><summary>Show</summary><p>We present an optimal uncertainty quantification (OUQ) framework for systems whose uncertain inputs are characterized by truncated moment constraints defined over subdomains. Based on this partial information, rigorous optimal upper and lower bounds on the probability of failure (PoF) are derived over the admissible set of probability measures, providing a principled basis for system safety certification. We formulate the OUQ problem under general subdomain moment constraints and develop a high-performance computational framework to compute the optimal bounds. This approach transforms the original infinite-dimensional optimization problems into finite-dimensional unconstrained ones parameterized solely by free canonical moments. To address the prohibitive cost of PoF evaluation in high-dimensional settings, we incorporate inverse transform sampling (ITS), enabling efficient and accurate PoF estimation within the OUQ optimization. We also demonstrate that constraining inputs only by zeroth-order moments over subdomains yields a formulation equivalent to evidence theory. Three groups of numerical examples demonstrate the framework's effectiveness and scalability. Results show that increasing the number of subdomains or the moment order systematically tightens the bound interval. For high-dimensional problems, the ITS strategy reduces computational costs by up to two orders of magnitude while maintaining relative error below 1%. Furthermore, we identify regimes where optimal bounds are sensitive to subdomain partitioning or higher-order moments, guiding uncertainty reduction efforts for safety certification.</p></details> | 31 pages, 13 figures |
| **[Owning the Intelligence: Global AI Patents Landscape and Europe's Quest for Technological Sovereignty](https://arxiv.org/abs/2512.19569v1)** | 2025-12-22 | <details><summary>Show</summary><p>Artificial intelligence has become a key arena of global technological competition and a central concern for Europe's quest for technological sovereignty. This paper analyzes global AI patenting from 2010 to 2023 to assess Europe's position in an increasingly bipolar innovation landscape dominated by the United States and China. Using linked patent, firm, ownership, and citation data, we examine the geography, specialization, and international diffusion of AI innovation. We find a highly concentrated patent landscape: China leads in patent volumes, while the United States dominates in citation impact and technological influence. Europe accounts for a limited share of AI patents but exhibits signals of relatively high patent quality. Technological proximity reveals global convergence toward U.S. innovation trajectories, with Europe remaining fragmented rather than forming an autonomous pole. Gravity-model estimates show that cross-border AI knowledge flows are driven primarily by technological capability and specialization, while geographic and institutional factors play a secondary role. EU membership does not significantly enhance intra-European knowledge diffusion, suggesting that technological capacity, rather than political integration, underpins participation in global AI innovation networks.</p></details> |  |
| **[A Statistical Framework for Understanding Causal Effects that Vary by Treatment Initiation Time in EHR-based Studies](https://arxiv.org/abs/2512.19553v1)** | 2025-12-22 | <details><summary>Show</summary><p>Comparative effectiveness studies using electronic health records (EHR) consider data from patients who could ``enter'' the study cohort at any point during an interval that spans many years in calendar time. Unlike treatments in tightly controlled trials, real-world treatments can evolve over calendar time, especially if comparators include standard of care, or procedures where techniques may improve. Efforts to assess whether treatment efficacy itself is changing are complicated by changing patient populations, with potential covariate shift in key effect modifiers. In this work, we propose a statistical framework to estimate calendar-time specific average treatment effects and describe both how and why effects vary across treatment initiation time in EHR-based studies. Our approach projects doubly robust, time-specific treatment effect estimates onto candidate marginal structural models and uses a model selection procedure to best describe how effects vary by treatment initiation time. We further introduce a novel summary metric, based on standardization analysis, to quantify the role of covariate shift in explaining observed effect changes and disentangle changes in treatment effects from changes in the patient population receiving treatment. Extensive simulations using EHR data from Kaiser Permanente are used to validate the utility of the framework, which we apply to study changes in relative weight loss following two bariatric surgical interventions versus no surgery among patients with severe obesity between 2005-2011.</p></details> |  |
| **[Yang-Mills energy quantization over non-collapsed degenerating Einstein manifolds and applications](https://arxiv.org/abs/2512.19552v1)** | 2025-12-22 | <details><summary>Show</summary><p>We investigate a sequence of Yang-Mills connections $A_j$ lying in vector bundles $E_j$ over non-collapsed degenerating closed Einstein 4-manifolds $(M_j, g_ j)$ with uniformly bounded Einstein constants and bounded diameters. We establish a compactness theory modular three types of bubbles. As applications, we get some quantization results for several important topological number associated with the vector bundles, for instance, the first Pontrjagin numbers $p_1(E)$ of vector bundles over Einstein 4-manifolds and the Euler numbers $χ(M;E)$ of holomorphic vector bundles over Kähler-Einstein surfaces. Furthermore, we get some quantization results about the volume $v(L_j)$ and certain cohomological numbers (e.g. $dim H^0(M_j;L_j)$) of holomorphic line bundles $L_j$ over non-collapsed degenerating Kähler-Einstein surfaces $(M_j,J_j,g_j)$ with the aid of the classical vanishing theorems, the classical Hirzebruch-Riemann-Roch type theorems, and the profound convergence theory of Kähler-Einstein manifolds. In particular, we obtain some interesting identities involving non-collapsed degenerating compact Kähler-Einstein surfaces with non-zero scalar curvature, which indicate that we can know the Euler number of $M_j$ for large $j$ provided some topological information of the limit orbifold $M_\infty$. For Kähler-Einstein Del Pezzo surfaces, an interesting implication is that we can provide some preliminary estimates for the number of singularities of various types in $M_\infty$ in an effective way. As an unexpected surprise, we find an identity which connects Milnor numbers for singularities in $M_\infty$ and the correction terms in the Hirzebruch-Riemann-Roch theorem for orbifolds. Some quantization results can be extended to the case of higher dimensional $n$-manifolds.</p></details> | Comments welcome! |
| **[A Reynolds-semi-robust method with hybrid velocity and pressure for the unsteady incompressible Navier--Stokes equations](https://arxiv.org/abs/2502.15293v2)** | 2025-12-22 | <details><summary>Show</summary><p>In this paper we propose and analyze a new Finite Element method for the solution of the two- and three-dimensional incompressible Navier--Stokes equations based on a hybrid discretization of both the velocity and pressure variables. The proposed method is pressure-robust, i.e., irrotational forcing terms do not affect the approximation of the velocity, and Reynolds-quasi-robust, with error estimates that, for smooth enough exact solutions, do not depend on the inverse of the viscosity. We carry out an in-depth convergence analysis highlighting pre-asymptotic convergence rates and validate the theoretical findings with a complete set of numerical experiments.</p></details> |  |
| **[QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models](https://arxiv.org/abs/2512.19526v1)** | 2025-12-22 | <details><summary>Show</summary><p>Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.</p></details> |  |
| **[Estimating Spatially Resolved Radiation Fields Using Neural Networks](https://arxiv.org/abs/2512.17654v2)** | 2025-12-22 | <details><summary>Show</summary><p>We present an in-depth analysis on how to build and train neural networks to estimate the spatial distribution of scattered radiation fields for radiation protection dosimetry in medical radiation fields, such as those found in interventional radiology and cardiology. We present three different synthetically generated datasets with increasing complexity for training, using a Monte-Carlo Simulation application based on Geant4. On those datasets, we evaluate convolutional and fully connected architectures of neural networks to demonstrate which design decisions work well for reconstructing the fluence and spectra distributions over the spatial domain of such radiation fields. All our datasets, as well as our training pipeline, are published as open source in separate repositories.</p></details> |  |
| **[Spectral representations of interpolation spaces of reproducing kernel Hilbert spaces](https://arxiv.org/abs/2508.16492v2)** | 2025-12-22 | <details><summary>Show</summary><p>In statistical learning theory, interpolation spaces of the form $[\mathrm{L}^2,H]_{θ,r}$, where $H$ is a reproducing kernel Hilbert space, are in widespread use. So far, however, they are only well understood for fine index $r=2$. We generalise existing results from $r=2$ to all possible values of $r$. In particular, we present a spectral decomposition of such spaces, analyse their embedding properties, and describe connections to the theory of Banach spaces of functions. We additionally present example applications of our results to regularisation error estimation in statistical learning.</p></details> | 29 pages |
| **[Addition is almost all you need: Compressing neural networks with double binary factorization](https://arxiv.org/abs/2505.11076v3)** | 2025-12-22 | <details><summary>Show</summary><p>Binary quantization approaches, which replace weight matrices with binary matrices and substitute costly multiplications with cheaper additions, offer a computationally efficient approach to address the increasing computational and storage requirements of Large Language Models (LLMs). However, the severe quantization constraint ($\pm1$) can lead to significant accuracy degradation. In this paper, we propose Double Binary Factorization (DBF), a novel method that factorizes dense weight matrices into products of two binary (sign) matrices, each accompanied by scaling vectors. DBF preserves the efficiency advantages of binary representations while achieving compression rates that are competitive with or superior to state-of-the-art methods. Specifically, in a 1-bit per weight range, DBF is better than existing binarization approaches. In a 2-bit per weight range, DBF is competitive with the best quantization methods like QuIP\# and QTIP. Unlike most existing compression techniques, which offer limited compression level choices, DBF allows fine-grained control over compression ratios by adjusting the factorization's intermediate dimension. Based on this advantage, we further introduce an algorithm for estimating non-uniform layer-wise compression ratios for DBF, based on previously developed channel pruning criteria. Code available at: https://github.com/usamec/double_binary</p></details> |  |
| **[Error Estimates for Sparse Tensor Products of B-spline Approximation Spaces](https://arxiv.org/abs/2510.21517v3)** | 2025-12-22 | <details><summary>Show</summary><p>This work introduces and analyzes B-spline approximation spaces defined on general geometric domains obtained through a mapping from a parameter domain. These spaces are constructed as sparse-grid tensor products of univariate spaces in the parameter domain and are mapped to the physical domain via a geometric parametrization. Both the univariate approximation spaces and the geometric mapping are built using maximally smooth B-splines. We construct two such spaces, employing either the sparse-grid combination technique or the hierarchical subspace decomposition of sparse-grid tensor products, and we prove their mathematical equivalence. Furthermore, we derive approximation error estimates and inverse inequalities that highlight the advantages of sparse-grid tensor products. Specifically, under suitable regularity assumptions on the solution, these spaces achieve the same approximation order as standard tensor product spaces while using significantly fewer degrees of freedom. Additionally, our estimates indicate that, in the case of non-tensor-product domains, stronger regularity assumptions on the solution -- particularly concerning isotropic (non-mixed) derivatives -- are required to achieve optimal convergence rates compared to sparse-grid methods defined on tensor-product domains.</p></details> |  |
| **[Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks](https://arxiv.org/abs/2511.14455v3)** | 2025-12-22 | <details><summary>Show</summary><p>We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\varphi=\varphi(x,u)$ such that $\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.</p></details> |  |
| **[Large deviations for stochastic evolution equations beyond the coercive case](https://arxiv.org/abs/2512.19501v1)** | 2025-12-22 | <details><summary>Show</summary><p>We prove the small-noise large deviation principle (LDP) for stochastic evolution equations in an $L^2$-setting. As the coefficients are allowed to be non-coercive, our framework encompasses a much broader scope than variational settings. To replace coercivity, we require only well-posedness of the stochastic evolution equation and two concrete, verifiable a priori estimates. Furthermore, we accommodate drift nonlinearities satisfying a modified criticality condition, and we allow for vanishing drift perturbations. The latter permits the inclusion of Itô--Stratonovich correction terms, enabling the treatment of both noise interpretations. In another paper, our results have been applied to the 3D primitive equations with full transport noise. In the current paper, we give an application to a reaction-diffusion system which lacks coercivity, further demonstrating the versatility of the framework. Finally, we show that even in the coercive case, we obtain new LDP results for equations with critical nonlinearities that rely on our modified criticality condition, including the stochastic 2D Allen--Cahn equation in the weak setting.</p></details> |  |
| **[Superradiant decay in non-Markovian Waveguide Quantum Electrodynamics](https://arxiv.org/abs/2511.22332v2)** | 2025-12-22 | <details><summary>Show</summary><p>An array of initially excited emitters coupled to a one-dimensional waveguide exhibits superradiant decay under the Born-Markov approximation, manifested as a coherent burst of photons in the output field. In this work, we employ tensor-network methods to investigate its non-Markovian dynamics induced by finite time delays in photon exchange among the emitters. We find that the superradiant burst breaks into a structured train of correlated photons, each intensity peak corresponding to a specific photon number. We quantify the emitter-photon and emitter-emitter entanglement generated during this process and show that the latter emerges in the long-time limit, as part of the excitation becomes trapped within the emitters' singlet subspace. We finally consider the decay of the system's most radiant state, the symmetric Dicke state, and show that time delay can lead to decay rates exceeding those predicted by the Markovian approximation.</p></details> | 16 pages, 8 figures |
| **[Asymptotic theory for nonparametric testing of $k$-monotonicity in discrete distributions](https://arxiv.org/abs/2407.01751v2)** | 2025-12-22 | <details><summary>Show</summary><p>In shape-constrained nonparametric inference, it is often necessary to perform preliminary tests to verify whether a probability mass function (p.m.f.) satisfies qualitative constraints such as monotonicity, convexity, or in general $k$-monotonicity. In this paper, we are interested in nonparametric testing of $k$-monotonicity of a finitely supported discrete distribution. We consider a unified testing framework based on a natural statistic which is directly derived from the very definition of $k$-monotonicity. The introduced framework allows us to design a new consistent method to select the unknown knot points that are required to consistently approximate the limit distribution of several test statistics based either on the empirical measure or the shape-constrained estimators of the p.m.f. We show that the resulting tests are asymptotically valid and consistent for any fixed alternative. Additionally, for the test based solely on the empirical measure, we study the asymptotic power under contiguous alternatives and derive a quantitative separation result that provides sufficient conditions to achieve a given power. We employ this test to design an estimator for the largest parameter $k \in \mathbb N_0$ such that the p.m.f. is $j$-monotone for all $j = 0, \ldots, k$, and show that the estimator is different from the true parameter with probability which is asymptotically smaller than the nominal level of the test. A detailed simulation study is performed to assess the finite sample performance of all the proposed tests, and applications to several real datasets are presented to illustrate the theory.</p></details> |  |
| **[Causal inference with misspecified network interference structure](https://arxiv.org/abs/2302.11322v3)** | 2025-12-22 | <details><summary>Show</summary><p>Under interference, the treatment of one unit may affect the outcomes of other units. Such interference patterns between units are typically represented by a network. Correctly specifying this network requires identifying which units can affect others -- an inherently challenging task. Nevertheless, most existing approaches assume that a known and accurate network specification is given. In this paper, we study the consequences of such misspecification. We derive bounds on the bias arising from estimating causal effects using a misspecified network, showing that the estimation bias grows with the divergence between the assumed and true networks, quantified through their induced exposure probabilities. To address this challenge, we propose a novel estimator that leverages multiple networks simultaneously and remains unbiased if at least one of the networks is correct, even when we do not know which one. Therefore, the proposed estimator provides robustness to network specification. We illustrate key properties and demonstrate the utility of our proposed estimator through simulations and analysis of a social network field experiment.</p></details> |  |
| **[3DFETUS: Deep Learning-Based Standardization of Facial Planes in 3D Ultrasound](https://arxiv.org/abs/2511.10412v2)** | 2025-12-22 | <details><summary>Show</summary><p>The automatic localization and standardization of anatomical planes in 3D medical imaging remains a challenging problem due to variability in object pose, appearance, and image quality. In 3D ultrasound, these challenges are exacerbated by speckle noise and limited contrast, particularly in fetal imaging. To address these challenges in the context of facial assessment, we present: 1) GT++, a robust algorithm that estimates standard facial planes from 3D US volumes using annotated anatomical landmarks; and 2) 3DFETUS, a deep learning model that automates and standardizes their localization in 3D fetal US volumes. We evaluated our methods both qualitatively, through expert clinical review, and quantitatively. The proposed approach achieved a mean translation error of 3.21 $\pm$ 1.98mm and a mean rotation error of 5.31 $\pm$ 3.945$^\circ$ per plane, outperforming other state-of-the-art methods on 3D US volumes. Clinical assessments further confirmed the effectiveness of both GT++ and 3DFETUS, demonstrating statistically significant improvements in plane estimation accuracy.</p></details> |  |
| **[Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications](https://arxiv.org/abs/2512.19472v1)** | 2025-12-22 | <details><summary>Show</summary><p>The recent explosive growth in Deep Neural Networks applications raises concerns about the black-box usage of such models, with limited trasparency and trustworthiness in high-stakes domains, which have been crystallized as regulatory requirements such as the European Union Artificial Intelligence Act. While models with embedded confidence metrics have been proposed, such approaches cannot be applied to already existing models without retraining, limiting their broad application. On the other hand, post-hoc methods, which evaluate pre-trained models, focus on solving problems related to improving the confidence in the model's predictions, and detecting Out-Of-Distribution or Adversarial Attacks samples as independent applications. To tackle the limited applicability of already existing methods, we introduce Multi-Layer Analysis for Confidence Scoring (MACS), a unified post-hoc framework that analyzes intermediate activations to produce classification-maps. From the classification-maps, we derive a score applicable for confidence estimation, detecting distributional shifts and adversarial attacks, unifying the three problems in a common framework, and achieving performances that surpass the state-of-the-art approaches in our experiments with the VGG16 and ViTb16 models with a fraction of their computational overhead.</p></details> |  |
| **[Fully Asynchronous Unsourced Random Access over Fading Channels](https://arxiv.org/abs/2512.19468v1)** | 2025-12-22 | <details><summary>Show</summary><p>We examine unsourced random access in a fully asynchronous setup, where active users transmit their data without restriction on the start time over a fading channel. In the proposed scheme, the transmitted signal consists of a pilot sequence and a polar codeword, with the polar codeword distributed across the data part of the packet in an on-off pattern. The receiver uses a double sliding-window decoder, where the inner window employs iterative decoding with joint timing and pilot detection, channel estimation, single-user decoding, and successive interference cancellation to recover the message bits, while the outer window enhances interference cancellation. The numerical results indicate that the proposed scheme exhibits only a slight performance loss compared to the synchronous benchmark while being more applicable in practice.</p></details> |  |
| **[Gap-free Information Transfer in 4D-STEM via Fusion of Complementary Scattering Channels](https://arxiv.org/abs/2512.19460v1)** | 2025-12-22 | <details><summary>Show</summary><p>Linear phase-contrast scanning transmission electron microscopy (STEM) techniques compatible with high-throughput 4D-STEM acquisition are widely used to enhance phase contrast in weakly scattering and beam-sensitive materials. In these modalities, contrast transfer is often suppressed at low spatial frequencies, resulting in a characteristic contrast gap that limits quantitative imaging. Approaches that retain low-frequency phase contrast exist but typically require substantially increased experimental complexity, restricting routine use. Dark-field STEM imaging captures this missing low-frequency information through electrons scattered outside the bright-field disk, but discards a large fraction of the scattered signal and is therefore dose-inefficient. Fused Full-field STEM (FF-STEM) is introduced as a 4D-STEM imaging modality that overcomes this limitation by combining ptychographic phase reconstruction with tilt-corrected dark-field imaging within a single acquisition. Bright-field data are used to estimate probe aberrations and reconstruct a high-resolution phase image, while dark-field data provide complementary low-frequency contrast. The two channels are optimally fused in Fourier space using minimum-variance weighting based on the spectral signal-to-noise ratio, yielding transfer-gap-free images with high contrast and quantitative fidelity. FF-STEM preserves the upsampling and depth-sectioning capabilities of ptychography, adds robust low-frequency contrast characteristic of dark-field imaging, and enables dose-efficient, near-real-time reconstruction.</p></details> |  |
| **[Two-Dimensional Tomographic Reconstruction From Projections With Unknown Angles and Unknown Spatial Shifts](https://arxiv.org/abs/2511.22890v2)** | 2025-12-22 | <details><summary>Show</summary><p>In parallel beam computed tomography (CT), an object is reconstructed from a series of projections taken at different angles. However, in some industrial and biomedical imaging applications, the projection geometry is unknown, completely or partially. In this paper, we present a technique for two-dimensional (2D) tomography in which both viewing angles and spatial shifts associated with the projections are unknown. There exists literature on 2D unknown view tomography (UVT), but most existing 2D UVT algorithms assume that the projections are centered; that is, there are no spatial shifts in the projections. To tackle these geometric ambiguities, we first modify an existing graph Laplacian-based algorithm for 2D UVT to incorporate spatial shifts, and then use it as the initialization for the proposed three-way alternating minimization algorithm that jointly estimates the 2D structure, its projection angles, and the corresponding shifts. We evaluate our method on noisy projections of ribosome images and demonstrate that it achieves superior reconstruction compared to the baseline that neglects shifts.</p></details> | <details><summary>5 pag...</summary><p>5 pages, 2 figures, 1 table, submitted to the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</p></details> |
| **[Regularity for fully nonlinear elliptic equations in generalized Orlicz spaces](https://arxiv.org/abs/2512.16600v2)** | 2025-12-22 | <details><summary>Show</summary><p>In this paper, we establish an optimal global Calderón-Zygmund type estimate for the viscosity solution to the Dirichlet boundary problem of fully nonlinear elliptic equations with possibly nonconvex nonlinearities. We prove that the Hessian of the solution is as integrable as the nonhomogeneous term in the setting of a given generalized Orlicz space even when the nonlinearity is asymptotically convex with respect to the Hessian of the solution.</p></details> |  |
| **[Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm](https://arxiv.org/abs/2512.19440v1)** | 2025-12-22 | <details><summary>Show</summary><p>Kernel logistic regression (KLR) is a widely used supervised learning method for binary and multi-class classification, which provides estimates of the conditional probabilities of class membership for the data points. Unlike other kernel methods such as Support Vector Machines (SVMs), KLRs are generally not sparse. Previous attempts to deal with sparsity in KLR include a heuristic method referred to as the Import Vector Machine (IVM) and ad hoc regularizations such as the $\ell_{1/2}$-based one. Achieving a good trade-off between prediction accuracy and sparsity is still a challenging issue with a potential significant impact from the application point of view. In this work, we revisit binary KLR and propose an extension of the training formulation proposed by Keerthi et al., which is able to induce sparsity in the trained model, while maintaining good testing accuracy. To efficiently solve the dual of this formulation, we devise a decomposition algorithm of Sequential Minimal Optimization type which exploits second-order information, and for which we establish global convergence. Numerical experiments conducted on 12 datasets from the literature show that the proposed binary KLR approach achieves a competitive trade-off between accuracy and sparsity with respect to IVM, $\ell_{1/2}$-based regularization for KLR, and SVM while retaining the advantages of providing informative estimates of the class membership probabilities.</p></details> |  |
| **[Brownian bridge limit of path measures in the upper tail of KPZ models](https://arxiv.org/abs/2311.12009v2)** | 2025-12-22 | <details><summary>Show</summary><p>For models in the KPZ universality class, such as the zero temperature model of planar last passage-percolation (LPP) and the positive temperature model of directed polymers, its upper tail behavior has been a topic of recent interest, with particular focus on the associated path measures (i.e., geodesics or polymers). For Exponential LPP, diffusive fluctuation had been established in Basu-Ganguly. In the directed landscape, the continuum limit of LPP, the limiting Gaussianity at one point, as well as of related finite-dimensional distributions of the KPZ fixed point, were established, using exact formulas in Liu and Wang-Liu. It was further conjectured in these works that the limit of the corresponding geodesic should be a Brownian bridge. We prove it in both zero and positive temperatures; for the latter, neither the one-point limit nor the scale of fluctuations was previously known. Instead of relying on formulas (which are still missing in the positive temperature literature), our arguments are geometric and probabilistic, using the results on the shape of the weight and free energy profiles under the upper tail from Ganguly-Hegde as a starting point. Another key ingredient involves novel coalescence estimates, developed using the recently discovered shift-invariance Borodin-Gorin-Wheeler in these models. Finally, our proof also yields insight into the structure of the polymer measure under the upper tail conditioning, establishing a quenched localization exponent around a random backbone.</p></details> | <details><summary>77 pa...</summary><p>77 pages, 6 figures. Updated to fix an error, relying on the form of the BK inequality recently established in arXiv:2512.17823</p></details> |
| **[Sharp upper tail behavior of line ensembles via the tangent method](https://arxiv.org/abs/2208.08922v3)** | 2025-12-22 | <details><summary>Show</summary><p>We develop a new probabilistic and geometric method to obtain several sharp results pertaining to the upper tail behavior of continuum Gibbs measures on infinite ensembles of random continuous curves, also known as line ensembles, satisfying some natural assumptions. The arguments make crucial use of Brownian resampling invariance properties and correlation inequalities admitted by such Gibbs measures. We obtain sharp one-point upper tail estimates showing that the probability of the value at zero being larger than $θ$ is $\exp(-\frac{4}{3}θ^{3/2}(1+o(1)))$. A key intermediate step is developing a precise understanding of the profile when conditioned on the value at zero equaling $θ$. Our method further allows one to obtain multi-point asymptotics which were out of reach of previous approaches. As an example, we prove sharp explicit two-point upper tail estimates. This framework is then used to establish the corresponding results for the KPZ equation, which are all new. Even for the zero-temperature case of the Airy$_2$ process, our arguments yield new proofs for one-point estimates previously known due to its connections to random matrix theory, as well as new two-point asymptotics. To showcase the reach of the method, we obtain the same results in a purely non-integrable setting under only assumptions of stationarity and extremality in the class of Gibbs measures. Our method bears resemblance to the tangent method introduced by Colomo-Sportiello and mathematically realized by Aggarwal in the context of the six-vertex model.</p></details> | <details><summary>96 pa...</summary><p>96 pages, 8 figures. v3: Previously conditional results for the KPZ line ensemble have been made unconditional using arXiv:2512.17823. Abstract suitably modified</p></details> |
| **[Hybrid Analytical-Machine Learning Framework for Ripple Factor Estimation in Cockcroft-Walton Voltage Multipliers with Residual Correction for Non-Ideal Effects](https://arxiv.org/abs/2512.19434v1)** | 2025-12-22 | <details><summary>Show</summary><p>Cockcroft-Walton (CW) voltage multipliers suffer from output ripple that classical analytical models underestimate due to neglected non-idealities like diode drops and capacitor ESR, particularly in high-stage, low-frequency and heavy-load regimes. This paper proposes a hybrid framework that generates a comprehensive 324-case MATLAB/Simulink dataset varying stages (2-8), input voltage (5-25 kV), capacitance (1-10 μF), frequency (50-500 Hz) and load (6-60 MΩ), then trains a Random Forest model to predict residuals between simulated and theoretical peak-to-peak ripple. The approach achieves 70.6% RMSE reduction (131 V vs. 448 V) globally and 66.7% in critical regimes, with near-zero bias, enabling physically interpretable design optimization while outperforming pure ML in extrapolation reliability.</p></details> | <details><summary>6 Pag...</summary><p>6 Pages, 2 figures, IEEE Conference Template used</p></details> |
| **[AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting](https://arxiv.org/abs/2505.18822v2)** | 2025-12-22 | <details><summary>Show</summary><p>Modern large reasoning models demonstrate impressive problem-solving capabilities by employing sophisticated reasoning strategies. However, they often struggle to balance efficiency and effectiveness, frequently generating unnecessarily lengthy reasoning chains for simple problems. In this work, we propose AdaCtrl, a novel framework to support both difficulty-aware adaptive reasoning budget allocation and explicit user control over reasoning depth. AdaCtrl dynamically adjusts its reasoning length based on self-assessed problem difficulty, while also allowing users to manually control the budget to prioritize either efficiency or effectiveness. This is achieved through a two-stage training pipeline: an initial cold-start fine-tuning phase to instill the ability to self-aware difficulty and adjust reasoning budget, followed by a difficulty-aware reinforcement learning (RL) stage that refines the model's adaptive reasoning strategies and calibrates its difficulty assessments based on its evolving capabilities during online training. To enable intuitive user interaction, we design explicit length-triggered tags that function as a natural interface for budget control. Empirical results show that AdaCtrl adapts reasoning length based on estimated difficulty, compared to the standard training baseline that also incorporates fine-tuning and RL, it yields performance improvements and simultaneously reduces response length by 10.06% and 12.14% on the more challenging AIME2024 and AIME2025 datasets, which require elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K datasets, where more concise responses are sufficient. Furthermore, AdaCtrl enables precise user control over the reasoning budget, allowing for tailored responses to meet specific needs.</p></details> |  |
| **[Hölder regularity of doubly nonlinear nonlocal quasilinear parabolic equations in some mixed singular-degenerate regime](https://arxiv.org/abs/2512.19421v1)** | 2025-12-22 | <details><summary>Show</summary><p>We study local Hölder regularity of bounded, weak solutions for the nonlocal quasilinear equations of the form \[ (|u|^{q-2}u)_t + \text{P.V.} \int_{\mathbb{R}^n} \frac{|u(x,t) - u(y,t)|^{p-2}(u(x,t)-u(y,t))}{|x-y|^{n+sp}} dy = 0, \] with $p\in (1,\infty)$, $q\in (1,\infty)$ and $s \in (0,1)$. Analogous Hölder continuity result in the local case is known in the purely singular case $\{1<p<2, p<q\}$, purely degenerate case $\{2<p, q<p\}$, scale invariant case $\{p=q\}$ and translation invariant case $\{q=2,1<p<\infty\}$. In the nonlocal setting, Hölder regularity is known when the equation is either translation invariant $\{q=2, 1<p<\infty\}$ or scale invariant $\{q=p, 1<p<\infty\}$ or purely degenerate case $\{2<p, q<p\}$. Similar strategy can be used to obtain Hölder regularity in the purely singular case $\{1<p<2, p<q\}$. In this paper, we adapt several ideas developed over the past few years and combine it with a new intrinsic scaling to prove Hölder regularity in the mixed singular-degenerate range $\max\{p,q,2\} < \min\left\{q + \tfrac{p-1}{1+\frac{n}{sp}}, 2 + \tfrac{p-1}{1+\frac{n}{sp}}\right\}$. The proof explicitly makes use of the nonlocal nature of the problem and as a consequence, our estimates are not stable at $s \rightarrow 0$. We note that the analogous regularity in the local problem remains open.</p></details> | 43 pages |
| **[Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming](https://arxiv.org/abs/2512.15735v2)** | 2025-12-22 | <details><summary>Show</summary><p>This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.</p></details> | <details><summary>we ha...</summary><p>we have identified some technical issues, including the mathematical derivation. After discussion, all authors have agreed that the analysis requires a thorough re-derivation to ensure correctness and rigor</p></details> |
| **[Estimates for short character sums evaluated at homogeneous polynomials](https://arxiv.org/abs/2501.12325v2)** | 2025-12-22 | <details><summary>Show</summary><p>Let $p$ be a prime. We prove bounds on short Dirichlet character sums evaluated at a class of homogeneous polynomials in arbitrary dimensions. In every dimension, this bound is nontrivial for sums over boxes with side lengths as short as $p^{1/4 + κ}$ for any $κ>0$. Our methods capitalize on the relationship between characters mod $p$ and characters over finite field extensions as well as bounds on the multiplicative energy of sets in products of finite fields.</p></details> | <details><summary>47 pa...</summary><p>47 pages; v2 aligns with published version</p></details> |
| **[Uniform pathwise stability of additive singular SDEs driven by fractional Brownian motion](https://arxiv.org/abs/2511.05262v2)** | 2025-12-22 | <details><summary>Show</summary><p>We study the long-time behaviour of solutions to a class of $d$-dimensional stochastic differential equations driven by fractional Brownian motion with Hurst parameter $H \in (0,1)$. The drift consists of a dissipative Lipschitz term and a singular term of regularity $γ>1-1/(2H)$ in Besov-Hölder scales. We establish well-posedness and, through a Markovian enhancement, existence of an invariant measure. If the singular contribution is sufficiently small, we prove exponential contraction of solutions, and thereby, uniqueness of the invariant measure. Our methods rely on uniform pathwise estimates which utilise together the dissipativity of the drift and the regularisation effect of the noise.</p></details> |  |
| **[Networked Communication for Decentralised Cooperative Agents in Mean-Field Control](https://arxiv.org/abs/2503.09400v2)** | 2025-12-22 | <details><summary>Show</summary><p>The mean-field framework has been used to find approximate solutions to problems involving very large populations of symmetric, anonymous agents, which may be intractable by other methods. The cooperative mean-field control (MFC) problem has received less attention than the non-cooperative mean-field game (MFG), despite the former potentially being more useful as a tool for engineering large-scale collective behaviours. Decentralised communication algorithms have recently been introduced to MFGs, giving benefits to learning speed and robustness. Inspired by this, we introduce networked communication to MFC - where populations arguably have broader incentive to communicate - and in particular to the setting where decentralised agents learn online from a single, non-episodic run of the empirical system. We adapt recent MFG algorithms to this new setting, as well as contributing a novel sub-routine allowing networked agents to estimate the global average reward from their local neighbourhood. Previous theoretical analysis of decentralised communication in MFGs does not extend trivially to MFC. We therefore contribute new theory proving that in MFC the networked communication scheme allows agents to increase social welfare faster than under both of the two typical alternative architectures, namely independent and centralised learning. We provide experiments that support this new result across different classes of cooperative game, and also give numerous ablation studies and additional experiments concerning numbers of communication round and robustness to communication failures.</p></details> |  |
| **[Networked Communication for Mean-Field Games with Function Approximation and Empirical Mean-Field Estimation](https://arxiv.org/abs/2408.11607v3)** | 2025-12-22 | <details><summary>Show</summary><p>Recent algorithms allow decentralised agents, possibly connected via a communication network, to learn equilibria in mean-field games from a non-episodic run of the empirical system. However, these algorithms are for tabular settings: this computationally limits the size of agents' observation space, meaning the algorithms cannot handle anything but small state spaces, nor generalise beyond policies depending only on the agent's local state to so-called 'population-dependent' policies. We address this limitation by introducing function approximation to the existing setting, drawing on the Munchausen Online Mirror Descent method that has previously been employed only in finite-horizon, episodic, centralised settings. While this permits us to include the mean field in the observation for players' policies, it is unrealistic to assume decentralised agents have access to this global information: we therefore also provide new algorithms allowing agents to locally estimate the global empirical distribution, and to improve this estimate via inter-agent communication. We prove theoretically that exchanging policy information helps networked agents outperform both independent and even centralised agents in function-approximation settings. Our experiments demonstrate this happening empirically, and show that the communication network allows decentralised agents to estimate the mean field for population-dependent policies.</p></details> |  |
| **[Networked Communication for Decentralised Agents in Mean-Field Games](https://arxiv.org/abs/2306.02766v6)** | 2025-12-22 | <details><summary>Show</summary><p>Methods like multi-agent reinforcement learning struggle to scale with growing population size. Mean-field games (MFGs) are a game-theoretic approach that can circumvent this by finding a solution for an abstract infinite population, which can then be used as an approximate solution for the $N$-agent problem. However, classical mean-field algorithms usually only work under restrictive conditions. We take steps to address this by introducing networked communication to MFGs, in particular to settings that use a single, non-episodic run of $N$ decentralised agents to simulate the infinite population, as is likely to be most reasonable in real-world deployments. We prove that our architecture's sample guarantees lie between those of earlier theoretical algorithms for the centralised- and independent-learning architectures, varying dependent on network structure and the number of communication rounds. However, the sample guarantees of the three theoretical algorithms do not actually result in practical convergence times. We thus contribute practical enhancements to all three algorithms allowing us to present their first empirical demonstrations. We then show that in practical settings where the theoretical hyperparameters are not observed, giving fewer loops but poorer estimation of the Q-function, our communication scheme still respects the earlier theoretical analysis: it considerably accelerates learning over the independent case, which hardly seems to learn at all, and often performs similarly to the centralised case, while removing the restrictive assumption of the latter. We provide ablations and additional studies showing that our networked approach also has advantages over both alternatives in terms of robustness to update failures and to changes in population size.</p></details> |  |
| **[DeepGESI: A Non-Intrusive Objective Evaluation Model for Predicting Speech Intelligibility in Hearing-Impaired Listeners](https://arxiv.org/abs/2512.19374v1)** | 2025-12-22 | <details><summary>Show</summary><p>Speech intelligibility assessment is essential for many speech-related applications. However, most objective intelligibility metrics are intrusive, as they require clean reference speech in addition to the degraded or processed signal for evaluation. Furthermore, existing metrics such as STOI are primarily designed for normal hearing listeners, and their predictive accuracy for hearing impaired speech intelligibility remains limited. On the other hand, the GESI (Gammachirp Envelope Similarity Index) can be used to estimate intelligibility for hearing-impaired listeners, but it is also intrusive, as it depends on reference signals. This requirement limits its applicability in real-world scenarios. To overcome this limitation, this study proposes DeepGESI, a non-intrusive deep learning-based model capable of accurately and efficiently predicting the speech intelligibility of hearing-impaired listeners without requiring any clean reference speech. Experimental results demonstrate that, under the test conditions of the 2nd Clarity Prediction Challenge(CPC2) dataset, the GESI scores predicted by DeepGESI exhibit a strong correlation with the actual GESI scores. In addition, the proposed model achieves a substantially faster prediction speed compared to conventional methods.</p></details> |  |
| **[ForeSpeed: A real-world video dataset of CCTV cameras with different settings for vehicle speed estimation](https://arxiv.org/abs/2512.19364v1)** | 2025-12-22 | <details><summary>Show</summary><p>The need to estimate the speed of road vehicles has become increasingly important in the field of video forensics, particularly with the widespread deployment of CCTV cameras worldwide. Despite the development of various approaches, the accuracy of forensic speed estimation from real-world footage remains highly dependent on several factors, including camera specifications, acquisition methods, spatial and temporal resolution, compression methods, and scene perspective, which can significantly influence performance. In this paper, we introduce ForeSpeed, a comprehensive dataset designed to support the evaluation of speed estimation techniques in real-world scenarios using CCTV footage. The dataset includes recordings of a vehicle traveling at known speeds, captured by three digital and three analog cameras from two distinct perspectives. Real-world road metrics are provided to enable the restoration of the scene geometry. Videos were stored with multiple compression factors and settings, to simulate real world scenarios in which export procedures are not always performed according to forensic standards. Overall, ForeSpeed, includes a collection of 322 videos. As a case study, we employed the ForeSpeed dataset to benchmark a speed estimation algorithm available in a commercial product (Amped FIVE). Results demonstrate that while the method reliably estimates average speed across various conditions, its uncertainty range significantly increases when the scene involves strong perspective distortion. The ForeSpeed dataset is publicly available to the forensic community, with the aim of facilitating the evaluation of current methodologies and inspiring the development of new, robust solutions tailored to collision investigation and forensic incident analysis.</p></details> |  |
| **[GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting](https://arxiv.org/abs/2412.14579v2)** | 2025-12-22 | <details><summary>Show</summary><p>Weakly-supervised 3D occupancy perception is crucial for vision-based autonomous driving in outdoor environments. Previous methods based on NeRF often face a challenge in balancing the number of samples used. Too many samples can decrease efficiency, while too few can compromise accuracy, leading to variations in the mean Intersection over Union (mIoU) by 5-10 points. Furthermore, even with surrounding-view image inputs, only a single image is rendered from each viewpoint at any given moment. This limitation leads to duplicated predictions, which significantly impacts the practicality of the approach. However, this issue has largely been overlooked in existing research. To address this, we propose GSRender, which uses 3D Gaussian Splatting for weakly-supervised occupancy estimation, simplifying the sampling process. Additionally, we introduce the Ray Compensation module, which reduces duplicated predictions by compensating for features from adjacent frames. Finally, we redesign the dynamic loss to remove the influence of dynamic objects from adjacent frames. Extensive experiments show that our approach achieves SOTA results in RayIoU (+6.0), while also narrowing the gap with 3D- supervised methods. This work lays a solid foundation for weakly-supervised occupancy perception. The code is available at https://github.com/Jasper-sudo-Sun/GSRender.</p></details> |  |
| **[NA-DiD: Extending Difference-in-Differences with Capabilities](https://arxiv.org/abs/2507.12690v2)** | 2025-12-22 | <details><summary>Show</summary><p>This paper introduces the Non-Additive Difference-in-Differences (NA-DiD) framework, which extends classical DiD by incorporating non-additive measures the Choquet integral for effect aggregation. It serves as a novel econometric tool for impact evaluation, particularly in settings with non-additive treatment effects. First, we introduce the integral representation of the classial DiD model, and then extend it to non-additive measures, therefore deriving the formulae for NA-DiD estimation. Then, we give its theoretical properties. Applying NA-DiD to a simulated hospital hygiene intervention, we find that classical DiD can overestimate treatment effects, f.e. failing to account for compliance erosion. In contrast, NA-DiD provides a more accurate estimate by incorporating non-linear aggregation. The Julia implementation of the techniques used and introduced in this article is provided in the appendices.</p></details> | <details><summary>I hav...</summary><p>I have received a reviews from a journal, and after reading them I came to the conclusion that this research does not meet novelty threshold for a paper I would like to put in public. I missed some references, which are more relevant to the problem I try to solve and offer a better solution</p></details> |
| **[TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking](https://arxiv.org/abs/2512.02789v2)** | 2025-12-22 | <details><summary>Show</summary><p>The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.</p></details> |  |
| **[A hybrid-Hill estimator enabled by heavy-tailed block maxima](https://arxiv.org/abs/2512.19338v1)** | 2025-12-22 | <details><summary>Show</summary><p>When analysing extreme values, two alternative statistical approaches have historically been held in contention: the seminal block maxima method (or annual maxima method, spurred by hydrological applications) and the peaks-over-threshold. Clamoured amongst statisticians as wasteful of potentially informative data, the block maxima method gradually fell into disfavour whilst peaks-over-threshold-based methodologies were ushered to the centre stage of extreme value statistics. This paper proposes a hybrid method which reconciles these two hitherto disconnected approaches. Appealing in its simplicity, our main result introduces a new universal limiting characterisation of extremes that eschews the customary requirement of a sufficiently large block size for the plausible block maxima-fit to an extreme value distribution. We advocate that inference should be drawn solely on larger block maxima, from which practice the mainstream peaks-over-threshold methodology coalesces. The asymptotic properties of the promised hybrid-Hill estimator herald more than its efficiency, but rather that a fully-fledged unified semi-parametric stream of statistics for extreme values is viable. A finite sample simulation study demonstrates that a reduced-bias off-shoot of the hybrid-Hill estimator fares exceptionally well against the incumbent maximum likelihood estimation that relies on a numerical fit to the entire sample of block maxima.</p></details> | 31 pages, 5 figures |
| **[Orthogonal Approximate Message Passing with Optimal Spectral Initializations for Rectangular Spiked Matrix Models](https://arxiv.org/abs/2512.19334v1)** | 2025-12-22 | <details><summary>Show</summary><p>We propose an orthogonal approximate message passing (OAMP) algorithm for signal estimation in the rectangular spiked matrix model with general rotationally invariant (RI) noise. We establish a rigorous state evolution that precisely characterizes the algorithm's high-dimensional dynamics and enables the construction of iteration-wise optimal denoisers. Within this framework, we accommodate spectral initializations under minimal assumptions on the empirical noise spectrum. In the rectangular setting, where a single rank-one component typically generates multiple informative outliers, we further propose a procedure for combining these outliers under mild non-Gaussian signal assumptions. For general RI noise models, the predicted performance of the proposed optimal OAMP algorithm agrees with replica-symmetric predictions for the associated Bayes-optimal estimator, and we conjecture that it is statistically optimal within a broad class of iterative estimation methods.</p></details> |  |
| **[The asymptotic distribution of the likelihood ratio test statistic in two-peak discovery experiments](https://arxiv.org/abs/2512.19333v1)** | 2025-12-22 | <details><summary>Show</summary><p>Likelihood ratio tests are widely used in high-energy physics, where the test statistic is usually assumed to follow a chi-squared distribution with a number of degrees of freedom specified by Wilks' theorem. This assumption breaks down when parameters such as signal or coupling strengths are restricted to be non-negative and their values under the null hypothesis lie on the boundary of the parameter space. Based on a recent clarification concerning the correct asymptotic distribution of the likelihood ratio test statistic for cases where two of the parameters are on the boundary, we revisit the the question of significance estimation for two-peak signal-plus-background counting experiments. In the high-energy physics literature, such experiments are commonly analyzed using Wilks' chi-squared distribution or the one-parameter Chernoff limit. We demonstrate that these approaches can lead to strongly miscalibrated significances, and that the test statistic distribution is instead well described by a chi-squared mixture with weights determined by the Fisher information matrix. Our results highlight the need for boundary-aware asymptotics in the analysis of two-peak counting experiments.</p></details> | 25 pages, 6 figures |
| **[High dimensional matrix estimation through elliptical factor models](https://arxiv.org/abs/2512.19325v1)** | 2025-12-22 | <details><summary>Show</summary><p>Elliptical factor models play a central role in modern high-dimensional data analysis, particularly due to their ability to capture heavy-tailed and heterogeneous dependence structures. Within this framework, Tyler's M-estimator (Tyler, 1987a) enjoys several optimality properties and robustness advantages. In this paper, we develop high-dimensional scatter matrix, covariance matrix and precision matrix estimators grounded in Tyler's M-estimation. We first adapt the Principal Orthogonal complEment Thresholding (POET) framework (Fan et al., 2013) by incorporating the spatial-sign covariance matrix as an effective initial estimator. Building on this idea, we further propose a direct extension of POET tailored for Tyler's M-estimation, referred to as the POET-TME method. We establish the consistency rates for the resulting estimators under elliptical factor models. Comprehensive simulation studies and a real data application illustrate the superior performance of POET-TME, especially in the presence of heavy-tailed distributions, demonstrating the practical value of our methodological contributions.</p></details> |  |
| **[MAGIC: Achieving Superior Model Merging via Magnitude Calibration](https://arxiv.org/abs/2512.19320v1)** | 2025-12-22 | <details><summary>Show</summary><p>The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC</p></details> |  |
| **[Joint parameter estimation and multidimensional reconciliation for continuous-variable quantum key distribution](https://arxiv.org/abs/2508.05558v2)** | 2025-12-22 | <details><summary>Show</summary><p>Accurate quantum channel parameter estimation is essential for effective information reconciliation in continuous-variable quantum key distribution (CV-QKD). However, conventional maximum likelihood (ML) estimators rely on a large amount of discarded data (or pilot symbols), leading to a significant loss in symbol efficiency. Moreover, the separation between the estimation and reconciliation phases can introduce error propagation. In this paper, we propose a novel joint message-passing scheme that unifies channel parameter estimation and information reconciliation within a Bayesian framework. By leveraging the expectation-maximization (EM) algorithm, the proposed method simultaneously estimates unknown parameters during decoding, eliminating the need for separate ML estimation. Furthermore, we introduce a hybrid multidimensional rotation scheme that removes the requirement for norm feedback, significantly reducing classical channel overhead. To the best of our knowledge, this is the first work to unify multidimensional reconciliation and channel parameter estimation in CV-QKD, providing a practical solution for high-efficiency reconciliation with minimal pilots.</p></details> | 11 pages, 6 figures |
| **[Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market](https://arxiv.org/abs/2503.04521v2)** | 2025-12-22 | <details><summary>Show</summary><p>The convergence of edge computing and Artificial Intelligence (AI) gives rise to Edge-AI, which enables the deployment of real-time AI applications at the network edge. A key research challenge in Edge-AI is edge inference acceleration, which aims to realize low-latency high-accuracy Deep Neural Network (DNN) inference by offloading partitioned inference tasks from end devices to edge servers. However, existing research has yet to adopt a practical Edge-AI market perspective, which would explore the personalized inference needs of AI users (e.g., inference accuracy, latency, and task complexity), the revenue incentives for AI service providers that offer edge inference services, and multi-stakeholder governance within a market-oriented context. To bridge this gap, we propose an Auction-based Edge Inference Pricing Mechanism (AERIA) for revenue maximization to tackle the multi-dimensional optimization problem of DNN model partition, edge inference pricing, and resource allocation. We develop a multi-exit device-edge synergistic inference scheme for on-demand DNN inference acceleration, and theoretically analyze the auction dynamics amongst the AI service providers, AI users and edge infrastructure provider. Owing to the strategic mechanism design via randomized consensus estimate and cost sharing techniques, the Edge-AI market attains several desirable properties. These include competitiveness in revenue maximization, incentive compatibility, and envy-freeness, which are crucial to maintain the effectiveness, truthfulness, and fairness in auction outcomes. Extensive simulations based on four representative DNN inference workloads demonstrate that AERIA significantly outperforms several state-of-the-art approaches in revenue maximization. This validates the efficacy of AERIA for on-demand DNN inference in the Edge-AI market.</p></details> | <details><summary>Accep...</summary><p>Accepted for publication in IEEE Transactions on Mobile Computing. Index Terms: Edge-AI, DNN Inference Offloading, Resource Management, Dynamic Pricing, Auction Mechanism</p></details> |
| **[Local smoothing and maximal estimates for average over surfaces of codimension 2 in $\mathbb R^4$](https://arxiv.org/abs/2507.22695v2)** | 2025-12-22 | <details><summary>Show</summary><p>In this paper, we obtain local smoothing estimates for the averages over nondegenerate surfaces of codimension $2$ in $\mathbb R^4$. We make use of multilinear restriction estimates and decoupling inequalities for a hypersurface in $\mathbb R^5$, a conical extension of a two-dimensional nondegenerate surface along two flat directions. We also establish sharp $L^p$--$L^q$ estimates for maximal averages over nondegenerate surfaces of half the ambient dimension in $\mathbb R^{2n}$ for even $n \ge 2$.</p></details> | <details><summary>32 pa...</summary><p>32 pages, The title was changed, and the introduction was expanded</p></details> |
| **[Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context](https://arxiv.org/abs/2512.19283v1)** | 2025-12-22 | <details><summary>Show</summary><p>Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.</p></details> | <details><summary>Proje...</summary><p>Project Page: https://kyungwoncho.github.io/HaMoS/</p></details> |
| **[Spearman's rho for zero-inflated count data: formulation and attainable bounds](https://arxiv.org/abs/2503.13148v2)** | 2025-12-22 | <details><summary>Show</summary><p>We propose an alternative formulation of Spearman's rho for zero-inflated count data. The formulation yields an estimator with explicitly attainable bounds, facilitating interpretation in settings where the standard range [-1,1] is no longer informative.</p></details> |  |
| **[On the large time behavior of the 2D inhomogeneous incompressible viscous flows](https://arxiv.org/abs/2512.19281v1)** | 2025-12-22 | <details><summary>Show</summary><p>This paper studies the two-dimensional inhomogeneous Navier--Stokes equations governing stratified flows in a bounded domain under a gravitational potential \(f\). Our main results are as follows. First, we provide a rigorous characterization of steady states, proving that under the Dirichlet condition \(\mathbf{u}|_{\partial Ω} = \mathbf{0}\), all admissible equilibria are hydrostatic and satisfy \(\nabla p_s = -ρ_s \nabla f\). Second, through a perturbative analysis around arbitrary hydrostatic profiles, we show that despite possible transient growth induced by the Rayleigh--Taylor mechanism, the system always relaxes to a hydrostatic equilibrium. Third, we identify a necessary and sufficient condition on the initial density perturbation for convergence to a linear hydrostatic density profile of the form \(ρ_s = -γf + β\), with \(γ> 0\) and \(β> 0\). Finally, we establish improved regularity estimates for strong solutions corresponding to initial data in the Sobolev space \(H^3(Ω)\).</p></details> |  |
| **[Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals](https://arxiv.org/abs/2512.19280v1)** | 2025-12-22 | <details><summary>Show</summary><p>Axial piston pumps are crucial components in fluid power systems, where reliable fault diagnosis is essential for ensuring operational safety and efficiency. Traditional data-driven methods require extensive labeled fault data, which is often impractical to obtain, while model-based approaches suffer from parameter uncertainties. This paper proposes a digital twin (DT)-driven zero-shot fault diagnosis framework utilizing fluid-borne noise (FBN) signals. The framework calibrates a high-fidelity DT model using only healthy-state data, generates synthetic fault signals for training deep learning classifiers, and employs a physics-informed neural network (PINN) as a virtual sensor for flow ripple estimation. Gradient-weighted class activation mapping (Grad-CAM) is integrated to visualize the decision-making process of neural networks, revealing that large kernels matching the subsequence length in time-domain inputs and small kernels in time-frequency domain inputs enable higher diagnostic accuracy by focusing on physically meaningful features. Experimental validations demonstrate that training on signals from the calibrated DT model yields diagnostic accuracies exceeding 95\% on real-world benchmarks, while uncalibrated models result in significantly lower performance, highlighting the framework's effectiveness in data-scarce scenarios.</p></details> |  |
| **[Scale-Invariant Robust Estimation of High-Dimensional Kronecker-Structured Matrices](https://arxiv.org/abs/2512.19273v1)** | 2025-12-22 | <details><summary>Show</summary><p>High-dimensional Kronecker-structured estimation faces a conflict between non-convex scaling ambiguities and statistical robustness. The arbitrary factor scaling distorts gradient magnitudes, rendering standard fixed-threshold robust methods ineffective. We resolve this via Scaled Robust Gradient Descent (SRGD), which stabilizes optimization by de-scaling gradients before truncation. To further enforce interpretability, we introduce Scaled Hard Thresholding (SHT) for invariant variable selection. A two-step estimation procedure, built upon robust initialization and SRGD--SHT iterative updates, is proposed for canonical matrix problems, such as trace regression, matrix GLMs, and bilinear models. The convergence rates are established for heavy-tailed predictors and noise, identifying a phase transition where optimal convergence rates recover under finite noise variance and degrade optimally for heavier tails. Experiments on simulated data and two real-world applications confirm superior robustness and efficiency of the proposed procedure.</p></details> | 85 pages, 11 figues |
| **[Precise control of high-frequency ultrasounds in thin crystals for the development of tunable narrowband and directional gamma-ray sources](https://arxiv.org/abs/2512.19268v1)** | 2025-12-22 | <details><summary>Show</summary><p>This work presents a complete methodology for the precise characterization of the acoustic field inside crystal-based devices driven by high-frequency ultrasounds towards the generation of tunable narrowband and directional gamma radiation via undulation of ultra-relativistic charged particles. Such gamma-ray sources have long been anticipated by the scientific community, as they promise new powerful tools for the study of high-energy physical phenomena and the development of novel nuclear technologies. In such devices, a piezoelectric transducer induces tens of MHz harmonic waves inside a silicon monocrystal. Ultra-relativistic charged particles traversing the crystal get trapped within the channels formed by the extremely strong electric fields of the acoustically modulated lattice planes, undergoing undulation and emitting gamma radiation. Precise characterization of the acoustic field in the crystal is crucial for the determination of the expected characteristics of the secondarily generated gamma rays. For this purpose, fast laser refraction imaging is used here to image the acoustic waves by exploiting the spatial redistribution of a laser beam optical intensity caused by the acoustic field. A dedicated computational model is developed for the estimation of the spatial distribution of the pressure and lattice deformation inside the crystal. This methodology provides a framework for future novel gamma-ray sources in high-energy facilities.</p></details> |  |
| **[DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method](https://arxiv.org/abs/2508.17054v3)** | 2025-12-22 | <details><summary>Show</summary><p>Previous dominant methods for scene flow estimation focus mainly on input from two consecutive frames, neglecting valuable information in the temporal domain. While recent trends shift towards multi-frame reasoning, they suffer from rapidly escalating computational costs as the number of frames grows. To leverage temporal information more efficiently, we propose DeltaFlow ($Δ$Flow), a lightweight 3D framework that captures motion cues via a $Δ$ scheme, extracting temporal features with minimal computational cost, regardless of the number of frames. Additionally, scene flow estimation faces challenges such as imbalanced object class distributions and motion inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to enhance learning across underrepresented classes and an Instance Consistency Loss to enforce coherent object motion, improving flow accuracy. Extensive evaluations on the Argoverse 2, Waymo and nuScenes datasets show that $Δ$Flow achieves state-of-the-art performance with up to 22% lower error and $2\times$ faster inference compared to the next-best multi-frame supervised method, while also demonstrating a strong cross-domain generalization ability. The code is open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model weights.</p></details> | <details><summary>NeurI...</summary><p>NeurIPS 2025 Spotlight, 18 pages (10 main pages + 8 supp materail), 11 figures, code at https://github.com/Kin-Zhang/DeltaFlow</p></details> |
| **[Pointwise bounds on Dirichlet Green's functions for a singular drift term](https://arxiv.org/abs/2511.12741v2)** | 2025-12-22 | <details><summary>Show</summary><p>We introduce a technique to obtain pointwise upper and lower bounds for the Green's function of elliptic operators whose principal part is the Laplacian and that include a drift term diverging near the boundary like a power of the inverse distance with exponent less than 1, in the unit ball B(0,1) \subset \mathbb{R}^n, n \ge 3. The constants in the upper estimates are uniform in B(0,r) for each r < 1, with explicit dependence on r. The drift here belongs to C^{1,α}_{\mathrm{loc}} and may, more generally, be majorized by a function radially integrable up to the boundary. These appear to be the first such estimates for non-coercive drifts and remain new even for smooth drifts, suggesting extensions to singular potentials and other settings where energy methods fail.</p></details> | Fixed several typos |
| **[Estimation of Population Linear Spectral Statistics by Marchenko--Pastur Inversion](https://arxiv.org/abs/2504.03390v4)** | 2025-12-22 | <details><summary>Show</summary><p>A new method of estimating population linear spectral statistics from high-dimensional data is introduced. When the dimension $d$ grows with the sample size $n$ such that $\frac{d}{n} \to c>0$, the proposed method is the first with proven convergence rate of $\mathcal{O}(n^{\varepsilon - 1})$ for any $\varepsilon > 0$ in a general nonparametric setting. For Gaussian data, a CLT for the estimation error with normalization factor $n$ is shown.</p></details> | 67 pages, 10 figures |

## Sound DOA estimation
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning](https://arxiv.org/abs/2512.19687v1)** | 2025-12-22 | <details><summary>Show</summary><p>We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.</p></details> |  |
| **[Partition Function Estimation Using Analog Quantum Processors](https://arxiv.org/abs/2512.19685v1)** | 2025-12-22 | <details><summary>Show</summary><p>We evaluate using programmable superconducting flux qubit D-Wave quantum annealers to approximate the partition function of Ising models. We propose the use of two distinct quantum annealer sampling methods: chains of Monte Carlo-like reverse quantum anneals, and standard linear-ramp quantum annealing. The control parameters used to attenuate the quality of the simulations are the effective analog energy scale of the J coupling, the total annealing time, and for the case of reverse annealing the anneal-pause. The core estimation technique is to sample across the energy spectrum of the classical Hamiltonian of interest, and therefore obtain a density of states estimate for each energy level, which in turn can be used to compute an estimate of the partition function with some sampling error. This estimation technique is powerful because once the distribution is sampled it allows thermodynamic quantity computation at arbitrary temperatures. On a $25$ spin $\pm J$ hardware graph native Ising model we find parameter regimes of the D-Wave processors that provide comparable result quality to two standard classical Monte Carlo methods, Multiple Histogram Reweighting and Wang-Landau. Remarkably, we find that fast quench-like anneals can quickly generate ensemble distributions that are very good estimates of the true partition function of the classical Ising model; on a Pegasus graph-structured QPU we report a logarithmic relative error of $7.6 \times 10^{-6}$, from $171,000$ samples generated using $0.2$ seconds of QPU time with an anneal time of $8$ nanoseconds per sample which is interestingly within the closed system dynamics timescale of the superconducting qubits.</p></details> |  |
| **[An Adaptive Graphical Lasso Approach to Modeling Symptom Networks of Common Mental Disorders in Eritrean Refugee Population](https://arxiv.org/abs/2512.19681v1)** | 2025-12-22 | <details><summary>Show</summary><p>Despite the significant public health burden of common mental disorders (CMDs) among refugee populations, their underlying symptom structures remain underexplored. This study uses Gaussian graphical modeling to examine the symptom network of post-traumatic stress disorder (PTSD), depression, anxiety, and somatic distress among Eritrean refugees in the Greater Washington, DC area. Given the small sample size (n) and high-dimensional symptom space (p), we propose a novel extension of the standard graphical LASSO by incorporating adaptive penalization, which improves sparsity selection and network estimation stability under n < p conditions. To evaluate the reliability of the network, we apply bootstrap resampling and use centrality measures to identify the most influential symptoms. Our analysis identifies six distinct symptom clusters, with somatic-anxiety symptoms forming the most interconnected group. Notably, symptoms such as nausea and reliving past experiences emerge as central symptoms linking PTSD, anxiety, depression, and somatic distress. Additionally, we identify symptoms like feeling fearful, sleep problems, and loss of interest in activities as key symptoms, either being closely positioned to many others or acting as important bridges that help maintain the overall network connectivity, thereby highlighting their potential importance as possible intervention targets.</p></details> | 34 pages, 7 figures |
| **[FMCW Radar Principles and Human Activity Recognition Systems: Foundations, Techniques, and Applications](https://arxiv.org/abs/2410.08483v2)** | 2025-12-22 | <details><summary>Show</summary><p>This book introduces the theoretical foundations of FMCW radar systems, including range and velocity estimation, signal processing techniques, and the generation of radar point clouds. A detailed discussion of Python and MATLAB as the primary programming tools for radar signal processing is provided, including the integration of libraries like NumPy, Matplotlib, and SciPy for data analysis and visualization. In addition, the book covers advanced techniques such as deep learning applications for radar signal processing, focusing on Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks, and Transformers for analyzing radar data. Furthermore, it highlights state-of-the-art methods for human activity recognition using radar, leveraging a combination of traditional signal processing techniques and machine learning models. The book is designed to cater to both beginners and experts in radar signal processing, offering practical examples, code implementations, and insights into the future of radar technology in various domains, including autonomous systems and security applications.</p></details> | 203pages |
| **[Deep Legendre Transform](https://arxiv.org/abs/2512.19649v1)** | 2025-12-22 | <details><summary>Show</summary><p>We introduce a novel deep learning algorithm for computing convex conjugates of differentiable convex functions, a fundamental operation in convex analysis with various applications in different fields such as optimization, control theory, physics and economics. While traditional numerical methods suffer from the curse of dimensionality and become computationally intractable in high dimensions, more recent neural network-based approaches scale better, but have mostly been studied with the aim of solving optimal transport problems and require the solution of complicated optimization or max-min problems. Using an implicit Fenchel formulation of convex conjugation, our approach facilitates an efficient gradient-based framework for the minimization of approximation errors and, as a byproduct, also provides a posteriori error estimates for the approximation quality. Numerical experiments demonstrate our method's ability to deliver accurate results across different high-dimensional examples. Moreover, by employing symbolic regression with Kolmogorov--Arnold networks, it is able to obtain the exact convex conjugates of specific convex functions.</p></details> | <details><summary>Accep...</summary><p>Accepted at NeurIPS 2025 (poster). NeurIPS page: https://neurips.cc/virtual/2025/loc/san-diego/poster/120307</p></details> |
| **[On the class of exponential statistical structures of type B](https://arxiv.org/abs/2510.26863v2)** | 2025-12-22 | <details><summary>Show</summary><p>The article is devoted to the study of exponential statistical structures of type B, which constitute a subclass of exponential families of probability distributions. This class is characterized by a number of analytical and probabilistic properties that make it a convenient tool for solving both theoretical and applied problems in statistics. The relevance of this research lies in the need to generalize known classes of distributions and to develop a unified framework for their analysis, which is essential for applications in stochastic modeling, machine learning, financial mathematics. The paper proposes a formal definition of type B. Necessary and sufficient conditions for a statistical structure to belong to class B are established, and it is proved that such structures can be represented through a dominating measure with an explicit Laplace transform. The obtained results make it possible to describe a wide range of well-known one-dimensional and multivariate distributions, including the Binomial, Poisson, Normal, Gamma, Polynomial, Logarithmic distributions, as well as specific cases such as the Borel-Tanner and Random Walk distributions. Particular attention is given to the proof of structural theorems that determine the stability of class B under linear transformations and the addition of independent random vectors. Recursive relations for initial and central moments as well as for semi-invariants are obtained, providing an efficient analytical and computational framework for their evaluation. Furthermore, the tails of type B distributions are investigated using the properties of the Laplace transform. New exponential inequalities for estimating the probabilities of large deviations are derived. The obtained results can be applied in theoretical studies and in practical problems of stochastic modeling.</p></details> |  |
| **[Milstein-type Schemes for Hyperbolic SPDEs](https://arxiv.org/abs/2512.19647v1)** | 2025-12-22 | <details><summary>Show</summary><p>This article studies the temporal approximation of hyperbolic semilinear stochastic evolution equations with multiplicative Gaussian noise by Milstein-type schemes. We take the term hyperbolic to mean that the leading operator generates a contractive, not necessarily analytic $C_0$-semigroup. Optimal convergence rates are derived for the pathwise uniform strong error \[ E_h^\infty := \Big(\mathbb{E}\max_{1\le j \le M}\|U_{t_j}-u_j\|_X^p\Big)^{1/p} \] on a Hilbert space $X$ for $p\in [2,\infty)$. Here, $U$ is the mild solution and $u_j$ its Milstein approximation at time $t_j=jh$ with step size $h>0$ and final time $T=Mh>0$. For sufficiently regular nonlinearity and noise, we establish strong convergence of order one, with the error satisfying $E_h^\infty\lesssim h\sqrt{\log(T/h)}$ for rational Milstein schemes and $E_h^\infty \lesssim h$ for exponential Milstein schemes. This extends previous results from parabolic to hyperbolic SPDEs and from exponential to rational Milstein schemes. Root-mean-square error estimates are strengthened to pathwise uniform estimates. Numerical experiments validate the convergence rates for the stochastic Schrödinger equation. Further applications to Maxwell's and transport equations are included.</p></details> | <details><summary>47 pa...</summary><p>47 pages, 3 figure, 4 tables. Comments are welcome!</p></details> |
| **[The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference](https://arxiv.org/abs/2512.19643v1)** | 2025-12-22 | <details><summary>Show</summary><p>Numerical simulation of time-dependent partial differential equations (PDEs) is central to scientific and engineering applications, but high-fidelity solvers are often prohibitively expensive for long-horizon or time-critical settings. Neural operator (NO) surrogates offer fast inference across parametric and functional inputs; however, most autoregressive NO frameworks remain vulnerable to compounding errors, and ensemble-averaged metrics provide limited guarantees for individual inference trajectories. In practice, error accumulation can become unacceptable beyond the training horizon, and existing methods lack mechanisms for online monitoring or correction. To address this gap, we propose ANCHOR (Adaptive Numerical Correction for High-fidelity Operator Rollouts), an online, instance-aware hybrid inference framework for stable long-horizon prediction of nonlinear, time-dependent PDEs. ANCHOR treats a pretrained NO as the primary inference engine and adaptively couples it with a classical numerical solver using a physics-informed, residual-based error estimator. Inspired by adaptive time-stepping in numerical analysis, ANCHOR monitors an exponential moving average (EMA) of the normalized PDE residual to detect accumulating error and trigger corrective solver interventions without requiring access to ground-truth solutions. We show that the EMA-based estimator correlates strongly with the true relative L2 error, enabling data-free, instance-aware error control during inference. Evaluations on four canonical PDEs: 1D and 2D Burgers', 2D Allen-Cahn, and 3D heat conduction, demonstrate that ANCHOR reliably bounds long-horizon error growth, stabilizes extrapolative rollouts, and significantly improves robustness over standalone neural operators, while remaining substantially more efficient than high-fidelity numerical solvers.</p></details> | 18 pages, 7 figures |
| **[Quantum Imaging of Birefringent Samples using Hong-Ou-Mandel Interference](https://arxiv.org/abs/2512.19637v1)** | 2025-12-22 | <details><summary>Show</summary><p>Two-photon interference in a Hong-Ou-Mandel (HOM) interferometer can be used as a quantum sensing mechanism due to the sensitivity of the interference dip to perturbations of the photon indistinguishability. In particular, recent works have generalized this concept to microscopy setups, but the sensitivity to optical path differences constrains its application to samples with thickness variation typically below a few micrometers if tracking changes in the coincidences at a fixed delay. Extending the concept to polarization microscopy and circumventing this limitation, this manuscript explores the use of a narrowband photon pair source with coherence length >1 mm to broaden the HOM dip. Thus, realistic sample-thickness variations introduce negligible temporal distinguishability, and changes in coincidence rate at the dip centre are then dominated by sample-induced polarization effects. To compute the polarization rotation, we develop a statistical model for the interferometer, derive the Fisher information, and establish a maximum-likelihood estimator for the local fast-axis angle. Recording dip and baseline frames at each sample position via raster scanning, the experimental results validate the framework, agreeing with classical polarized-intensity images while demonstrating operation at a maximum-precision regime and insensitiveness to layer thickness. Overall, the approach enclosed provides a quantum-based quantitative imaging of birefringent structures, which can motivate further advantageous applications, including enhanced signal-to-noise ratio and lower damage imaging of photosensitive samples.</p></details> |  |
| **[Approximate co-sufficient sampling with regularization](https://arxiv.org/abs/2309.08063v3)** | 2025-12-22 | <details><summary>Show</summary><p>In this work, we consider the problem of goodness-of-fit (GoF) testing for parametric models. This testing problem involves a composite null hypothesis, due to the unknown values of the model parameters. In some special cases, co-sufficient sampling (CSS) can remove the influence of these unknown parameters via conditioning on a sufficient statistic -- often, the maximum likelihood estimator (MLE) of the unknown parameters. However, many common parametric settings do not permit this approach, since conditioning on a sufficient statistic leads to a powerless test. The recent approximate co-sufficient sampling (aCSS) framework of Barber and Janson (2022) offers an alternative, replacing sufficiency with an approximately sufficient statistic (namely, a noisy version of the MLE). This approach recovers power in a range of settings where CSS cannot be applied, but can only be applied in settings where the unconstrained MLE is well-defined and well-behaved, which implicitly assumes a low-dimensional regime. In this work, we extend aCSS to the setting of constrained and penalized maximum likelihood estimation, so that more complex estimation problems can now be handled within the aCSS framework, including examples such as mixtures-of-Gaussians (where the unconstrained MLE is not well-defined due to degeneracy) and high-dimensional Gaussian linear models (where the MLE can perform well under regularization, such as an $\ell_1$ penalty or a shape constraint).</p></details> |  |
| **[LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry](https://arxiv.org/abs/2512.19629v1)** | 2025-12-22 | <details><summary>Show</summary><p>Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the \href{https://steinate.github.io/logoplanner.github.io/}{project page}.</p></details> | <details><summary>Proje...</summary><p>Project page:https://steinate.github.io/logoplanner.github.io/</p></details> |
| **[Quantum Listenings -- Amateur Sonification of Vacuum and other Noises](https://arxiv.org/abs/2507.08813v2)** | 2025-12-22 | <details><summary>Show</summary><p>The sensory perceptions of vision and sound may be considered as complementary doorways towards interpreting and understanding physical phenomena. We provide a few selected samples where scientific data of systems usually not directly accessible to humans may be listened to. The examples are chosen close to the regime where quantum mechanics is applicable. Visual and auditory renderings are compared with some connections to music, illustrating in particular a kind of fractal complexity along the time axis.</p></details> | <details><summary>versi...</summary><p>version after review, 10 pages, to appear in Eur. Phys. J. Web Conf. (2026) proceedings of conference "Complexity and Disorder" (Paris, Jan 2025)</p></details> |
| **[Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior](https://arxiv.org/abs/2512.19584v1)** | 2025-12-22 | <details><summary>Show</summary><p>Dynamic PET enables the quantitative estimation of physiology-related parameters and is widely utilized in research and increasingly adopted in clinical settings. Parametric imaging in dynamic PET requires kinetic modeling to estimate voxel-wise physiological parameters based on specific kinetic models. However, parametric images estimated through kinetic model fitting often suffer from low image quality due to the inherently ill-posed nature of the fitting process and the limited counts resulting from non-continuous data acquisition across multiple bed positions in whole-body PET. In this work, we proposed a diffusion model-based kinetic modeling framework for parametric image estimation, using the Patlak model as an example. The score function of the diffusion model was pre-trained on static total-body PET images and served as a prior for both Patlak slope and intercept images by leveraging their patch-wise similarity. During inference, the kinetic model was incorporated as a data-consistency constraint to guide the parametric image estimation. The proposed framework was evaluated on total-body dynamic PET datasets with different dose levels, demonstrating the feasibility and promising performance of the proposed framework in improving parametric image quality.</p></details> | 10 pages, 9 figures |
| **[A modified Brinkman penalization fictitious domain method for the unsteady Navier-Stokes equations](https://arxiv.org/abs/2512.19580v1)** | 2025-12-22 | <details><summary>Show</summary><p>This paper investigates a modification of the fictitious domain method with continuation in the lower-order coefficients for the unsteady Navier-Stokes equations governing the motion of an incompressible homogeneous fluid in a bounded 2D or 3D domain. The modification enables {a solution-dependent} choice of the critical parameter. Global-in-time existence and convergence of a weak solution to the auxiliary problem are proved, and local-in-time existence and convergence of a unique strong solution are established. For the strong solution, a new higher-order convergence rate estimate in the penalization parameter is obtained. The introduced framework allows us to apply a pointwise divergence free finite element method as a discretization technique, leading to strongly mass conservative discrete fictitious domain method. A numerical example illustrates the performance of the method.</p></details> |  |
| **[Deep Learning for Primordial $B$-mode Extraction](https://arxiv.org/abs/2512.19577v1)** | 2025-12-22 | <details><summary>Show</summary><p>The search for primordial gravitational waves is a central goal of cosmic microwave background (CMB) surveys. Isolating the characteristic $B$-mode polarization signal sourced by primordial gravitational waves is challenging for several reasons: the amplitude of the signal is inherently small; astrophysical foregrounds produce $B$-mode polarization contaminating the signal; and secondary $B$-mode polarization fluctuations are produced via the conversion of $E$ modes. Current and future low-noise, multi-frequency observations enable sufficient precision to address the first two of these challenges such that secondary $B$ modes will become the bottleneck for improved constraints on the amplitude of primordial gravitational waves. The dominant source of secondary $B$-mode polarization is gravitational lensing by large scale structure. Various strategies have been developed to estimate the lensing deflection and to reverse its effects the CMB, thus reducing confusion from lensing $B$ modes in the search for primordial gravitational waves. However, a few complications remain. First, there may be additional sources of secondary $B$-mode polarization, for example from patchy reionization or from cosmic polarization rotation. Second, the statistics of delensed CMB maps can become complicated and non-Gaussian, especially when advanced lensing reconstruction techniques are applied. We previously demonstrated how a deep learning network, ResUNet-CMB, can provide nearly optimal simultaneous estimates of multiple sources of secondary $B$-mode polarization. In this paper, we show how deep learning can be applied to estimate and remove multiple sources of secondary $B$-mode polarization, and we further show how this technique can be used in a likelihood analysis to produce nearly optimal, unbiased estimates of the amplitude of primordial gravitational waves.</p></details> | <details><summary>12 pa...</summary><p>12 pages, 8 figures. Code available from https://github.com/EEmGuzman/resunet-cmb</p></details> |
| **[Degeneration of Riemann surfaces and small eigenvalues of the Laplacian](https://arxiv.org/abs/2509.06151v2)** | 2025-12-22 | <details><summary>Show</summary><p>For a one-parameter degeneration of compact Riemann surfaces endowed with the Kähler metric induced from the Kähler metric on the total space of the family, we determine the exact magnitude of the small eigenvalues of the Laplacian as a function on the parameter space, under the assumption that the singular fiber is reduced. The novelty in our approach is that we compute the asymptotic behavior of certain difference of (logarithm of) analytic torsions in the degeneration in two ways. On the one hand, via heat kernel estimates, it is shown that the leading asymptotic is determined by the product of the small eigenvalues. On the other hand, using Quillen metrics, the leading asymptotic is connected with the period integrals, which we explicitly evaluate.</p></details> | 45 pages |
| **[Optimal Uncertainty Quantification under General Moment Constraints on Input Subdomains](https://arxiv.org/abs/2512.19572v1)** | 2025-12-22 | <details><summary>Show</summary><p>We present an optimal uncertainty quantification (OUQ) framework for systems whose uncertain inputs are characterized by truncated moment constraints defined over subdomains. Based on this partial information, rigorous optimal upper and lower bounds on the probability of failure (PoF) are derived over the admissible set of probability measures, providing a principled basis for system safety certification. We formulate the OUQ problem under general subdomain moment constraints and develop a high-performance computational framework to compute the optimal bounds. This approach transforms the original infinite-dimensional optimization problems into finite-dimensional unconstrained ones parameterized solely by free canonical moments. To address the prohibitive cost of PoF evaluation in high-dimensional settings, we incorporate inverse transform sampling (ITS), enabling efficient and accurate PoF estimation within the OUQ optimization. We also demonstrate that constraining inputs only by zeroth-order moments over subdomains yields a formulation equivalent to evidence theory. Three groups of numerical examples demonstrate the framework's effectiveness and scalability. Results show that increasing the number of subdomains or the moment order systematically tightens the bound interval. For high-dimensional problems, the ITS strategy reduces computational costs by up to two orders of magnitude while maintaining relative error below 1%. Furthermore, we identify regimes where optimal bounds are sensitive to subdomain partitioning or higher-order moments, guiding uncertainty reduction efforts for safety certification.</p></details> | 31 pages, 13 figures |
| **[Owning the Intelligence: Global AI Patents Landscape and Europe's Quest for Technological Sovereignty](https://arxiv.org/abs/2512.19569v1)** | 2025-12-22 | <details><summary>Show</summary><p>Artificial intelligence has become a key arena of global technological competition and a central concern for Europe's quest for technological sovereignty. This paper analyzes global AI patenting from 2010 to 2023 to assess Europe's position in an increasingly bipolar innovation landscape dominated by the United States and China. Using linked patent, firm, ownership, and citation data, we examine the geography, specialization, and international diffusion of AI innovation. We find a highly concentrated patent landscape: China leads in patent volumes, while the United States dominates in citation impact and technological influence. Europe accounts for a limited share of AI patents but exhibits signals of relatively high patent quality. Technological proximity reveals global convergence toward U.S. innovation trajectories, with Europe remaining fragmented rather than forming an autonomous pole. Gravity-model estimates show that cross-border AI knowledge flows are driven primarily by technological capability and specialization, while geographic and institutional factors play a secondary role. EU membership does not significantly enhance intra-European knowledge diffusion, suggesting that technological capacity, rather than political integration, underpins participation in global AI innovation networks.</p></details> |  |
| **[Equivalences in diagrammatic sets](https://arxiv.org/abs/2410.00123v2)** | 2025-12-22 | <details><summary>Show</summary><p>We show that diagrammatic sets, a topologically sound alternative to polygraphs and strict $ω$-categories, admit an internal notion of equivalence in the sense of coinductive weak invertibility. We prove that equivalences have the expected properties: they include all degenerate cells, are closed under 2-out-of-3, and satisfy an appropriate version of the "division lemma", which ensures that enwrapping a diagram with equivalences at all sides is an invertible operation up to higher equivalence. On the way to this result, we develop methods, such as an algebraic calculus of natural equivalences, for handling the weak units and unitors which set this framework apart from strict $ω$-categories.</p></details> | <details><summary>43 pa...</summary><p>43 pages. v2: Manuscript accepted for publication in Journal of Pure and Applied Algebra</p></details> |
| **[A Statistical Framework for Understanding Causal Effects that Vary by Treatment Initiation Time in EHR-based Studies](https://arxiv.org/abs/2512.19553v1)** | 2025-12-22 | <details><summary>Show</summary><p>Comparative effectiveness studies using electronic health records (EHR) consider data from patients who could ``enter'' the study cohort at any point during an interval that spans many years in calendar time. Unlike treatments in tightly controlled trials, real-world treatments can evolve over calendar time, especially if comparators include standard of care, or procedures where techniques may improve. Efforts to assess whether treatment efficacy itself is changing are complicated by changing patient populations, with potential covariate shift in key effect modifiers. In this work, we propose a statistical framework to estimate calendar-time specific average treatment effects and describe both how and why effects vary across treatment initiation time in EHR-based studies. Our approach projects doubly robust, time-specific treatment effect estimates onto candidate marginal structural models and uses a model selection procedure to best describe how effects vary by treatment initiation time. We further introduce a novel summary metric, based on standardization analysis, to quantify the role of covariate shift in explaining observed effect changes and disentangle changes in treatment effects from changes in the patient population receiving treatment. Extensive simulations using EHR data from Kaiser Permanente are used to validate the utility of the framework, which we apply to study changes in relative weight loss following two bariatric surgical interventions versus no surgery among patients with severe obesity between 2005-2011.</p></details> |  |
| **[Yang-Mills energy quantization over non-collapsed degenerating Einstein manifolds and applications](https://arxiv.org/abs/2512.19552v1)** | 2025-12-22 | <details><summary>Show</summary><p>We investigate a sequence of Yang-Mills connections $A_j$ lying in vector bundles $E_j$ over non-collapsed degenerating closed Einstein 4-manifolds $(M_j, g_ j)$ with uniformly bounded Einstein constants and bounded diameters. We establish a compactness theory modular three types of bubbles. As applications, we get some quantization results for several important topological number associated with the vector bundles, for instance, the first Pontrjagin numbers $p_1(E)$ of vector bundles over Einstein 4-manifolds and the Euler numbers $χ(M;E)$ of holomorphic vector bundles over Kähler-Einstein surfaces. Furthermore, we get some quantization results about the volume $v(L_j)$ and certain cohomological numbers (e.g. $dim H^0(M_j;L_j)$) of holomorphic line bundles $L_j$ over non-collapsed degenerating Kähler-Einstein surfaces $(M_j,J_j,g_j)$ with the aid of the classical vanishing theorems, the classical Hirzebruch-Riemann-Roch type theorems, and the profound convergence theory of Kähler-Einstein manifolds. In particular, we obtain some interesting identities involving non-collapsed degenerating compact Kähler-Einstein surfaces with non-zero scalar curvature, which indicate that we can know the Euler number of $M_j$ for large $j$ provided some topological information of the limit orbifold $M_\infty$. For Kähler-Einstein Del Pezzo surfaces, an interesting implication is that we can provide some preliminary estimates for the number of singularities of various types in $M_\infty$ in an effective way. As an unexpected surprise, we find an identity which connects Milnor numbers for singularities in $M_\infty$ and the correction terms in the Hirzebruch-Riemann-Roch theorem for orbifolds. Some quantization results can be extended to the case of higher dimensional $n$-manifolds.</p></details> | Comments welcome! |
| **[A Reynolds-semi-robust method with hybrid velocity and pressure for the unsteady incompressible Navier--Stokes equations](https://arxiv.org/abs/2502.15293v2)** | 2025-12-22 | <details><summary>Show</summary><p>In this paper we propose and analyze a new Finite Element method for the solution of the two- and three-dimensional incompressible Navier--Stokes equations based on a hybrid discretization of both the velocity and pressure variables. The proposed method is pressure-robust, i.e., irrotational forcing terms do not affect the approximation of the velocity, and Reynolds-quasi-robust, with error estimates that, for smooth enough exact solutions, do not depend on the inverse of the viscosity. We carry out an in-depth convergence analysis highlighting pre-asymptotic convergence rates and validate the theoretical findings with a complete set of numerical experiments.</p></details> |  |
| **[QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models](https://arxiv.org/abs/2512.19526v1)** | 2025-12-22 | <details><summary>Show</summary><p>Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.</p></details> |  |
| **[Estimating Spatially Resolved Radiation Fields Using Neural Networks](https://arxiv.org/abs/2512.17654v2)** | 2025-12-22 | <details><summary>Show</summary><p>We present an in-depth analysis on how to build and train neural networks to estimate the spatial distribution of scattered radiation fields for radiation protection dosimetry in medical radiation fields, such as those found in interventional radiology and cardiology. We present three different synthetically generated datasets with increasing complexity for training, using a Monte-Carlo Simulation application based on Geant4. On those datasets, we evaluate convolutional and fully connected architectures of neural networks to demonstrate which design decisions work well for reconstructing the fluence and spectra distributions over the spatial domain of such radiation fields. All our datasets, as well as our training pipeline, are published as open source in separate repositories.</p></details> |  |
| **[Spectral representations of interpolation spaces of reproducing kernel Hilbert spaces](https://arxiv.org/abs/2508.16492v2)** | 2025-12-22 | <details><summary>Show</summary><p>In statistical learning theory, interpolation spaces of the form $[\mathrm{L}^2,H]_{θ,r}$, where $H$ is a reproducing kernel Hilbert space, are in widespread use. So far, however, they are only well understood for fine index $r=2$. We generalise existing results from $r=2$ to all possible values of $r$. In particular, we present a spectral decomposition of such spaces, analyse their embedding properties, and describe connections to the theory of Banach spaces of functions. We additionally present example applications of our results to regularisation error estimation in statistical learning.</p></details> | 29 pages |
| **[Addition is almost all you need: Compressing neural networks with double binary factorization](https://arxiv.org/abs/2505.11076v3)** | 2025-12-22 | <details><summary>Show</summary><p>Binary quantization approaches, which replace weight matrices with binary matrices and substitute costly multiplications with cheaper additions, offer a computationally efficient approach to address the increasing computational and storage requirements of Large Language Models (LLMs). However, the severe quantization constraint ($\pm1$) can lead to significant accuracy degradation. In this paper, we propose Double Binary Factorization (DBF), a novel method that factorizes dense weight matrices into products of two binary (sign) matrices, each accompanied by scaling vectors. DBF preserves the efficiency advantages of binary representations while achieving compression rates that are competitive with or superior to state-of-the-art methods. Specifically, in a 1-bit per weight range, DBF is better than existing binarization approaches. In a 2-bit per weight range, DBF is competitive with the best quantization methods like QuIP\# and QTIP. Unlike most existing compression techniques, which offer limited compression level choices, DBF allows fine-grained control over compression ratios by adjusting the factorization's intermediate dimension. Based on this advantage, we further introduce an algorithm for estimating non-uniform layer-wise compression ratios for DBF, based on previously developed channel pruning criteria. Code available at: https://github.com/usamec/double_binary</p></details> |  |
| **[Energy dissipation mechanisms in an acoustically-driven slit](https://arxiv.org/abs/2512.19507v1)** | 2025-12-22 | <details><summary>Show</summary><p>We quantify how incident acoustic energy is converted into vortical motion and viscous dissipation for a two-dimensional plane-wave passing through a slit geometry. We perform direct numerical simulations over a broad parameter space in incident sound pressure level (ISPL), Strouhal number (St), and Reynolds number (Re). Spectral proper orthogonal decomposition (SPOD) yields energy-ranked coherent structures at each frequency, from which we construct mode-by-mode fields for spectral kinetic energy (KE) and viscous loss (VL) components to examine the mechanisms of acoustic absorption. At ISPL=150dB, the acoustic-hydrodynamic energy conversion is highest when the acoustic displacement amplitude is comparable to the slit thickness, corresponding to a Keulegan-Carpenter number of order unity. In this regime, the oscillatory boundary layer undergoes periodic separation, resulting in vortex shedding that dominates acoustic damping. VL accounts for 20-60% of the KE contribution. For higher acoustic frequencies, the confinement of the Stokes layer produces X-shaped near-slit modes, reducing the total energy input by approximately 50%. The influence of Re depends on amplitude. At ISPL=150dB, larger Re values correspond to suppressed broadband fluctuations and sharpened harmonic peaks. At ISPL = 120dB, the boundary layers remain attached, vortex shedding is weak, absorption monotonically scales with viscosity, and the Re- and St-dependencies become comparable. Across all conditions, more than 99% of the VL is confined to a compact region surrounding the slit mouth. The KE-VL spectra describe parameter regimes that enhance or suppress acoustic damping in slit geometries, providing a physically interpretable basis for acoustic-based design.</p></details> |  |
| **[Error Estimates for Sparse Tensor Products of B-spline Approximation Spaces](https://arxiv.org/abs/2510.21517v3)** | 2025-12-22 | <details><summary>Show</summary><p>This work introduces and analyzes B-spline approximation spaces defined on general geometric domains obtained through a mapping from a parameter domain. These spaces are constructed as sparse-grid tensor products of univariate spaces in the parameter domain and are mapped to the physical domain via a geometric parametrization. Both the univariate approximation spaces and the geometric mapping are built using maximally smooth B-splines. We construct two such spaces, employing either the sparse-grid combination technique or the hierarchical subspace decomposition of sparse-grid tensor products, and we prove their mathematical equivalence. Furthermore, we derive approximation error estimates and inverse inequalities that highlight the advantages of sparse-grid tensor products. Specifically, under suitable regularity assumptions on the solution, these spaces achieve the same approximation order as standard tensor product spaces while using significantly fewer degrees of freedom. Additionally, our estimates indicate that, in the case of non-tensor-product domains, stronger regularity assumptions on the solution -- particularly concerning isotropic (non-mixed) derivatives -- are required to achieve optimal convergence rates compared to sparse-grid methods defined on tensor-product domains.</p></details> |  |
| **[Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks](https://arxiv.org/abs/2511.14455v3)** | 2025-12-22 | <details><summary>Show</summary><p>We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\varphi=\varphi(x,u)$ such that $\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.</p></details> |  |
| **[Large deviations for stochastic evolution equations beyond the coercive case](https://arxiv.org/abs/2512.19501v1)** | 2025-12-22 | <details><summary>Show</summary><p>We prove the small-noise large deviation principle (LDP) for stochastic evolution equations in an $L^2$-setting. As the coefficients are allowed to be non-coercive, our framework encompasses a much broader scope than variational settings. To replace coercivity, we require only well-posedness of the stochastic evolution equation and two concrete, verifiable a priori estimates. Furthermore, we accommodate drift nonlinearities satisfying a modified criticality condition, and we allow for vanishing drift perturbations. The latter permits the inclusion of Itô--Stratonovich correction terms, enabling the treatment of both noise interpretations. In another paper, our results have been applied to the 3D primitive equations with full transport noise. In the current paper, we give an application to a reaction-diffusion system which lacks coercivity, further demonstrating the versatility of the framework. Finally, we show that even in the coercive case, we obtain new LDP results for equations with critical nonlinearities that rely on our modified criticality condition, including the stochastic 2D Allen--Cahn equation in the weak setting.</p></details> |  |
| **[Asymptotic theory for nonparametric testing of $k$-monotonicity in discrete distributions](https://arxiv.org/abs/2407.01751v2)** | 2025-12-22 | <details><summary>Show</summary><p>In shape-constrained nonparametric inference, it is often necessary to perform preliminary tests to verify whether a probability mass function (p.m.f.) satisfies qualitative constraints such as monotonicity, convexity, or in general $k$-monotonicity. In this paper, we are interested in nonparametric testing of $k$-monotonicity of a finitely supported discrete distribution. We consider a unified testing framework based on a natural statistic which is directly derived from the very definition of $k$-monotonicity. The introduced framework allows us to design a new consistent method to select the unknown knot points that are required to consistently approximate the limit distribution of several test statistics based either on the empirical measure or the shape-constrained estimators of the p.m.f. We show that the resulting tests are asymptotically valid and consistent for any fixed alternative. Additionally, for the test based solely on the empirical measure, we study the asymptotic power under contiguous alternatives and derive a quantitative separation result that provides sufficient conditions to achieve a given power. We employ this test to design an estimator for the largest parameter $k \in \mathbb N_0$ such that the p.m.f. is $j$-monotone for all $j = 0, \ldots, k$, and show that the estimator is different from the true parameter with probability which is asymptotically smaller than the nominal level of the test. A detailed simulation study is performed to assess the finite sample performance of all the proposed tests, and applications to several real datasets are presented to illustrate the theory.</p></details> |  |
| **[Causal inference with misspecified network interference structure](https://arxiv.org/abs/2302.11322v3)** | 2025-12-22 | <details><summary>Show</summary><p>Under interference, the treatment of one unit may affect the outcomes of other units. Such interference patterns between units are typically represented by a network. Correctly specifying this network requires identifying which units can affect others -- an inherently challenging task. Nevertheless, most existing approaches assume that a known and accurate network specification is given. In this paper, we study the consequences of such misspecification. We derive bounds on the bias arising from estimating causal effects using a misspecified network, showing that the estimation bias grows with the divergence between the assumed and true networks, quantified through their induced exposure probabilities. To address this challenge, we propose a novel estimator that leverages multiple networks simultaneously and remains unbiased if at least one of the networks is correct, even when we do not know which one. Therefore, the proposed estimator provides robustness to network specification. We illustrate key properties and demonstrate the utility of our proposed estimator through simulations and analysis of a social network field experiment.</p></details> |  |
| **[3DFETUS: Deep Learning-Based Standardization of Facial Planes in 3D Ultrasound](https://arxiv.org/abs/2511.10412v2)** | 2025-12-22 | <details><summary>Show</summary><p>The automatic localization and standardization of anatomical planes in 3D medical imaging remains a challenging problem due to variability in object pose, appearance, and image quality. In 3D ultrasound, these challenges are exacerbated by speckle noise and limited contrast, particularly in fetal imaging. To address these challenges in the context of facial assessment, we present: 1) GT++, a robust algorithm that estimates standard facial planes from 3D US volumes using annotated anatomical landmarks; and 2) 3DFETUS, a deep learning model that automates and standardizes their localization in 3D fetal US volumes. We evaluated our methods both qualitatively, through expert clinical review, and quantitatively. The proposed approach achieved a mean translation error of 3.21 $\pm$ 1.98mm and a mean rotation error of 5.31 $\pm$ 3.945$^\circ$ per plane, outperforming other state-of-the-art methods on 3D US volumes. Clinical assessments further confirmed the effectiveness of both GT++ and 3DFETUS, demonstrating statistically significant improvements in plane estimation accuracy.</p></details> |  |
| **[Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications](https://arxiv.org/abs/2512.19472v1)** | 2025-12-22 | <details><summary>Show</summary><p>The recent explosive growth in Deep Neural Networks applications raises concerns about the black-box usage of such models, with limited trasparency and trustworthiness in high-stakes domains, which have been crystallized as regulatory requirements such as the European Union Artificial Intelligence Act. While models with embedded confidence metrics have been proposed, such approaches cannot be applied to already existing models without retraining, limiting their broad application. On the other hand, post-hoc methods, which evaluate pre-trained models, focus on solving problems related to improving the confidence in the model's predictions, and detecting Out-Of-Distribution or Adversarial Attacks samples as independent applications. To tackle the limited applicability of already existing methods, we introduce Multi-Layer Analysis for Confidence Scoring (MACS), a unified post-hoc framework that analyzes intermediate activations to produce classification-maps. From the classification-maps, we derive a score applicable for confidence estimation, detecting distributional shifts and adversarial attacks, unifying the three problems in a common framework, and achieving performances that surpass the state-of-the-art approaches in our experiments with the VGG16 and ViTb16 models with a fraction of their computational overhead.</p></details> |  |
| **[Fully Asynchronous Unsourced Random Access over Fading Channels](https://arxiv.org/abs/2512.19468v1)** | 2025-12-22 | <details><summary>Show</summary><p>We examine unsourced random access in a fully asynchronous setup, where active users transmit their data without restriction on the start time over a fading channel. In the proposed scheme, the transmitted signal consists of a pilot sequence and a polar codeword, with the polar codeword distributed across the data part of the packet in an on-off pattern. The receiver uses a double sliding-window decoder, where the inner window employs iterative decoding with joint timing and pilot detection, channel estimation, single-user decoding, and successive interference cancellation to recover the message bits, while the outer window enhances interference cancellation. The numerical results indicate that the proposed scheme exhibits only a slight performance loss compared to the synchronous benchmark while being more applicable in practice.</p></details> |  |
| **[Gap-free Information Transfer in 4D-STEM via Fusion of Complementary Scattering Channels](https://arxiv.org/abs/2512.19460v1)** | 2025-12-22 | <details><summary>Show</summary><p>Linear phase-contrast scanning transmission electron microscopy (STEM) techniques compatible with high-throughput 4D-STEM acquisition are widely used to enhance phase contrast in weakly scattering and beam-sensitive materials. In these modalities, contrast transfer is often suppressed at low spatial frequencies, resulting in a characteristic contrast gap that limits quantitative imaging. Approaches that retain low-frequency phase contrast exist but typically require substantially increased experimental complexity, restricting routine use. Dark-field STEM imaging captures this missing low-frequency information through electrons scattered outside the bright-field disk, but discards a large fraction of the scattered signal and is therefore dose-inefficient. Fused Full-field STEM (FF-STEM) is introduced as a 4D-STEM imaging modality that overcomes this limitation by combining ptychographic phase reconstruction with tilt-corrected dark-field imaging within a single acquisition. Bright-field data are used to estimate probe aberrations and reconstruct a high-resolution phase image, while dark-field data provide complementary low-frequency contrast. The two channels are optimally fused in Fourier space using minimum-variance weighting based on the spectral signal-to-noise ratio, yielding transfer-gap-free images with high contrast and quantitative fidelity. FF-STEM preserves the upsampling and depth-sectioning capabilities of ptychography, adds robust low-frequency contrast characteristic of dark-field imaging, and enables dose-efficient, near-real-time reconstruction.</p></details> |  |
| **[Two-Dimensional Tomographic Reconstruction From Projections With Unknown Angles and Unknown Spatial Shifts](https://arxiv.org/abs/2511.22890v2)** | 2025-12-22 | <details><summary>Show</summary><p>In parallel beam computed tomography (CT), an object is reconstructed from a series of projections taken at different angles. However, in some industrial and biomedical imaging applications, the projection geometry is unknown, completely or partially. In this paper, we present a technique for two-dimensional (2D) tomography in which both viewing angles and spatial shifts associated with the projections are unknown. There exists literature on 2D unknown view tomography (UVT), but most existing 2D UVT algorithms assume that the projections are centered; that is, there are no spatial shifts in the projections. To tackle these geometric ambiguities, we first modify an existing graph Laplacian-based algorithm for 2D UVT to incorporate spatial shifts, and then use it as the initialization for the proposed three-way alternating minimization algorithm that jointly estimates the 2D structure, its projection angles, and the corresponding shifts. We evaluate our method on noisy projections of ribosome images and demonstrate that it achieves superior reconstruction compared to the baseline that neglects shifts.</p></details> | <details><summary>5 pag...</summary><p>5 pages, 2 figures, 1 table, submitted to the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</p></details> |
| **[A critical threshold for the cosmological Euler-Poisson system](https://arxiv.org/abs/2512.19454v1)** | 2025-12-22 | <details><summary>Show</summary><p>We consider the gravitational Euler-Poisson system with a linear equation of state on an expanding cosmological model of the Universe. The expansion of the spatial sections introduces an additional dissipating effect in the Euler equation. We prescribe the expansion rate of space by a scale factor $a(t)=t^α$ with $α\in(0,1)$, which describes the growth of length scales over time. This model is regularly applied in cosmology to study classical fluids in an expanding Universe. We study the behaviour of solutions to this system arising from small, near-homogeneous initial data and discover a \emph{critical} change of behaviour near the expansion rate $α=2/3$, which corresponds to the matter-dominated regime in cosmology. In particular, we prove that for $α>2/3$ the fluid variables are global in time and remain small provided they are sufficiently small in a suitable norm initially. In the complementary regime $α\leq2/3$, we present numerical evidence for shock formation of solutions to the Euler equation for arbitrarily small initial data. In combination, this establishes the existence of a critical stability threshold for barotropic fluids in expanding domains. In contrast to our previous work on the corresponding relativistic system, the threshold in the classical system considered here is independent of the speed of sound of the fluid. This establishes that fluids in cosmology behave fundamentally different in the non-relativistic regime than in the relativistic one.</p></details> |  |
| **[Regularity for fully nonlinear elliptic equations in generalized Orlicz spaces](https://arxiv.org/abs/2512.16600v2)** | 2025-12-22 | <details><summary>Show</summary><p>In this paper, we establish an optimal global Calderón-Zygmund type estimate for the viscosity solution to the Dirichlet boundary problem of fully nonlinear elliptic equations with possibly nonconvex nonlinearities. We prove that the Hessian of the solution is as integrable as the nonhomogeneous term in the setting of a given generalized Orlicz space even when the nonlinearity is asymptotically convex with respect to the Hessian of the solution.</p></details> |  |
| **[Real-Time Streamable Generative Speech Restoration with Flow Matching](https://arxiv.org/abs/2512.19442v1)** | 2025-12-22 | <details><summary>Show</summary><p>Diffusion-based generative models have greatly impacted the speech processing field in recent years, exhibiting high speech naturalness and spawning a new research direction. Their application in real-time communication is, however, still lagging behind due to their computation-heavy nature involving multiple calls of large DNNs. Here, we present Stream.FM, a frame-causal flow-based generative model with an algorithmic latency of 32 milliseconds (ms) and a total latency of 48 ms, paving the way for generative speech processing in real-time communication. We propose a buffered streaming inference scheme and an optimized DNN architecture, show how learned few-step numerical solvers can boost output quality at a fixed compute budget, explore model weight compression to find favorable points along a compute/quality tradeoff, and contribute a model variant with 24 ms total latency for the speech enhancement task. Our work looks beyond theoretical latencies, showing that high-quality streaming generative speech processing can be realized on consumer GPUs available today. Stream.FM can solve a variety of speech processing tasks in a streaming fashion: speech enhancement, dereverberation, codec post-filtering, bandwidth extension, STFT phase retrieval, and Mel vocoding. As we verify through comprehensive evaluations and a MUSHRA listening test, Stream.FM establishes a state-of-the-art for generative streaming speech restoration, exhibits only a reasonable reduction in quality compared to a non-streaming variant, and outperforms our recent work (Diffusion Buffer) on generative streaming speech enhancement while operating at a lower latency.</p></details> | <details><summary>This ...</summary><p>This work has been submitted to the IEEE for possible publication</p></details> |
| **[Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm](https://arxiv.org/abs/2512.19440v1)** | 2025-12-22 | <details><summary>Show</summary><p>Kernel logistic regression (KLR) is a widely used supervised learning method for binary and multi-class classification, which provides estimates of the conditional probabilities of class membership for the data points. Unlike other kernel methods such as Support Vector Machines (SVMs), KLRs are generally not sparse. Previous attempts to deal with sparsity in KLR include a heuristic method referred to as the Import Vector Machine (IVM) and ad hoc regularizations such as the $\ell_{1/2}$-based one. Achieving a good trade-off between prediction accuracy and sparsity is still a challenging issue with a potential significant impact from the application point of view. In this work, we revisit binary KLR and propose an extension of the training formulation proposed by Keerthi et al., which is able to induce sparsity in the trained model, while maintaining good testing accuracy. To efficiently solve the dual of this formulation, we devise a decomposition algorithm of Sequential Minimal Optimization type which exploits second-order information, and for which we establish global convergence. Numerical experiments conducted on 12 datasets from the literature show that the proposed binary KLR approach achieves a competitive trade-off between accuracy and sparsity with respect to IVM, $\ell_{1/2}$-based regularization for KLR, and SVM while retaining the advantages of providing informative estimates of the class membership probabilities.</p></details> |  |
| **[Machine Unlearning in Speech Emotion Recognition via Forget Set Alone](https://arxiv.org/abs/2510.04251v2)** | 2025-12-22 | <details><summary>Show</summary><p>Speech emotion recognition aims to identify emotional states from speech signals and has been widely applied in human-computer interaction, education, healthcare, and many other fields. However, since speech data contain rich sensitive information, partial data can be required to be deleted by speakers due to privacy concerns. Current machine unlearning approaches largely depend on data beyond the samples to be forgotten. However, this reliance poses challenges when data redistribution is restricted and demands substantial computational resources in the context of big data. We propose a novel adversarial-attack-based approach that fine-tunes a pre-trained speech emotion recognition model using only the data to be forgotten. The experimental results demonstrate that the proposed approach can effectively remove the knowledge of the data to be forgotten from the model, while preserving high model performance on the test set for emotion recognition.</p></details> | <details><summary>Submi...</summary><p>Submitted to ICASSP 2026</p></details> |
| **[Brownian bridge limit of path measures in the upper tail of KPZ models](https://arxiv.org/abs/2311.12009v2)** | 2025-12-22 | <details><summary>Show</summary><p>For models in the KPZ universality class, such as the zero temperature model of planar last passage-percolation (LPP) and the positive temperature model of directed polymers, its upper tail behavior has been a topic of recent interest, with particular focus on the associated path measures (i.e., geodesics or polymers). For Exponential LPP, diffusive fluctuation had been established in Basu-Ganguly. In the directed landscape, the continuum limit of LPP, the limiting Gaussianity at one point, as well as of related finite-dimensional distributions of the KPZ fixed point, were established, using exact formulas in Liu and Wang-Liu. It was further conjectured in these works that the limit of the corresponding geodesic should be a Brownian bridge. We prove it in both zero and positive temperatures; for the latter, neither the one-point limit nor the scale of fluctuations was previously known. Instead of relying on formulas (which are still missing in the positive temperature literature), our arguments are geometric and probabilistic, using the results on the shape of the weight and free energy profiles under the upper tail from Ganguly-Hegde as a starting point. Another key ingredient involves novel coalescence estimates, developed using the recently discovered shift-invariance Borodin-Gorin-Wheeler in these models. Finally, our proof also yields insight into the structure of the polymer measure under the upper tail conditioning, establishing a quenched localization exponent around a random backbone.</p></details> | <details><summary>77 pa...</summary><p>77 pages, 6 figures. Updated to fix an error, relying on the form of the BK inequality recently established in arXiv:2512.17823</p></details> |
| **[Sharp upper tail behavior of line ensembles via the tangent method](https://arxiv.org/abs/2208.08922v3)** | 2025-12-22 | <details><summary>Show</summary><p>We develop a new probabilistic and geometric method to obtain several sharp results pertaining to the upper tail behavior of continuum Gibbs measures on infinite ensembles of random continuous curves, also known as line ensembles, satisfying some natural assumptions. The arguments make crucial use of Brownian resampling invariance properties and correlation inequalities admitted by such Gibbs measures. We obtain sharp one-point upper tail estimates showing that the probability of the value at zero being larger than $θ$ is $\exp(-\frac{4}{3}θ^{3/2}(1+o(1)))$. A key intermediate step is developing a precise understanding of the profile when conditioned on the value at zero equaling $θ$. Our method further allows one to obtain multi-point asymptotics which were out of reach of previous approaches. As an example, we prove sharp explicit two-point upper tail estimates. This framework is then used to establish the corresponding results for the KPZ equation, which are all new. Even for the zero-temperature case of the Airy$_2$ process, our arguments yield new proofs for one-point estimates previously known due to its connections to random matrix theory, as well as new two-point asymptotics. To showcase the reach of the method, we obtain the same results in a purely non-integrable setting under only assumptions of stationarity and extremality in the class of Gibbs measures. Our method bears resemblance to the tangent method introduced by Colomo-Sportiello and mathematically realized by Aggarwal in the context of the six-vertex model.</p></details> | <details><summary>96 pa...</summary><p>96 pages, 8 figures. v3: Previously conditional results for the KPZ line ensemble have been made unconditional using arXiv:2512.17823. Abstract suitably modified</p></details> |
| **[Hybrid Analytical-Machine Learning Framework for Ripple Factor Estimation in Cockcroft-Walton Voltage Multipliers with Residual Correction for Non-Ideal Effects](https://arxiv.org/abs/2512.19434v1)** | 2025-12-22 | <details><summary>Show</summary><p>Cockcroft-Walton (CW) voltage multipliers suffer from output ripple that classical analytical models underestimate due to neglected non-idealities like diode drops and capacitor ESR, particularly in high-stage, low-frequency and heavy-load regimes. This paper proposes a hybrid framework that generates a comprehensive 324-case MATLAB/Simulink dataset varying stages (2-8), input voltage (5-25 kV), capacitance (1-10 μF), frequency (50-500 Hz) and load (6-60 MΩ), then trains a Random Forest model to predict residuals between simulated and theoretical peak-to-peak ripple. The approach achieves 70.6% RMSE reduction (131 V vs. 448 V) globally and 66.7% in critical regimes, with near-zero bias, enabling physically interpretable design optimization while outperforming pure ML in extrapolation reliability.</p></details> | <details><summary>6 Pag...</summary><p>6 Pages, 2 figures, IEEE Conference Template used</p></details> |
| **[AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting](https://arxiv.org/abs/2505.18822v2)** | 2025-12-22 | <details><summary>Show</summary><p>Modern large reasoning models demonstrate impressive problem-solving capabilities by employing sophisticated reasoning strategies. However, they often struggle to balance efficiency and effectiveness, frequently generating unnecessarily lengthy reasoning chains for simple problems. In this work, we propose AdaCtrl, a novel framework to support both difficulty-aware adaptive reasoning budget allocation and explicit user control over reasoning depth. AdaCtrl dynamically adjusts its reasoning length based on self-assessed problem difficulty, while also allowing users to manually control the budget to prioritize either efficiency or effectiveness. This is achieved through a two-stage training pipeline: an initial cold-start fine-tuning phase to instill the ability to self-aware difficulty and adjust reasoning budget, followed by a difficulty-aware reinforcement learning (RL) stage that refines the model's adaptive reasoning strategies and calibrates its difficulty assessments based on its evolving capabilities during online training. To enable intuitive user interaction, we design explicit length-triggered tags that function as a natural interface for budget control. Empirical results show that AdaCtrl adapts reasoning length based on estimated difficulty, compared to the standard training baseline that also incorporates fine-tuning and RL, it yields performance improvements and simultaneously reduces response length by 10.06% and 12.14% on the more challenging AIME2024 and AIME2025 datasets, which require elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K datasets, where more concise responses are sufficient. Furthermore, AdaCtrl enables precise user control over the reasoning budget, allowing for tailored responses to meet specific needs.</p></details> |  |
| **[Hölder regularity of doubly nonlinear nonlocal quasilinear parabolic equations in some mixed singular-degenerate regime](https://arxiv.org/abs/2512.19421v1)** | 2025-12-22 | <details><summary>Show</summary><p>We study local Hölder regularity of bounded, weak solutions for the nonlocal quasilinear equations of the form \[ (|u|^{q-2}u)_t + \text{P.V.} \int_{\mathbb{R}^n} \frac{|u(x,t) - u(y,t)|^{p-2}(u(x,t)-u(y,t))}{|x-y|^{n+sp}} dy = 0, \] with $p\in (1,\infty)$, $q\in (1,\infty)$ and $s \in (0,1)$. Analogous Hölder continuity result in the local case is known in the purely singular case $\{1<p<2, p<q\}$, purely degenerate case $\{2<p, q<p\}$, scale invariant case $\{p=q\}$ and translation invariant case $\{q=2,1<p<\infty\}$. In the nonlocal setting, Hölder regularity is known when the equation is either translation invariant $\{q=2, 1<p<\infty\}$ or scale invariant $\{q=p, 1<p<\infty\}$ or purely degenerate case $\{2<p, q<p\}$. Similar strategy can be used to obtain Hölder regularity in the purely singular case $\{1<p<2, p<q\}$. In this paper, we adapt several ideas developed over the past few years and combine it with a new intrinsic scaling to prove Hölder regularity in the mixed singular-degenerate range $\max\{p,q,2\} < \min\left\{q + \tfrac{p-1}{1+\frac{n}{sp}}, 2 + \tfrac{p-1}{1+\frac{n}{sp}}\right\}$. The proof explicitly makes use of the nonlocal nature of the problem and as a consequence, our estimates are not stable at $s \rightarrow 0$. We note that the analogous regularity in the local problem remains open.</p></details> | 43 pages |
| **[Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming](https://arxiv.org/abs/2512.15735v2)** | 2025-12-22 | <details><summary>Show</summary><p>This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.</p></details> | <details><summary>we ha...</summary><p>we have identified some technical issues, including the mathematical derivation. After discussion, all authors have agreed that the analysis requires a thorough re-derivation to ensure correctness and rigor</p></details> |
| **[Estimates for short character sums evaluated at homogeneous polynomials](https://arxiv.org/abs/2501.12325v2)** | 2025-12-22 | <details><summary>Show</summary><p>Let $p$ be a prime. We prove bounds on short Dirichlet character sums evaluated at a class of homogeneous polynomials in arbitrary dimensions. In every dimension, this bound is nontrivial for sums over boxes with side lengths as short as $p^{1/4 + κ}$ for any $κ>0$. Our methods capitalize on the relationship between characters mod $p$ and characters over finite field extensions as well as bounds on the multiplicative energy of sets in products of finite fields.</p></details> | <details><summary>47 pa...</summary><p>47 pages; v2 aligns with published version</p></details> |
| **[Uniform pathwise stability of additive singular SDEs driven by fractional Brownian motion](https://arxiv.org/abs/2511.05262v2)** | 2025-12-22 | <details><summary>Show</summary><p>We study the long-time behaviour of solutions to a class of $d$-dimensional stochastic differential equations driven by fractional Brownian motion with Hurst parameter $H \in (0,1)$. The drift consists of a dissipative Lipschitz term and a singular term of regularity $γ>1-1/(2H)$ in Besov-Hölder scales. We establish well-posedness and, through a Markovian enhancement, existence of an invariant measure. If the singular contribution is sufficiently small, we prove exponential contraction of solutions, and thereby, uniqueness of the invariant measure. Our methods rely on uniform pathwise estimates which utilise together the dissipativity of the drift and the regularisation effect of the noise.</p></details> |  |
| **[Networked Communication for Decentralised Cooperative Agents in Mean-Field Control](https://arxiv.org/abs/2503.09400v2)** | 2025-12-22 | <details><summary>Show</summary><p>The mean-field framework has been used to find approximate solutions to problems involving very large populations of symmetric, anonymous agents, which may be intractable by other methods. The cooperative mean-field control (MFC) problem has received less attention than the non-cooperative mean-field game (MFG), despite the former potentially being more useful as a tool for engineering large-scale collective behaviours. Decentralised communication algorithms have recently been introduced to MFGs, giving benefits to learning speed and robustness. Inspired by this, we introduce networked communication to MFC - where populations arguably have broader incentive to communicate - and in particular to the setting where decentralised agents learn online from a single, non-episodic run of the empirical system. We adapt recent MFG algorithms to this new setting, as well as contributing a novel sub-routine allowing networked agents to estimate the global average reward from their local neighbourhood. Previous theoretical analysis of decentralised communication in MFGs does not extend trivially to MFC. We therefore contribute new theory proving that in MFC the networked communication scheme allows agents to increase social welfare faster than under both of the two typical alternative architectures, namely independent and centralised learning. We provide experiments that support this new result across different classes of cooperative game, and also give numerous ablation studies and additional experiments concerning numbers of communication round and robustness to communication failures.</p></details> |  |
| **[Networked Communication for Mean-Field Games with Function Approximation and Empirical Mean-Field Estimation](https://arxiv.org/abs/2408.11607v3)** | 2025-12-22 | <details><summary>Show</summary><p>Recent algorithms allow decentralised agents, possibly connected via a communication network, to learn equilibria in mean-field games from a non-episodic run of the empirical system. However, these algorithms are for tabular settings: this computationally limits the size of agents' observation space, meaning the algorithms cannot handle anything but small state spaces, nor generalise beyond policies depending only on the agent's local state to so-called 'population-dependent' policies. We address this limitation by introducing function approximation to the existing setting, drawing on the Munchausen Online Mirror Descent method that has previously been employed only in finite-horizon, episodic, centralised settings. While this permits us to include the mean field in the observation for players' policies, it is unrealistic to assume decentralised agents have access to this global information: we therefore also provide new algorithms allowing agents to locally estimate the global empirical distribution, and to improve this estimate via inter-agent communication. We prove theoretically that exchanging policy information helps networked agents outperform both independent and even centralised agents in function-approximation settings. Our experiments demonstrate this happening empirically, and show that the communication network allows decentralised agents to estimate the mean field for population-dependent policies.</p></details> |  |
| **[Networked Communication for Decentralised Agents in Mean-Field Games](https://arxiv.org/abs/2306.02766v6)** | 2025-12-22 | <details><summary>Show</summary><p>Methods like multi-agent reinforcement learning struggle to scale with growing population size. Mean-field games (MFGs) are a game-theoretic approach that can circumvent this by finding a solution for an abstract infinite population, which can then be used as an approximate solution for the $N$-agent problem. However, classical mean-field algorithms usually only work under restrictive conditions. We take steps to address this by introducing networked communication to MFGs, in particular to settings that use a single, non-episodic run of $N$ decentralised agents to simulate the infinite population, as is likely to be most reasonable in real-world deployments. We prove that our architecture's sample guarantees lie between those of earlier theoretical algorithms for the centralised- and independent-learning architectures, varying dependent on network structure and the number of communication rounds. However, the sample guarantees of the three theoretical algorithms do not actually result in practical convergence times. We thus contribute practical enhancements to all three algorithms allowing us to present their first empirical demonstrations. We then show that in practical settings where the theoretical hyperparameters are not observed, giving fewer loops but poorer estimation of the Q-function, our communication scheme still respects the earlier theoretical analysis: it considerably accelerates learning over the independent case, which hardly seems to learn at all, and often performs similarly to the centralised case, while removing the restrictive assumption of the latter. We provide ablations and additional studies showing that our networked approach also has advantages over both alternatives in terms of robustness to update failures and to changes in population size.</p></details> |  |
| **[DeepGESI: A Non-Intrusive Objective Evaluation Model for Predicting Speech Intelligibility in Hearing-Impaired Listeners](https://arxiv.org/abs/2512.19374v1)** | 2025-12-22 | <details><summary>Show</summary><p>Speech intelligibility assessment is essential for many speech-related applications. However, most objective intelligibility metrics are intrusive, as they require clean reference speech in addition to the degraded or processed signal for evaluation. Furthermore, existing metrics such as STOI are primarily designed for normal hearing listeners, and their predictive accuracy for hearing impaired speech intelligibility remains limited. On the other hand, the GESI (Gammachirp Envelope Similarity Index) can be used to estimate intelligibility for hearing-impaired listeners, but it is also intrusive, as it depends on reference signals. This requirement limits its applicability in real-world scenarios. To overcome this limitation, this study proposes DeepGESI, a non-intrusive deep learning-based model capable of accurately and efficiently predicting the speech intelligibility of hearing-impaired listeners without requiring any clean reference speech. Experimental results demonstrate that, under the test conditions of the 2nd Clarity Prediction Challenge(CPC2) dataset, the GESI scores predicted by DeepGESI exhibit a strong correlation with the actual GESI scores. In addition, the proposed model achieves a substantially faster prediction speed compared to conventional methods.</p></details> |  |
| **[ForeSpeed: A real-world video dataset of CCTV cameras with different settings for vehicle speed estimation](https://arxiv.org/abs/2512.19364v1)** | 2025-12-22 | <details><summary>Show</summary><p>The need to estimate the speed of road vehicles has become increasingly important in the field of video forensics, particularly with the widespread deployment of CCTV cameras worldwide. Despite the development of various approaches, the accuracy of forensic speed estimation from real-world footage remains highly dependent on several factors, including camera specifications, acquisition methods, spatial and temporal resolution, compression methods, and scene perspective, which can significantly influence performance. In this paper, we introduce ForeSpeed, a comprehensive dataset designed to support the evaluation of speed estimation techniques in real-world scenarios using CCTV footage. The dataset includes recordings of a vehicle traveling at known speeds, captured by three digital and three analog cameras from two distinct perspectives. Real-world road metrics are provided to enable the restoration of the scene geometry. Videos were stored with multiple compression factors and settings, to simulate real world scenarios in which export procedures are not always performed according to forensic standards. Overall, ForeSpeed, includes a collection of 322 videos. As a case study, we employed the ForeSpeed dataset to benchmark a speed estimation algorithm available in a commercial product (Amped FIVE). Results demonstrate that while the method reliably estimates average speed across various conditions, its uncertainty range significantly increases when the scene involves strong perspective distortion. The ForeSpeed dataset is publicly available to the forensic community, with the aim of facilitating the evaluation of current methodologies and inspiring the development of new, robust solutions tailored to collision investigation and forensic incident analysis.</p></details> |  |
| **[GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting](https://arxiv.org/abs/2412.14579v2)** | 2025-12-22 | <details><summary>Show</summary><p>Weakly-supervised 3D occupancy perception is crucial for vision-based autonomous driving in outdoor environments. Previous methods based on NeRF often face a challenge in balancing the number of samples used. Too many samples can decrease efficiency, while too few can compromise accuracy, leading to variations in the mean Intersection over Union (mIoU) by 5-10 points. Furthermore, even with surrounding-view image inputs, only a single image is rendered from each viewpoint at any given moment. This limitation leads to duplicated predictions, which significantly impacts the practicality of the approach. However, this issue has largely been overlooked in existing research. To address this, we propose GSRender, which uses 3D Gaussian Splatting for weakly-supervised occupancy estimation, simplifying the sampling process. Additionally, we introduce the Ray Compensation module, which reduces duplicated predictions by compensating for features from adjacent frames. Finally, we redesign the dynamic loss to remove the influence of dynamic objects from adjacent frames. Extensive experiments show that our approach achieves SOTA results in RayIoU (+6.0), while also narrowing the gap with 3D- supervised methods. This work lays a solid foundation for weakly-supervised occupancy perception. The code is available at https://github.com/Jasper-sudo-Sun/GSRender.</p></details> |  |
| **[NA-DiD: Extending Difference-in-Differences with Capabilities](https://arxiv.org/abs/2507.12690v2)** | 2025-12-22 | <details><summary>Show</summary><p>This paper introduces the Non-Additive Difference-in-Differences (NA-DiD) framework, which extends classical DiD by incorporating non-additive measures the Choquet integral for effect aggregation. It serves as a novel econometric tool for impact evaluation, particularly in settings with non-additive treatment effects. First, we introduce the integral representation of the classial DiD model, and then extend it to non-additive measures, therefore deriving the formulae for NA-DiD estimation. Then, we give its theoretical properties. Applying NA-DiD to a simulated hospital hygiene intervention, we find that classical DiD can overestimate treatment effects, f.e. failing to account for compliance erosion. In contrast, NA-DiD provides a more accurate estimate by incorporating non-linear aggregation. The Julia implementation of the techniques used and introduced in this article is provided in the appendices.</p></details> | <details><summary>I hav...</summary><p>I have received a reviews from a journal, and after reading them I came to the conclusion that this research does not meet novelty threshold for a paper I would like to put in public. I missed some references, which are more relevant to the problem I try to solve and offer a better solution</p></details> |
| **[TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking](https://arxiv.org/abs/2512.02789v2)** | 2025-12-22 | <details><summary>Show</summary><p>The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.</p></details> |  |
| **[A hybrid-Hill estimator enabled by heavy-tailed block maxima](https://arxiv.org/abs/2512.19338v1)** | 2025-12-22 | <details><summary>Show</summary><p>When analysing extreme values, two alternative statistical approaches have historically been held in contention: the seminal block maxima method (or annual maxima method, spurred by hydrological applications) and the peaks-over-threshold. Clamoured amongst statisticians as wasteful of potentially informative data, the block maxima method gradually fell into disfavour whilst peaks-over-threshold-based methodologies were ushered to the centre stage of extreme value statistics. This paper proposes a hybrid method which reconciles these two hitherto disconnected approaches. Appealing in its simplicity, our main result introduces a new universal limiting characterisation of extremes that eschews the customary requirement of a sufficiently large block size for the plausible block maxima-fit to an extreme value distribution. We advocate that inference should be drawn solely on larger block maxima, from which practice the mainstream peaks-over-threshold methodology coalesces. The asymptotic properties of the promised hybrid-Hill estimator herald more than its efficiency, but rather that a fully-fledged unified semi-parametric stream of statistics for extreme values is viable. A finite sample simulation study demonstrates that a reduced-bias off-shoot of the hybrid-Hill estimator fares exceptionally well against the incumbent maximum likelihood estimation that relies on a numerical fit to the entire sample of block maxima.</p></details> | 31 pages, 5 figures |
| **[Orthogonal Approximate Message Passing with Optimal Spectral Initializations for Rectangular Spiked Matrix Models](https://arxiv.org/abs/2512.19334v1)** | 2025-12-22 | <details><summary>Show</summary><p>We propose an orthogonal approximate message passing (OAMP) algorithm for signal estimation in the rectangular spiked matrix model with general rotationally invariant (RI) noise. We establish a rigorous state evolution that precisely characterizes the algorithm's high-dimensional dynamics and enables the construction of iteration-wise optimal denoisers. Within this framework, we accommodate spectral initializations under minimal assumptions on the empirical noise spectrum. In the rectangular setting, where a single rank-one component typically generates multiple informative outliers, we further propose a procedure for combining these outliers under mild non-Gaussian signal assumptions. For general RI noise models, the predicted performance of the proposed optimal OAMP algorithm agrees with replica-symmetric predictions for the associated Bayes-optimal estimator, and we conjecture that it is statistically optimal within a broad class of iterative estimation methods.</p></details> |  |
| **[The asymptotic distribution of the likelihood ratio test statistic in two-peak discovery experiments](https://arxiv.org/abs/2512.19333v1)** | 2025-12-22 | <details><summary>Show</summary><p>Likelihood ratio tests are widely used in high-energy physics, where the test statistic is usually assumed to follow a chi-squared distribution with a number of degrees of freedom specified by Wilks' theorem. This assumption breaks down when parameters such as signal or coupling strengths are restricted to be non-negative and their values under the null hypothesis lie on the boundary of the parameter space. Based on a recent clarification concerning the correct asymptotic distribution of the likelihood ratio test statistic for cases where two of the parameters are on the boundary, we revisit the the question of significance estimation for two-peak signal-plus-background counting experiments. In the high-energy physics literature, such experiments are commonly analyzed using Wilks' chi-squared distribution or the one-parameter Chernoff limit. We demonstrate that these approaches can lead to strongly miscalibrated significances, and that the test statistic distribution is instead well described by a chi-squared mixture with weights determined by the Fisher information matrix. Our results highlight the need for boundary-aware asymptotics in the analysis of two-peak counting experiments.</p></details> | 25 pages, 6 figures |
| **[High dimensional matrix estimation through elliptical factor models](https://arxiv.org/abs/2512.19325v1)** | 2025-12-22 | <details><summary>Show</summary><p>Elliptical factor models play a central role in modern high-dimensional data analysis, particularly due to their ability to capture heavy-tailed and heterogeneous dependence structures. Within this framework, Tyler's M-estimator (Tyler, 1987a) enjoys several optimality properties and robustness advantages. In this paper, we develop high-dimensional scatter matrix, covariance matrix and precision matrix estimators grounded in Tyler's M-estimation. We first adapt the Principal Orthogonal complEment Thresholding (POET) framework (Fan et al., 2013) by incorporating the spatial-sign covariance matrix as an effective initial estimator. Building on this idea, we further propose a direct extension of POET tailored for Tyler's M-estimation, referred to as the POET-TME method. We establish the consistency rates for the resulting estimators under elliptical factor models. Comprehensive simulation studies and a real data application illustrate the superior performance of POET-TME, especially in the presence of heavy-tailed distributions, demonstrating the practical value of our methodological contributions.</p></details> |  |
| **[Joint parameter estimation and multidimensional reconciliation for continuous-variable quantum key distribution](https://arxiv.org/abs/2508.05558v2)** | 2025-12-22 | <details><summary>Show</summary><p>Accurate quantum channel parameter estimation is essential for effective information reconciliation in continuous-variable quantum key distribution (CV-QKD). However, conventional maximum likelihood (ML) estimators rely on a large amount of discarded data (or pilot symbols), leading to a significant loss in symbol efficiency. Moreover, the separation between the estimation and reconciliation phases can introduce error propagation. In this paper, we propose a novel joint message-passing scheme that unifies channel parameter estimation and information reconciliation within a Bayesian framework. By leveraging the expectation-maximization (EM) algorithm, the proposed method simultaneously estimates unknown parameters during decoding, eliminating the need for separate ML estimation. Furthermore, we introduce a hybrid multidimensional rotation scheme that removes the requirement for norm feedback, significantly reducing classical channel overhead. To the best of our knowledge, this is the first work to unify multidimensional reconciliation and channel parameter estimation in CV-QKD, providing a practical solution for high-efficiency reconciliation with minimal pilots.</p></details> | 11 pages, 6 figures |
| **[Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market](https://arxiv.org/abs/2503.04521v2)** | 2025-12-22 | <details><summary>Show</summary><p>The convergence of edge computing and Artificial Intelligence (AI) gives rise to Edge-AI, which enables the deployment of real-time AI applications at the network edge. A key research challenge in Edge-AI is edge inference acceleration, which aims to realize low-latency high-accuracy Deep Neural Network (DNN) inference by offloading partitioned inference tasks from end devices to edge servers. However, existing research has yet to adopt a practical Edge-AI market perspective, which would explore the personalized inference needs of AI users (e.g., inference accuracy, latency, and task complexity), the revenue incentives for AI service providers that offer edge inference services, and multi-stakeholder governance within a market-oriented context. To bridge this gap, we propose an Auction-based Edge Inference Pricing Mechanism (AERIA) for revenue maximization to tackle the multi-dimensional optimization problem of DNN model partition, edge inference pricing, and resource allocation. We develop a multi-exit device-edge synergistic inference scheme for on-demand DNN inference acceleration, and theoretically analyze the auction dynamics amongst the AI service providers, AI users and edge infrastructure provider. Owing to the strategic mechanism design via randomized consensus estimate and cost sharing techniques, the Edge-AI market attains several desirable properties. These include competitiveness in revenue maximization, incentive compatibility, and envy-freeness, which are crucial to maintain the effectiveness, truthfulness, and fairness in auction outcomes. Extensive simulations based on four representative DNN inference workloads demonstrate that AERIA significantly outperforms several state-of-the-art approaches in revenue maximization. This validates the efficacy of AERIA for on-demand DNN inference in the Edge-AI market.</p></details> | <details><summary>Accep...</summary><p>Accepted for publication in IEEE Transactions on Mobile Computing. Index Terms: Edge-AI, DNN Inference Offloading, Resource Management, Dynamic Pricing, Auction Mechanism</p></details> |
| **[Local smoothing and maximal estimates for average over surfaces of codimension 2 in $\mathbb R^4$](https://arxiv.org/abs/2507.22695v2)** | 2025-12-22 | <details><summary>Show</summary><p>In this paper, we obtain local smoothing estimates for the averages over nondegenerate surfaces of codimension $2$ in $\mathbb R^4$. We make use of multilinear restriction estimates and decoupling inequalities for a hypersurface in $\mathbb R^5$, a conical extension of a two-dimensional nondegenerate surface along two flat directions. We also establish sharp $L^p$--$L^q$ estimates for maximal averages over nondegenerate surfaces of half the ambient dimension in $\mathbb R^{2n}$ for even $n \ge 2$.</p></details> | <details><summary>32 pa...</summary><p>32 pages, The title was changed, and the introduction was expanded</p></details> |
| **[Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context](https://arxiv.org/abs/2512.19283v1)** | 2025-12-22 | <details><summary>Show</summary><p>Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.</p></details> | <details><summary>Proje...</summary><p>Project Page: https://kyungwoncho.github.io/HaMoS/</p></details> |
| **[Spearman's rho for zero-inflated count data: formulation and attainable bounds](https://arxiv.org/abs/2503.13148v2)** | 2025-12-22 | <details><summary>Show</summary><p>We propose an alternative formulation of Spearman's rho for zero-inflated count data. The formulation yields an estimator with explicitly attainable bounds, facilitating interpretation in settings where the standard range [-1,1] is no longer informative.</p></details> |  |
| **[On the large time behavior of the 2D inhomogeneous incompressible viscous flows](https://arxiv.org/abs/2512.19281v1)** | 2025-12-22 | <details><summary>Show</summary><p>This paper studies the two-dimensional inhomogeneous Navier--Stokes equations governing stratified flows in a bounded domain under a gravitational potential \(f\). Our main results are as follows. First, we provide a rigorous characterization of steady states, proving that under the Dirichlet condition \(\mathbf{u}|_{\partial Ω} = \mathbf{0}\), all admissible equilibria are hydrostatic and satisfy \(\nabla p_s = -ρ_s \nabla f\). Second, through a perturbative analysis around arbitrary hydrostatic profiles, we show that despite possible transient growth induced by the Rayleigh--Taylor mechanism, the system always relaxes to a hydrostatic equilibrium. Third, we identify a necessary and sufficient condition on the initial density perturbation for convergence to a linear hydrostatic density profile of the form \(ρ_s = -γf + β\), with \(γ> 0\) and \(β> 0\). Finally, we establish improved regularity estimates for strong solutions corresponding to initial data in the Sobolev space \(H^3(Ω)\).</p></details> |  |
| **[Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals](https://arxiv.org/abs/2512.19280v1)** | 2025-12-22 | <details><summary>Show</summary><p>Axial piston pumps are crucial components in fluid power systems, where reliable fault diagnosis is essential for ensuring operational safety and efficiency. Traditional data-driven methods require extensive labeled fault data, which is often impractical to obtain, while model-based approaches suffer from parameter uncertainties. This paper proposes a digital twin (DT)-driven zero-shot fault diagnosis framework utilizing fluid-borne noise (FBN) signals. The framework calibrates a high-fidelity DT model using only healthy-state data, generates synthetic fault signals for training deep learning classifiers, and employs a physics-informed neural network (PINN) as a virtual sensor for flow ripple estimation. Gradient-weighted class activation mapping (Grad-CAM) is integrated to visualize the decision-making process of neural networks, revealing that large kernels matching the subsequence length in time-domain inputs and small kernels in time-frequency domain inputs enable higher diagnostic accuracy by focusing on physically meaningful features. Experimental validations demonstrate that training on signals from the calibrated DT model yields diagnostic accuracies exceeding 95\% on real-world benchmarks, while uncalibrated models result in significantly lower performance, highlighting the framework's effectiveness in data-scarce scenarios.</p></details> |  |
| **[Scale-Invariant Robust Estimation of High-Dimensional Kronecker-Structured Matrices](https://arxiv.org/abs/2512.19273v1)** | 2025-12-22 | <details><summary>Show</summary><p>High-dimensional Kronecker-structured estimation faces a conflict between non-convex scaling ambiguities and statistical robustness. The arbitrary factor scaling distorts gradient magnitudes, rendering standard fixed-threshold robust methods ineffective. We resolve this via Scaled Robust Gradient Descent (SRGD), which stabilizes optimization by de-scaling gradients before truncation. To further enforce interpretability, we introduce Scaled Hard Thresholding (SHT) for invariant variable selection. A two-step estimation procedure, built upon robust initialization and SRGD--SHT iterative updates, is proposed for canonical matrix problems, such as trace regression, matrix GLMs, and bilinear models. The convergence rates are established for heavy-tailed predictors and noise, identifying a phase transition where optimal convergence rates recover under finite noise variance and degrade optimally for heavier tails. Experiments on simulated data and two real-world applications confirm superior robustness and efficiency of the proposed procedure.</p></details> | 85 pages, 11 figues |
| **[Sonified Quantum Seizures. Sonification of time series in epileptic seizures and simulation of seizures via quantum modelling](https://arxiv.org/abs/2512.19272v1)** | 2025-12-22 | <details><summary>Show</summary><p>We apply sonification strategies and quantum computing to the analysis of an episode of seizure. We first sonify the signal from a selection of channels (from real ECoG data), obtaining a polyphonic sequence. Then, we propose two quantum approaches to simulate a similar episode of seizure, and we sonify the results. The comparison of sonifications can give hints on similarities and discrepancies between real data and simulations, helping refine the \textit{in silico} model. This is a pioneering approach, showing how the combination of quantum computing and sonification can broaden the perspective of real-data investigation, and helping define a new test bench for analysis and prediction of seizures.</p></details> | <details><summary>Prese...</summary><p>Presented at ISQCMC '25: 3rd International Symposium on Quantum Computing and Musical Creativity</p></details> |
| **[Precise control of high-frequency ultrasounds in thin crystals for the development of tunable narrowband and directional gamma-ray sources](https://arxiv.org/abs/2512.19268v1)** | 2025-12-22 | <details><summary>Show</summary><p>This work presents a complete methodology for the precise characterization of the acoustic field inside crystal-based devices driven by high-frequency ultrasounds towards the generation of tunable narrowband and directional gamma radiation via undulation of ultra-relativistic charged particles. Such gamma-ray sources have long been anticipated by the scientific community, as they promise new powerful tools for the study of high-energy physical phenomena and the development of novel nuclear technologies. In such devices, a piezoelectric transducer induces tens of MHz harmonic waves inside a silicon monocrystal. Ultra-relativistic charged particles traversing the crystal get trapped within the channels formed by the extremely strong electric fields of the acoustically modulated lattice planes, undergoing undulation and emitting gamma radiation. Precise characterization of the acoustic field in the crystal is crucial for the determination of the expected characteristics of the secondarily generated gamma rays. For this purpose, fast laser refraction imaging is used here to image the acoustic waves by exploiting the spatial redistribution of a laser beam optical intensity caused by the acoustic field. A dedicated computational model is developed for the estimation of the spatial distribution of the pressure and lattice deformation inside the crystal. This methodology provides a framework for future novel gamma-ray sources in high-energy facilities.</p></details> |  |
| **[DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method](https://arxiv.org/abs/2508.17054v3)** | 2025-12-22 | <details><summary>Show</summary><p>Previous dominant methods for scene flow estimation focus mainly on input from two consecutive frames, neglecting valuable information in the temporal domain. While recent trends shift towards multi-frame reasoning, they suffer from rapidly escalating computational costs as the number of frames grows. To leverage temporal information more efficiently, we propose DeltaFlow ($Δ$Flow), a lightweight 3D framework that captures motion cues via a $Δ$ scheme, extracting temporal features with minimal computational cost, regardless of the number of frames. Additionally, scene flow estimation faces challenges such as imbalanced object class distributions and motion inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to enhance learning across underrepresented classes and an Instance Consistency Loss to enforce coherent object motion, improving flow accuracy. Extensive evaluations on the Argoverse 2, Waymo and nuScenes datasets show that $Δ$Flow achieves state-of-the-art performance with up to 22% lower error and $2\times$ faster inference compared to the next-best multi-frame supervised method, while also demonstrating a strong cross-domain generalization ability. The code is open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model weights.</p></details> | <details><summary>NeurI...</summary><p>NeurIPS 2025 Spotlight, 18 pages (10 main pages + 8 supp materail), 11 figures, code at https://github.com/Kin-Zhang/DeltaFlow</p></details> |

## Broadband DOA estimation
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Partition Function Estimation Using Analog Quantum Processors](https://arxiv.org/abs/2512.19685v1)** | 2025-12-22 | <details><summary>Show</summary><p>We evaluate using programmable superconducting flux qubit D-Wave quantum annealers to approximate the partition function of Ising models. We propose the use of two distinct quantum annealer sampling methods: chains of Monte Carlo-like reverse quantum anneals, and standard linear-ramp quantum annealing. The control parameters used to attenuate the quality of the simulations are the effective analog energy scale of the J coupling, the total annealing time, and for the case of reverse annealing the anneal-pause. The core estimation technique is to sample across the energy spectrum of the classical Hamiltonian of interest, and therefore obtain a density of states estimate for each energy level, which in turn can be used to compute an estimate of the partition function with some sampling error. This estimation technique is powerful because once the distribution is sampled it allows thermodynamic quantity computation at arbitrary temperatures. On a $25$ spin $\pm J$ hardware graph native Ising model we find parameter regimes of the D-Wave processors that provide comparable result quality to two standard classical Monte Carlo methods, Multiple Histogram Reweighting and Wang-Landau. Remarkably, we find that fast quench-like anneals can quickly generate ensemble distributions that are very good estimates of the true partition function of the classical Ising model; on a Pegasus graph-structured QPU we report a logarithmic relative error of $7.6 \times 10^{-6}$, from $171,000$ samples generated using $0.2$ seconds of QPU time with an anneal time of $8$ nanoseconds per sample which is interestingly within the closed system dynamics timescale of the superconducting qubits.</p></details> |  |
| **[An Adaptive Graphical Lasso Approach to Modeling Symptom Networks of Common Mental Disorders in Eritrean Refugee Population](https://arxiv.org/abs/2512.19681v1)** | 2025-12-22 | <details><summary>Show</summary><p>Despite the significant public health burden of common mental disorders (CMDs) among refugee populations, their underlying symptom structures remain underexplored. This study uses Gaussian graphical modeling to examine the symptom network of post-traumatic stress disorder (PTSD), depression, anxiety, and somatic distress among Eritrean refugees in the Greater Washington, DC area. Given the small sample size (n) and high-dimensional symptom space (p), we propose a novel extension of the standard graphical LASSO by incorporating adaptive penalization, which improves sparsity selection and network estimation stability under n < p conditions. To evaluate the reliability of the network, we apply bootstrap resampling and use centrality measures to identify the most influential symptoms. Our analysis identifies six distinct symptom clusters, with somatic-anxiety symptoms forming the most interconnected group. Notably, symptoms such as nausea and reliving past experiences emerge as central symptoms linking PTSD, anxiety, depression, and somatic distress. Additionally, we identify symptoms like feeling fearful, sleep problems, and loss of interest in activities as key symptoms, either being closely positioned to many others or acting as important bridges that help maintain the overall network connectivity, thereby highlighting their potential importance as possible intervention targets.</p></details> | 34 pages, 7 figures |
| **[FMCW Radar Principles and Human Activity Recognition Systems: Foundations, Techniques, and Applications](https://arxiv.org/abs/2410.08483v2)** | 2025-12-22 | <details><summary>Show</summary><p>This book introduces the theoretical foundations of FMCW radar systems, including range and velocity estimation, signal processing techniques, and the generation of radar point clouds. A detailed discussion of Python and MATLAB as the primary programming tools for radar signal processing is provided, including the integration of libraries like NumPy, Matplotlib, and SciPy for data analysis and visualization. In addition, the book covers advanced techniques such as deep learning applications for radar signal processing, focusing on Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks, and Transformers for analyzing radar data. Furthermore, it highlights state-of-the-art methods for human activity recognition using radar, leveraging a combination of traditional signal processing techniques and machine learning models. The book is designed to cater to both beginners and experts in radar signal processing, offering practical examples, code implementations, and insights into the future of radar technology in various domains, including autonomous systems and security applications.</p></details> | 203pages |
| **[Deep Legendre Transform](https://arxiv.org/abs/2512.19649v1)** | 2025-12-22 | <details><summary>Show</summary><p>We introduce a novel deep learning algorithm for computing convex conjugates of differentiable convex functions, a fundamental operation in convex analysis with various applications in different fields such as optimization, control theory, physics and economics. While traditional numerical methods suffer from the curse of dimensionality and become computationally intractable in high dimensions, more recent neural network-based approaches scale better, but have mostly been studied with the aim of solving optimal transport problems and require the solution of complicated optimization or max-min problems. Using an implicit Fenchel formulation of convex conjugation, our approach facilitates an efficient gradient-based framework for the minimization of approximation errors and, as a byproduct, also provides a posteriori error estimates for the approximation quality. Numerical experiments demonstrate our method's ability to deliver accurate results across different high-dimensional examples. Moreover, by employing symbolic regression with Kolmogorov--Arnold networks, it is able to obtain the exact convex conjugates of specific convex functions.</p></details> | <details><summary>Accep...</summary><p>Accepted at NeurIPS 2025 (poster). NeurIPS page: https://neurips.cc/virtual/2025/loc/san-diego/poster/120307</p></details> |
| **[On the class of exponential statistical structures of type B](https://arxiv.org/abs/2510.26863v2)** | 2025-12-22 | <details><summary>Show</summary><p>The article is devoted to the study of exponential statistical structures of type B, which constitute a subclass of exponential families of probability distributions. This class is characterized by a number of analytical and probabilistic properties that make it a convenient tool for solving both theoretical and applied problems in statistics. The relevance of this research lies in the need to generalize known classes of distributions and to develop a unified framework for their analysis, which is essential for applications in stochastic modeling, machine learning, financial mathematics. The paper proposes a formal definition of type B. Necessary and sufficient conditions for a statistical structure to belong to class B are established, and it is proved that such structures can be represented through a dominating measure with an explicit Laplace transform. The obtained results make it possible to describe a wide range of well-known one-dimensional and multivariate distributions, including the Binomial, Poisson, Normal, Gamma, Polynomial, Logarithmic distributions, as well as specific cases such as the Borel-Tanner and Random Walk distributions. Particular attention is given to the proof of structural theorems that determine the stability of class B under linear transformations and the addition of independent random vectors. Recursive relations for initial and central moments as well as for semi-invariants are obtained, providing an efficient analytical and computational framework for their evaluation. Furthermore, the tails of type B distributions are investigated using the properties of the Laplace transform. New exponential inequalities for estimating the probabilities of large deviations are derived. The obtained results can be applied in theoretical studies and in practical problems of stochastic modeling.</p></details> |  |
| **[Milstein-type Schemes for Hyperbolic SPDEs](https://arxiv.org/abs/2512.19647v1)** | 2025-12-22 | <details><summary>Show</summary><p>This article studies the temporal approximation of hyperbolic semilinear stochastic evolution equations with multiplicative Gaussian noise by Milstein-type schemes. We take the term hyperbolic to mean that the leading operator generates a contractive, not necessarily analytic $C_0$-semigroup. Optimal convergence rates are derived for the pathwise uniform strong error \[ E_h^\infty := \Big(\mathbb{E}\max_{1\le j \le M}\|U_{t_j}-u_j\|_X^p\Big)^{1/p} \] on a Hilbert space $X$ for $p\in [2,\infty)$. Here, $U$ is the mild solution and $u_j$ its Milstein approximation at time $t_j=jh$ with step size $h>0$ and final time $T=Mh>0$. For sufficiently regular nonlinearity and noise, we establish strong convergence of order one, with the error satisfying $E_h^\infty\lesssim h\sqrt{\log(T/h)}$ for rational Milstein schemes and $E_h^\infty \lesssim h$ for exponential Milstein schemes. This extends previous results from parabolic to hyperbolic SPDEs and from exponential to rational Milstein schemes. Root-mean-square error estimates are strengthened to pathwise uniform estimates. Numerical experiments validate the convergence rates for the stochastic Schrödinger equation. Further applications to Maxwell's and transport equations are included.</p></details> | <details><summary>47 pa...</summary><p>47 pages, 3 figure, 4 tables. Comments are welcome!</p></details> |
| **[The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference](https://arxiv.org/abs/2512.19643v1)** | 2025-12-22 | <details><summary>Show</summary><p>Numerical simulation of time-dependent partial differential equations (PDEs) is central to scientific and engineering applications, but high-fidelity solvers are often prohibitively expensive for long-horizon or time-critical settings. Neural operator (NO) surrogates offer fast inference across parametric and functional inputs; however, most autoregressive NO frameworks remain vulnerable to compounding errors, and ensemble-averaged metrics provide limited guarantees for individual inference trajectories. In practice, error accumulation can become unacceptable beyond the training horizon, and existing methods lack mechanisms for online monitoring or correction. To address this gap, we propose ANCHOR (Adaptive Numerical Correction for High-fidelity Operator Rollouts), an online, instance-aware hybrid inference framework for stable long-horizon prediction of nonlinear, time-dependent PDEs. ANCHOR treats a pretrained NO as the primary inference engine and adaptively couples it with a classical numerical solver using a physics-informed, residual-based error estimator. Inspired by adaptive time-stepping in numerical analysis, ANCHOR monitors an exponential moving average (EMA) of the normalized PDE residual to detect accumulating error and trigger corrective solver interventions without requiring access to ground-truth solutions. We show that the EMA-based estimator correlates strongly with the true relative L2 error, enabling data-free, instance-aware error control during inference. Evaluations on four canonical PDEs: 1D and 2D Burgers', 2D Allen-Cahn, and 3D heat conduction, demonstrate that ANCHOR reliably bounds long-horizon error growth, stabilizes extrapolative rollouts, and significantly improves robustness over standalone neural operators, while remaining substantially more efficient than high-fidelity numerical solvers.</p></details> | 18 pages, 7 figures |
| **[Quantum Imaging of Birefringent Samples using Hong-Ou-Mandel Interference](https://arxiv.org/abs/2512.19637v1)** | 2025-12-22 | <details><summary>Show</summary><p>Two-photon interference in a Hong-Ou-Mandel (HOM) interferometer can be used as a quantum sensing mechanism due to the sensitivity of the interference dip to perturbations of the photon indistinguishability. In particular, recent works have generalized this concept to microscopy setups, but the sensitivity to optical path differences constrains its application to samples with thickness variation typically below a few micrometers if tracking changes in the coincidences at a fixed delay. Extending the concept to polarization microscopy and circumventing this limitation, this manuscript explores the use of a narrowband photon pair source with coherence length >1 mm to broaden the HOM dip. Thus, realistic sample-thickness variations introduce negligible temporal distinguishability, and changes in coincidence rate at the dip centre are then dominated by sample-induced polarization effects. To compute the polarization rotation, we develop a statistical model for the interferometer, derive the Fisher information, and establish a maximum-likelihood estimator for the local fast-axis angle. Recording dip and baseline frames at each sample position via raster scanning, the experimental results validate the framework, agreeing with classical polarized-intensity images while demonstrating operation at a maximum-precision regime and insensitiveness to layer thickness. Overall, the approach enclosed provides a quantum-based quantitative imaging of birefringent structures, which can motivate further advantageous applications, including enhanced signal-to-noise ratio and lower damage imaging of photosensitive samples.</p></details> |  |
| **[Approximate co-sufficient sampling with regularization](https://arxiv.org/abs/2309.08063v3)** | 2025-12-22 | <details><summary>Show</summary><p>In this work, we consider the problem of goodness-of-fit (GoF) testing for parametric models. This testing problem involves a composite null hypothesis, due to the unknown values of the model parameters. In some special cases, co-sufficient sampling (CSS) can remove the influence of these unknown parameters via conditioning on a sufficient statistic -- often, the maximum likelihood estimator (MLE) of the unknown parameters. However, many common parametric settings do not permit this approach, since conditioning on a sufficient statistic leads to a powerless test. The recent approximate co-sufficient sampling (aCSS) framework of Barber and Janson (2022) offers an alternative, replacing sufficiency with an approximately sufficient statistic (namely, a noisy version of the MLE). This approach recovers power in a range of settings where CSS cannot be applied, but can only be applied in settings where the unconstrained MLE is well-defined and well-behaved, which implicitly assumes a low-dimensional regime. In this work, we extend aCSS to the setting of constrained and penalized maximum likelihood estimation, so that more complex estimation problems can now be handled within the aCSS framework, including examples such as mixtures-of-Gaussians (where the unconstrained MLE is not well-defined due to degeneracy) and high-dimensional Gaussian linear models (where the MLE can perform well under regularization, such as an $\ell_1$ penalty or a shape constraint).</p></details> |  |
| **[LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry](https://arxiv.org/abs/2512.19629v1)** | 2025-12-22 | <details><summary>Show</summary><p>Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the \href{https://steinate.github.io/logoplanner.github.io/}{project page}.</p></details> | <details><summary>Proje...</summary><p>Project page:https://steinate.github.io/logoplanner.github.io/</p></details> |
| **[Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior](https://arxiv.org/abs/2512.19584v1)** | 2025-12-22 | <details><summary>Show</summary><p>Dynamic PET enables the quantitative estimation of physiology-related parameters and is widely utilized in research and increasingly adopted in clinical settings. Parametric imaging in dynamic PET requires kinetic modeling to estimate voxel-wise physiological parameters based on specific kinetic models. However, parametric images estimated through kinetic model fitting often suffer from low image quality due to the inherently ill-posed nature of the fitting process and the limited counts resulting from non-continuous data acquisition across multiple bed positions in whole-body PET. In this work, we proposed a diffusion model-based kinetic modeling framework for parametric image estimation, using the Patlak model as an example. The score function of the diffusion model was pre-trained on static total-body PET images and served as a prior for both Patlak slope and intercept images by leveraging their patch-wise similarity. During inference, the kinetic model was incorporated as a data-consistency constraint to guide the parametric image estimation. The proposed framework was evaluated on total-body dynamic PET datasets with different dose levels, demonstrating the feasibility and promising performance of the proposed framework in improving parametric image quality.</p></details> | 10 pages, 9 figures |
| **[A modified Brinkman penalization fictitious domain method for the unsteady Navier-Stokes equations](https://arxiv.org/abs/2512.19580v1)** | 2025-12-22 | <details><summary>Show</summary><p>This paper investigates a modification of the fictitious domain method with continuation in the lower-order coefficients for the unsteady Navier-Stokes equations governing the motion of an incompressible homogeneous fluid in a bounded 2D or 3D domain. The modification enables {a solution-dependent} choice of the critical parameter. Global-in-time existence and convergence of a weak solution to the auxiliary problem are proved, and local-in-time existence and convergence of a unique strong solution are established. For the strong solution, a new higher-order convergence rate estimate in the penalization parameter is obtained. The introduced framework allows us to apply a pointwise divergence free finite element method as a discretization technique, leading to strongly mass conservative discrete fictitious domain method. A numerical example illustrates the performance of the method.</p></details> |  |
| **[Deep Learning for Primordial $B$-mode Extraction](https://arxiv.org/abs/2512.19577v1)** | 2025-12-22 | <details><summary>Show</summary><p>The search for primordial gravitational waves is a central goal of cosmic microwave background (CMB) surveys. Isolating the characteristic $B$-mode polarization signal sourced by primordial gravitational waves is challenging for several reasons: the amplitude of the signal is inherently small; astrophysical foregrounds produce $B$-mode polarization contaminating the signal; and secondary $B$-mode polarization fluctuations are produced via the conversion of $E$ modes. Current and future low-noise, multi-frequency observations enable sufficient precision to address the first two of these challenges such that secondary $B$ modes will become the bottleneck for improved constraints on the amplitude of primordial gravitational waves. The dominant source of secondary $B$-mode polarization is gravitational lensing by large scale structure. Various strategies have been developed to estimate the lensing deflection and to reverse its effects the CMB, thus reducing confusion from lensing $B$ modes in the search for primordial gravitational waves. However, a few complications remain. First, there may be additional sources of secondary $B$-mode polarization, for example from patchy reionization or from cosmic polarization rotation. Second, the statistics of delensed CMB maps can become complicated and non-Gaussian, especially when advanced lensing reconstruction techniques are applied. We previously demonstrated how a deep learning network, ResUNet-CMB, can provide nearly optimal simultaneous estimates of multiple sources of secondary $B$-mode polarization. In this paper, we show how deep learning can be applied to estimate and remove multiple sources of secondary $B$-mode polarization, and we further show how this technique can be used in a likelihood analysis to produce nearly optimal, unbiased estimates of the amplitude of primordial gravitational waves.</p></details> | <details><summary>12 pa...</summary><p>12 pages, 8 figures. Code available from https://github.com/EEmGuzman/resunet-cmb</p></details> |
| **[Degeneration of Riemann surfaces and small eigenvalues of the Laplacian](https://arxiv.org/abs/2509.06151v2)** | 2025-12-22 | <details><summary>Show</summary><p>For a one-parameter degeneration of compact Riemann surfaces endowed with the Kähler metric induced from the Kähler metric on the total space of the family, we determine the exact magnitude of the small eigenvalues of the Laplacian as a function on the parameter space, under the assumption that the singular fiber is reduced. The novelty in our approach is that we compute the asymptotic behavior of certain difference of (logarithm of) analytic torsions in the degeneration in two ways. On the one hand, via heat kernel estimates, it is shown that the leading asymptotic is determined by the product of the small eigenvalues. On the other hand, using Quillen metrics, the leading asymptotic is connected with the period integrals, which we explicitly evaluate.</p></details> | 45 pages |
| **[Optimal Uncertainty Quantification under General Moment Constraints on Input Subdomains](https://arxiv.org/abs/2512.19572v1)** | 2025-12-22 | <details><summary>Show</summary><p>We present an optimal uncertainty quantification (OUQ) framework for systems whose uncertain inputs are characterized by truncated moment constraints defined over subdomains. Based on this partial information, rigorous optimal upper and lower bounds on the probability of failure (PoF) are derived over the admissible set of probability measures, providing a principled basis for system safety certification. We formulate the OUQ problem under general subdomain moment constraints and develop a high-performance computational framework to compute the optimal bounds. This approach transforms the original infinite-dimensional optimization problems into finite-dimensional unconstrained ones parameterized solely by free canonical moments. To address the prohibitive cost of PoF evaluation in high-dimensional settings, we incorporate inverse transform sampling (ITS), enabling efficient and accurate PoF estimation within the OUQ optimization. We also demonstrate that constraining inputs only by zeroth-order moments over subdomains yields a formulation equivalent to evidence theory. Three groups of numerical examples demonstrate the framework's effectiveness and scalability. Results show that increasing the number of subdomains or the moment order systematically tightens the bound interval. For high-dimensional problems, the ITS strategy reduces computational costs by up to two orders of magnitude while maintaining relative error below 1%. Furthermore, we identify regimes where optimal bounds are sensitive to subdomain partitioning or higher-order moments, guiding uncertainty reduction efforts for safety certification.</p></details> | 31 pages, 13 figures |
| **[Owning the Intelligence: Global AI Patents Landscape and Europe's Quest for Technological Sovereignty](https://arxiv.org/abs/2512.19569v1)** | 2025-12-22 | <details><summary>Show</summary><p>Artificial intelligence has become a key arena of global technological competition and a central concern for Europe's quest for technological sovereignty. This paper analyzes global AI patenting from 2010 to 2023 to assess Europe's position in an increasingly bipolar innovation landscape dominated by the United States and China. Using linked patent, firm, ownership, and citation data, we examine the geography, specialization, and international diffusion of AI innovation. We find a highly concentrated patent landscape: China leads in patent volumes, while the United States dominates in citation impact and technological influence. Europe accounts for a limited share of AI patents but exhibits signals of relatively high patent quality. Technological proximity reveals global convergence toward U.S. innovation trajectories, with Europe remaining fragmented rather than forming an autonomous pole. Gravity-model estimates show that cross-border AI knowledge flows are driven primarily by technological capability and specialization, while geographic and institutional factors play a secondary role. EU membership does not significantly enhance intra-European knowledge diffusion, suggesting that technological capacity, rather than political integration, underpins participation in global AI innovation networks.</p></details> |  |
| **[A Statistical Framework for Understanding Causal Effects that Vary by Treatment Initiation Time in EHR-based Studies](https://arxiv.org/abs/2512.19553v1)** | 2025-12-22 | <details><summary>Show</summary><p>Comparative effectiveness studies using electronic health records (EHR) consider data from patients who could ``enter'' the study cohort at any point during an interval that spans many years in calendar time. Unlike treatments in tightly controlled trials, real-world treatments can evolve over calendar time, especially if comparators include standard of care, or procedures where techniques may improve. Efforts to assess whether treatment efficacy itself is changing are complicated by changing patient populations, with potential covariate shift in key effect modifiers. In this work, we propose a statistical framework to estimate calendar-time specific average treatment effects and describe both how and why effects vary across treatment initiation time in EHR-based studies. Our approach projects doubly robust, time-specific treatment effect estimates onto candidate marginal structural models and uses a model selection procedure to best describe how effects vary by treatment initiation time. We further introduce a novel summary metric, based on standardization analysis, to quantify the role of covariate shift in explaining observed effect changes and disentangle changes in treatment effects from changes in the patient population receiving treatment. Extensive simulations using EHR data from Kaiser Permanente are used to validate the utility of the framework, which we apply to study changes in relative weight loss following two bariatric surgical interventions versus no surgery among patients with severe obesity between 2005-2011.</p></details> |  |
| **[Yang-Mills energy quantization over non-collapsed degenerating Einstein manifolds and applications](https://arxiv.org/abs/2512.19552v1)** | 2025-12-22 | <details><summary>Show</summary><p>We investigate a sequence of Yang-Mills connections $A_j$ lying in vector bundles $E_j$ over non-collapsed degenerating closed Einstein 4-manifolds $(M_j, g_ j)$ with uniformly bounded Einstein constants and bounded diameters. We establish a compactness theory modular three types of bubbles. As applications, we get some quantization results for several important topological number associated with the vector bundles, for instance, the first Pontrjagin numbers $p_1(E)$ of vector bundles over Einstein 4-manifolds and the Euler numbers $χ(M;E)$ of holomorphic vector bundles over Kähler-Einstein surfaces. Furthermore, we get some quantization results about the volume $v(L_j)$ and certain cohomological numbers (e.g. $dim H^0(M_j;L_j)$) of holomorphic line bundles $L_j$ over non-collapsed degenerating Kähler-Einstein surfaces $(M_j,J_j,g_j)$ with the aid of the classical vanishing theorems, the classical Hirzebruch-Riemann-Roch type theorems, and the profound convergence theory of Kähler-Einstein manifolds. In particular, we obtain some interesting identities involving non-collapsed degenerating compact Kähler-Einstein surfaces with non-zero scalar curvature, which indicate that we can know the Euler number of $M_j$ for large $j$ provided some topological information of the limit orbifold $M_\infty$. For Kähler-Einstein Del Pezzo surfaces, an interesting implication is that we can provide some preliminary estimates for the number of singularities of various types in $M_\infty$ in an effective way. As an unexpected surprise, we find an identity which connects Milnor numbers for singularities in $M_\infty$ and the correction terms in the Hirzebruch-Riemann-Roch theorem for orbifolds. Some quantization results can be extended to the case of higher dimensional $n$-manifolds.</p></details> | Comments welcome! |
| **[A Reynolds-semi-robust method with hybrid velocity and pressure for the unsteady incompressible Navier--Stokes equations](https://arxiv.org/abs/2502.15293v2)** | 2025-12-22 | <details><summary>Show</summary><p>In this paper we propose and analyze a new Finite Element method for the solution of the two- and three-dimensional incompressible Navier--Stokes equations based on a hybrid discretization of both the velocity and pressure variables. The proposed method is pressure-robust, i.e., irrotational forcing terms do not affect the approximation of the velocity, and Reynolds-quasi-robust, with error estimates that, for smooth enough exact solutions, do not depend on the inverse of the viscosity. We carry out an in-depth convergence analysis highlighting pre-asymptotic convergence rates and validate the theoretical findings with a complete set of numerical experiments.</p></details> |  |
| **[QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models](https://arxiv.org/abs/2512.19526v1)** | 2025-12-22 | <details><summary>Show</summary><p>Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.</p></details> |  |
| **[Estimating Spatially Resolved Radiation Fields Using Neural Networks](https://arxiv.org/abs/2512.17654v2)** | 2025-12-22 | <details><summary>Show</summary><p>We present an in-depth analysis on how to build and train neural networks to estimate the spatial distribution of scattered radiation fields for radiation protection dosimetry in medical radiation fields, such as those found in interventional radiology and cardiology. We present three different synthetically generated datasets with increasing complexity for training, using a Monte-Carlo Simulation application based on Geant4. On those datasets, we evaluate convolutional and fully connected architectures of neural networks to demonstrate which design decisions work well for reconstructing the fluence and spectra distributions over the spatial domain of such radiation fields. All our datasets, as well as our training pipeline, are published as open source in separate repositories.</p></details> |  |
| **[Spectral representations of interpolation spaces of reproducing kernel Hilbert spaces](https://arxiv.org/abs/2508.16492v2)** | 2025-12-22 | <details><summary>Show</summary><p>In statistical learning theory, interpolation spaces of the form $[\mathrm{L}^2,H]_{θ,r}$, where $H$ is a reproducing kernel Hilbert space, are in widespread use. So far, however, they are only well understood for fine index $r=2$. We generalise existing results from $r=2$ to all possible values of $r$. In particular, we present a spectral decomposition of such spaces, analyse their embedding properties, and describe connections to the theory of Banach spaces of functions. We additionally present example applications of our results to regularisation error estimation in statistical learning.</p></details> | 29 pages |
| **[Addition is almost all you need: Compressing neural networks with double binary factorization](https://arxiv.org/abs/2505.11076v3)** | 2025-12-22 | <details><summary>Show</summary><p>Binary quantization approaches, which replace weight matrices with binary matrices and substitute costly multiplications with cheaper additions, offer a computationally efficient approach to address the increasing computational and storage requirements of Large Language Models (LLMs). However, the severe quantization constraint ($\pm1$) can lead to significant accuracy degradation. In this paper, we propose Double Binary Factorization (DBF), a novel method that factorizes dense weight matrices into products of two binary (sign) matrices, each accompanied by scaling vectors. DBF preserves the efficiency advantages of binary representations while achieving compression rates that are competitive with or superior to state-of-the-art methods. Specifically, in a 1-bit per weight range, DBF is better than existing binarization approaches. In a 2-bit per weight range, DBF is competitive with the best quantization methods like QuIP\# and QTIP. Unlike most existing compression techniques, which offer limited compression level choices, DBF allows fine-grained control over compression ratios by adjusting the factorization's intermediate dimension. Based on this advantage, we further introduce an algorithm for estimating non-uniform layer-wise compression ratios for DBF, based on previously developed channel pruning criteria. Code available at: https://github.com/usamec/double_binary</p></details> |  |
| **[Energy dissipation mechanisms in an acoustically-driven slit](https://arxiv.org/abs/2512.19507v1)** | 2025-12-22 | <details><summary>Show</summary><p>We quantify how incident acoustic energy is converted into vortical motion and viscous dissipation for a two-dimensional plane-wave passing through a slit geometry. We perform direct numerical simulations over a broad parameter space in incident sound pressure level (ISPL), Strouhal number (St), and Reynolds number (Re). Spectral proper orthogonal decomposition (SPOD) yields energy-ranked coherent structures at each frequency, from which we construct mode-by-mode fields for spectral kinetic energy (KE) and viscous loss (VL) components to examine the mechanisms of acoustic absorption. At ISPL=150dB, the acoustic-hydrodynamic energy conversion is highest when the acoustic displacement amplitude is comparable to the slit thickness, corresponding to a Keulegan-Carpenter number of order unity. In this regime, the oscillatory boundary layer undergoes periodic separation, resulting in vortex shedding that dominates acoustic damping. VL accounts for 20-60% of the KE contribution. For higher acoustic frequencies, the confinement of the Stokes layer produces X-shaped near-slit modes, reducing the total energy input by approximately 50%. The influence of Re depends on amplitude. At ISPL=150dB, larger Re values correspond to suppressed broadband fluctuations and sharpened harmonic peaks. At ISPL = 120dB, the boundary layers remain attached, vortex shedding is weak, absorption monotonically scales with viscosity, and the Re- and St-dependencies become comparable. Across all conditions, more than 99% of the VL is confined to a compact region surrounding the slit mouth. The KE-VL spectra describe parameter regimes that enhance or suppress acoustic damping in slit geometries, providing a physically interpretable basis for acoustic-based design.</p></details> |  |
| **[Error Estimates for Sparse Tensor Products of B-spline Approximation Spaces](https://arxiv.org/abs/2510.21517v3)** | 2025-12-22 | <details><summary>Show</summary><p>This work introduces and analyzes B-spline approximation spaces defined on general geometric domains obtained through a mapping from a parameter domain. These spaces are constructed as sparse-grid tensor products of univariate spaces in the parameter domain and are mapped to the physical domain via a geometric parametrization. Both the univariate approximation spaces and the geometric mapping are built using maximally smooth B-splines. We construct two such spaces, employing either the sparse-grid combination technique or the hierarchical subspace decomposition of sparse-grid tensor products, and we prove their mathematical equivalence. Furthermore, we derive approximation error estimates and inverse inequalities that highlight the advantages of sparse-grid tensor products. Specifically, under suitable regularity assumptions on the solution, these spaces achieve the same approximation order as standard tensor product spaces while using significantly fewer degrees of freedom. Additionally, our estimates indicate that, in the case of non-tensor-product domains, stronger regularity assumptions on the solution -- particularly concerning isotropic (non-mixed) derivatives -- are required to achieve optimal convergence rates compared to sparse-grid methods defined on tensor-product domains.</p></details> |  |
| **[Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks](https://arxiv.org/abs/2511.14455v3)** | 2025-12-22 | <details><summary>Show</summary><p>We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\varphi=\varphi(x,u)$ such that $\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.</p></details> |  |
| **[Large deviations for stochastic evolution equations beyond the coercive case](https://arxiv.org/abs/2512.19501v1)** | 2025-12-22 | <details><summary>Show</summary><p>We prove the small-noise large deviation principle (LDP) for stochastic evolution equations in an $L^2$-setting. As the coefficients are allowed to be non-coercive, our framework encompasses a much broader scope than variational settings. To replace coercivity, we require only well-posedness of the stochastic evolution equation and two concrete, verifiable a priori estimates. Furthermore, we accommodate drift nonlinearities satisfying a modified criticality condition, and we allow for vanishing drift perturbations. The latter permits the inclusion of Itô--Stratonovich correction terms, enabling the treatment of both noise interpretations. In another paper, our results have been applied to the 3D primitive equations with full transport noise. In the current paper, we give an application to a reaction-diffusion system which lacks coercivity, further demonstrating the versatility of the framework. Finally, we show that even in the coercive case, we obtain new LDP results for equations with critical nonlinearities that rely on our modified criticality condition, including the stochastic 2D Allen--Cahn equation in the weak setting.</p></details> |  |
| **[Asymptotic theory for nonparametric testing of $k$-monotonicity in discrete distributions](https://arxiv.org/abs/2407.01751v2)** | 2025-12-22 | <details><summary>Show</summary><p>In shape-constrained nonparametric inference, it is often necessary to perform preliminary tests to verify whether a probability mass function (p.m.f.) satisfies qualitative constraints such as monotonicity, convexity, or in general $k$-monotonicity. In this paper, we are interested in nonparametric testing of $k$-monotonicity of a finitely supported discrete distribution. We consider a unified testing framework based on a natural statistic which is directly derived from the very definition of $k$-monotonicity. The introduced framework allows us to design a new consistent method to select the unknown knot points that are required to consistently approximate the limit distribution of several test statistics based either on the empirical measure or the shape-constrained estimators of the p.m.f. We show that the resulting tests are asymptotically valid and consistent for any fixed alternative. Additionally, for the test based solely on the empirical measure, we study the asymptotic power under contiguous alternatives and derive a quantitative separation result that provides sufficient conditions to achieve a given power. We employ this test to design an estimator for the largest parameter $k \in \mathbb N_0$ such that the p.m.f. is $j$-monotone for all $j = 0, \ldots, k$, and show that the estimator is different from the true parameter with probability which is asymptotically smaller than the nominal level of the test. A detailed simulation study is performed to assess the finite sample performance of all the proposed tests, and applications to several real datasets are presented to illustrate the theory.</p></details> |  |
| **[Causal inference with misspecified network interference structure](https://arxiv.org/abs/2302.11322v3)** | 2025-12-22 | <details><summary>Show</summary><p>Under interference, the treatment of one unit may affect the outcomes of other units. Such interference patterns between units are typically represented by a network. Correctly specifying this network requires identifying which units can affect others -- an inherently challenging task. Nevertheless, most existing approaches assume that a known and accurate network specification is given. In this paper, we study the consequences of such misspecification. We derive bounds on the bias arising from estimating causal effects using a misspecified network, showing that the estimation bias grows with the divergence between the assumed and true networks, quantified through their induced exposure probabilities. To address this challenge, we propose a novel estimator that leverages multiple networks simultaneously and remains unbiased if at least one of the networks is correct, even when we do not know which one. Therefore, the proposed estimator provides robustness to network specification. We illustrate key properties and demonstrate the utility of our proposed estimator through simulations and analysis of a social network field experiment.</p></details> |  |
| **[3DFETUS: Deep Learning-Based Standardization of Facial Planes in 3D Ultrasound](https://arxiv.org/abs/2511.10412v2)** | 2025-12-22 | <details><summary>Show</summary><p>The automatic localization and standardization of anatomical planes in 3D medical imaging remains a challenging problem due to variability in object pose, appearance, and image quality. In 3D ultrasound, these challenges are exacerbated by speckle noise and limited contrast, particularly in fetal imaging. To address these challenges in the context of facial assessment, we present: 1) GT++, a robust algorithm that estimates standard facial planes from 3D US volumes using annotated anatomical landmarks; and 2) 3DFETUS, a deep learning model that automates and standardizes their localization in 3D fetal US volumes. We evaluated our methods both qualitatively, through expert clinical review, and quantitatively. The proposed approach achieved a mean translation error of 3.21 $\pm$ 1.98mm and a mean rotation error of 5.31 $\pm$ 3.945$^\circ$ per plane, outperforming other state-of-the-art methods on 3D US volumes. Clinical assessments further confirmed the effectiveness of both GT++ and 3DFETUS, demonstrating statistically significant improvements in plane estimation accuracy.</p></details> |  |
| **[Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications](https://arxiv.org/abs/2512.19472v1)** | 2025-12-22 | <details><summary>Show</summary><p>The recent explosive growth in Deep Neural Networks applications raises concerns about the black-box usage of such models, with limited trasparency and trustworthiness in high-stakes domains, which have been crystallized as regulatory requirements such as the European Union Artificial Intelligence Act. While models with embedded confidence metrics have been proposed, such approaches cannot be applied to already existing models without retraining, limiting their broad application. On the other hand, post-hoc methods, which evaluate pre-trained models, focus on solving problems related to improving the confidence in the model's predictions, and detecting Out-Of-Distribution or Adversarial Attacks samples as independent applications. To tackle the limited applicability of already existing methods, we introduce Multi-Layer Analysis for Confidence Scoring (MACS), a unified post-hoc framework that analyzes intermediate activations to produce classification-maps. From the classification-maps, we derive a score applicable for confidence estimation, detecting distributional shifts and adversarial attacks, unifying the three problems in a common framework, and achieving performances that surpass the state-of-the-art approaches in our experiments with the VGG16 and ViTb16 models with a fraction of their computational overhead.</p></details> |  |
| **[Fully Asynchronous Unsourced Random Access over Fading Channels](https://arxiv.org/abs/2512.19468v1)** | 2025-12-22 | <details><summary>Show</summary><p>We examine unsourced random access in a fully asynchronous setup, where active users transmit their data without restriction on the start time over a fading channel. In the proposed scheme, the transmitted signal consists of a pilot sequence and a polar codeword, with the polar codeword distributed across the data part of the packet in an on-off pattern. The receiver uses a double sliding-window decoder, where the inner window employs iterative decoding with joint timing and pilot detection, channel estimation, single-user decoding, and successive interference cancellation to recover the message bits, while the outer window enhances interference cancellation. The numerical results indicate that the proposed scheme exhibits only a slight performance loss compared to the synchronous benchmark while being more applicable in practice.</p></details> |  |
| **[Gap-free Information Transfer in 4D-STEM via Fusion of Complementary Scattering Channels](https://arxiv.org/abs/2512.19460v1)** | 2025-12-22 | <details><summary>Show</summary><p>Linear phase-contrast scanning transmission electron microscopy (STEM) techniques compatible with high-throughput 4D-STEM acquisition are widely used to enhance phase contrast in weakly scattering and beam-sensitive materials. In these modalities, contrast transfer is often suppressed at low spatial frequencies, resulting in a characteristic contrast gap that limits quantitative imaging. Approaches that retain low-frequency phase contrast exist but typically require substantially increased experimental complexity, restricting routine use. Dark-field STEM imaging captures this missing low-frequency information through electrons scattered outside the bright-field disk, but discards a large fraction of the scattered signal and is therefore dose-inefficient. Fused Full-field STEM (FF-STEM) is introduced as a 4D-STEM imaging modality that overcomes this limitation by combining ptychographic phase reconstruction with tilt-corrected dark-field imaging within a single acquisition. Bright-field data are used to estimate probe aberrations and reconstruct a high-resolution phase image, while dark-field data provide complementary low-frequency contrast. The two channels are optimally fused in Fourier space using minimum-variance weighting based on the spectral signal-to-noise ratio, yielding transfer-gap-free images with high contrast and quantitative fidelity. FF-STEM preserves the upsampling and depth-sectioning capabilities of ptychography, adds robust low-frequency contrast characteristic of dark-field imaging, and enables dose-efficient, near-real-time reconstruction.</p></details> |  |
| **[Two-Dimensional Tomographic Reconstruction From Projections With Unknown Angles and Unknown Spatial Shifts](https://arxiv.org/abs/2511.22890v2)** | 2025-12-22 | <details><summary>Show</summary><p>In parallel beam computed tomography (CT), an object is reconstructed from a series of projections taken at different angles. However, in some industrial and biomedical imaging applications, the projection geometry is unknown, completely or partially. In this paper, we present a technique for two-dimensional (2D) tomography in which both viewing angles and spatial shifts associated with the projections are unknown. There exists literature on 2D unknown view tomography (UVT), but most existing 2D UVT algorithms assume that the projections are centered; that is, there are no spatial shifts in the projections. To tackle these geometric ambiguities, we first modify an existing graph Laplacian-based algorithm for 2D UVT to incorporate spatial shifts, and then use it as the initialization for the proposed three-way alternating minimization algorithm that jointly estimates the 2D structure, its projection angles, and the corresponding shifts. We evaluate our method on noisy projections of ribosome images and demonstrate that it achieves superior reconstruction compared to the baseline that neglects shifts.</p></details> | <details><summary>5 pag...</summary><p>5 pages, 2 figures, 1 table, submitted to the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</p></details> |
| **[Regularity for fully nonlinear elliptic equations in generalized Orlicz spaces](https://arxiv.org/abs/2512.16600v2)** | 2025-12-22 | <details><summary>Show</summary><p>In this paper, we establish an optimal global Calderón-Zygmund type estimate for the viscosity solution to the Dirichlet boundary problem of fully nonlinear elliptic equations with possibly nonconvex nonlinearities. We prove that the Hessian of the solution is as integrable as the nonhomogeneous term in the setting of a given generalized Orlicz space even when the nonlinearity is asymptotically convex with respect to the Hessian of the solution.</p></details> |  |
| **[Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm](https://arxiv.org/abs/2512.19440v1)** | 2025-12-22 | <details><summary>Show</summary><p>Kernel logistic regression (KLR) is a widely used supervised learning method for binary and multi-class classification, which provides estimates of the conditional probabilities of class membership for the data points. Unlike other kernel methods such as Support Vector Machines (SVMs), KLRs are generally not sparse. Previous attempts to deal with sparsity in KLR include a heuristic method referred to as the Import Vector Machine (IVM) and ad hoc regularizations such as the $\ell_{1/2}$-based one. Achieving a good trade-off between prediction accuracy and sparsity is still a challenging issue with a potential significant impact from the application point of view. In this work, we revisit binary KLR and propose an extension of the training formulation proposed by Keerthi et al., which is able to induce sparsity in the trained model, while maintaining good testing accuracy. To efficiently solve the dual of this formulation, we devise a decomposition algorithm of Sequential Minimal Optimization type which exploits second-order information, and for which we establish global convergence. Numerical experiments conducted on 12 datasets from the literature show that the proposed binary KLR approach achieves a competitive trade-off between accuracy and sparsity with respect to IVM, $\ell_{1/2}$-based regularization for KLR, and SVM while retaining the advantages of providing informative estimates of the class membership probabilities.</p></details> |  |
| **[Brownian bridge limit of path measures in the upper tail of KPZ models](https://arxiv.org/abs/2311.12009v2)** | 2025-12-22 | <details><summary>Show</summary><p>For models in the KPZ universality class, such as the zero temperature model of planar last passage-percolation (LPP) and the positive temperature model of directed polymers, its upper tail behavior has been a topic of recent interest, with particular focus on the associated path measures (i.e., geodesics or polymers). For Exponential LPP, diffusive fluctuation had been established in Basu-Ganguly. In the directed landscape, the continuum limit of LPP, the limiting Gaussianity at one point, as well as of related finite-dimensional distributions of the KPZ fixed point, were established, using exact formulas in Liu and Wang-Liu. It was further conjectured in these works that the limit of the corresponding geodesic should be a Brownian bridge. We prove it in both zero and positive temperatures; for the latter, neither the one-point limit nor the scale of fluctuations was previously known. Instead of relying on formulas (which are still missing in the positive temperature literature), our arguments are geometric and probabilistic, using the results on the shape of the weight and free energy profiles under the upper tail from Ganguly-Hegde as a starting point. Another key ingredient involves novel coalescence estimates, developed using the recently discovered shift-invariance Borodin-Gorin-Wheeler in these models. Finally, our proof also yields insight into the structure of the polymer measure under the upper tail conditioning, establishing a quenched localization exponent around a random backbone.</p></details> | <details><summary>77 pa...</summary><p>77 pages, 6 figures. Updated to fix an error, relying on the form of the BK inequality recently established in arXiv:2512.17823</p></details> |
| **[Sharp upper tail behavior of line ensembles via the tangent method](https://arxiv.org/abs/2208.08922v3)** | 2025-12-22 | <details><summary>Show</summary><p>We develop a new probabilistic and geometric method to obtain several sharp results pertaining to the upper tail behavior of continuum Gibbs measures on infinite ensembles of random continuous curves, also known as line ensembles, satisfying some natural assumptions. The arguments make crucial use of Brownian resampling invariance properties and correlation inequalities admitted by such Gibbs measures. We obtain sharp one-point upper tail estimates showing that the probability of the value at zero being larger than $θ$ is $\exp(-\frac{4}{3}θ^{3/2}(1+o(1)))$. A key intermediate step is developing a precise understanding of the profile when conditioned on the value at zero equaling $θ$. Our method further allows one to obtain multi-point asymptotics which were out of reach of previous approaches. As an example, we prove sharp explicit two-point upper tail estimates. This framework is then used to establish the corresponding results for the KPZ equation, which are all new. Even for the zero-temperature case of the Airy$_2$ process, our arguments yield new proofs for one-point estimates previously known due to its connections to random matrix theory, as well as new two-point asymptotics. To showcase the reach of the method, we obtain the same results in a purely non-integrable setting under only assumptions of stationarity and extremality in the class of Gibbs measures. Our method bears resemblance to the tangent method introduced by Colomo-Sportiello and mathematically realized by Aggarwal in the context of the six-vertex model.</p></details> | <details><summary>96 pa...</summary><p>96 pages, 8 figures. v3: Previously conditional results for the KPZ line ensemble have been made unconditional using arXiv:2512.17823. Abstract suitably modified</p></details> |
| **[Hybrid Analytical-Machine Learning Framework for Ripple Factor Estimation in Cockcroft-Walton Voltage Multipliers with Residual Correction for Non-Ideal Effects](https://arxiv.org/abs/2512.19434v1)** | 2025-12-22 | <details><summary>Show</summary><p>Cockcroft-Walton (CW) voltage multipliers suffer from output ripple that classical analytical models underestimate due to neglected non-idealities like diode drops and capacitor ESR, particularly in high-stage, low-frequency and heavy-load regimes. This paper proposes a hybrid framework that generates a comprehensive 324-case MATLAB/Simulink dataset varying stages (2-8), input voltage (5-25 kV), capacitance (1-10 μF), frequency (50-500 Hz) and load (6-60 MΩ), then trains a Random Forest model to predict residuals between simulated and theoretical peak-to-peak ripple. The approach achieves 70.6% RMSE reduction (131 V vs. 448 V) globally and 66.7% in critical regimes, with near-zero bias, enabling physically interpretable design optimization while outperforming pure ML in extrapolation reliability.</p></details> | <details><summary>6 Pag...</summary><p>6 Pages, 2 figures, IEEE Conference Template used</p></details> |
| **[AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting](https://arxiv.org/abs/2505.18822v2)** | 2025-12-22 | <details><summary>Show</summary><p>Modern large reasoning models demonstrate impressive problem-solving capabilities by employing sophisticated reasoning strategies. However, they often struggle to balance efficiency and effectiveness, frequently generating unnecessarily lengthy reasoning chains for simple problems. In this work, we propose AdaCtrl, a novel framework to support both difficulty-aware adaptive reasoning budget allocation and explicit user control over reasoning depth. AdaCtrl dynamically adjusts its reasoning length based on self-assessed problem difficulty, while also allowing users to manually control the budget to prioritize either efficiency or effectiveness. This is achieved through a two-stage training pipeline: an initial cold-start fine-tuning phase to instill the ability to self-aware difficulty and adjust reasoning budget, followed by a difficulty-aware reinforcement learning (RL) stage that refines the model's adaptive reasoning strategies and calibrates its difficulty assessments based on its evolving capabilities during online training. To enable intuitive user interaction, we design explicit length-triggered tags that function as a natural interface for budget control. Empirical results show that AdaCtrl adapts reasoning length based on estimated difficulty, compared to the standard training baseline that also incorporates fine-tuning and RL, it yields performance improvements and simultaneously reduces response length by 10.06% and 12.14% on the more challenging AIME2024 and AIME2025 datasets, which require elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K datasets, where more concise responses are sufficient. Furthermore, AdaCtrl enables precise user control over the reasoning budget, allowing for tailored responses to meet specific needs.</p></details> |  |
| **[Hölder regularity of doubly nonlinear nonlocal quasilinear parabolic equations in some mixed singular-degenerate regime](https://arxiv.org/abs/2512.19421v1)** | 2025-12-22 | <details><summary>Show</summary><p>We study local Hölder regularity of bounded, weak solutions for the nonlocal quasilinear equations of the form \[ (|u|^{q-2}u)_t + \text{P.V.} \int_{\mathbb{R}^n} \frac{|u(x,t) - u(y,t)|^{p-2}(u(x,t)-u(y,t))}{|x-y|^{n+sp}} dy = 0, \] with $p\in (1,\infty)$, $q\in (1,\infty)$ and $s \in (0,1)$. Analogous Hölder continuity result in the local case is known in the purely singular case $\{1<p<2, p<q\}$, purely degenerate case $\{2<p, q<p\}$, scale invariant case $\{p=q\}$ and translation invariant case $\{q=2,1<p<\infty\}$. In the nonlocal setting, Hölder regularity is known when the equation is either translation invariant $\{q=2, 1<p<\infty\}$ or scale invariant $\{q=p, 1<p<\infty\}$ or purely degenerate case $\{2<p, q<p\}$. Similar strategy can be used to obtain Hölder regularity in the purely singular case $\{1<p<2, p<q\}$. In this paper, we adapt several ideas developed over the past few years and combine it with a new intrinsic scaling to prove Hölder regularity in the mixed singular-degenerate range $\max\{p,q,2\} < \min\left\{q + \tfrac{p-1}{1+\frac{n}{sp}}, 2 + \tfrac{p-1}{1+\frac{n}{sp}}\right\}$. The proof explicitly makes use of the nonlocal nature of the problem and as a consequence, our estimates are not stable at $s \rightarrow 0$. We note that the analogous regularity in the local problem remains open.</p></details> | 43 pages |
| **[Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming](https://arxiv.org/abs/2512.15735v2)** | 2025-12-22 | <details><summary>Show</summary><p>This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.</p></details> | <details><summary>we ha...</summary><p>we have identified some technical issues, including the mathematical derivation. After discussion, all authors have agreed that the analysis requires a thorough re-derivation to ensure correctness and rigor</p></details> |
| **[Estimates for short character sums evaluated at homogeneous polynomials](https://arxiv.org/abs/2501.12325v2)** | 2025-12-22 | <details><summary>Show</summary><p>Let $p$ be a prime. We prove bounds on short Dirichlet character sums evaluated at a class of homogeneous polynomials in arbitrary dimensions. In every dimension, this bound is nontrivial for sums over boxes with side lengths as short as $p^{1/4 + κ}$ for any $κ>0$. Our methods capitalize on the relationship between characters mod $p$ and characters over finite field extensions as well as bounds on the multiplicative energy of sets in products of finite fields.</p></details> | <details><summary>47 pa...</summary><p>47 pages; v2 aligns with published version</p></details> |
| **[Uniform pathwise stability of additive singular SDEs driven by fractional Brownian motion](https://arxiv.org/abs/2511.05262v2)** | 2025-12-22 | <details><summary>Show</summary><p>We study the long-time behaviour of solutions to a class of $d$-dimensional stochastic differential equations driven by fractional Brownian motion with Hurst parameter $H \in (0,1)$. The drift consists of a dissipative Lipschitz term and a singular term of regularity $γ>1-1/(2H)$ in Besov-Hölder scales. We establish well-posedness and, through a Markovian enhancement, existence of an invariant measure. If the singular contribution is sufficiently small, we prove exponential contraction of solutions, and thereby, uniqueness of the invariant measure. Our methods rely on uniform pathwise estimates which utilise together the dissipativity of the drift and the regularisation effect of the noise.</p></details> |  |
| **[Networked Communication for Decentralised Cooperative Agents in Mean-Field Control](https://arxiv.org/abs/2503.09400v2)** | 2025-12-22 | <details><summary>Show</summary><p>The mean-field framework has been used to find approximate solutions to problems involving very large populations of symmetric, anonymous agents, which may be intractable by other methods. The cooperative mean-field control (MFC) problem has received less attention than the non-cooperative mean-field game (MFG), despite the former potentially being more useful as a tool for engineering large-scale collective behaviours. Decentralised communication algorithms have recently been introduced to MFGs, giving benefits to learning speed and robustness. Inspired by this, we introduce networked communication to MFC - where populations arguably have broader incentive to communicate - and in particular to the setting where decentralised agents learn online from a single, non-episodic run of the empirical system. We adapt recent MFG algorithms to this new setting, as well as contributing a novel sub-routine allowing networked agents to estimate the global average reward from their local neighbourhood. Previous theoretical analysis of decentralised communication in MFGs does not extend trivially to MFC. We therefore contribute new theory proving that in MFC the networked communication scheme allows agents to increase social welfare faster than under both of the two typical alternative architectures, namely independent and centralised learning. We provide experiments that support this new result across different classes of cooperative game, and also give numerous ablation studies and additional experiments concerning numbers of communication round and robustness to communication failures.</p></details> |  |
| **[Networked Communication for Mean-Field Games with Function Approximation and Empirical Mean-Field Estimation](https://arxiv.org/abs/2408.11607v3)** | 2025-12-22 | <details><summary>Show</summary><p>Recent algorithms allow decentralised agents, possibly connected via a communication network, to learn equilibria in mean-field games from a non-episodic run of the empirical system. However, these algorithms are for tabular settings: this computationally limits the size of agents' observation space, meaning the algorithms cannot handle anything but small state spaces, nor generalise beyond policies depending only on the agent's local state to so-called 'population-dependent' policies. We address this limitation by introducing function approximation to the existing setting, drawing on the Munchausen Online Mirror Descent method that has previously been employed only in finite-horizon, episodic, centralised settings. While this permits us to include the mean field in the observation for players' policies, it is unrealistic to assume decentralised agents have access to this global information: we therefore also provide new algorithms allowing agents to locally estimate the global empirical distribution, and to improve this estimate via inter-agent communication. We prove theoretically that exchanging policy information helps networked agents outperform both independent and even centralised agents in function-approximation settings. Our experiments demonstrate this happening empirically, and show that the communication network allows decentralised agents to estimate the mean field for population-dependent policies.</p></details> |  |
| **[Networked Communication for Decentralised Agents in Mean-Field Games](https://arxiv.org/abs/2306.02766v6)** | 2025-12-22 | <details><summary>Show</summary><p>Methods like multi-agent reinforcement learning struggle to scale with growing population size. Mean-field games (MFGs) are a game-theoretic approach that can circumvent this by finding a solution for an abstract infinite population, which can then be used as an approximate solution for the $N$-agent problem. However, classical mean-field algorithms usually only work under restrictive conditions. We take steps to address this by introducing networked communication to MFGs, in particular to settings that use a single, non-episodic run of $N$ decentralised agents to simulate the infinite population, as is likely to be most reasonable in real-world deployments. We prove that our architecture's sample guarantees lie between those of earlier theoretical algorithms for the centralised- and independent-learning architectures, varying dependent on network structure and the number of communication rounds. However, the sample guarantees of the three theoretical algorithms do not actually result in practical convergence times. We thus contribute practical enhancements to all three algorithms allowing us to present their first empirical demonstrations. We then show that in practical settings where the theoretical hyperparameters are not observed, giving fewer loops but poorer estimation of the Q-function, our communication scheme still respects the earlier theoretical analysis: it considerably accelerates learning over the independent case, which hardly seems to learn at all, and often performs similarly to the centralised case, while removing the restrictive assumption of the latter. We provide ablations and additional studies showing that our networked approach also has advantages over both alternatives in terms of robustness to update failures and to changes in population size.</p></details> |  |
| **[DeepGESI: A Non-Intrusive Objective Evaluation Model for Predicting Speech Intelligibility in Hearing-Impaired Listeners](https://arxiv.org/abs/2512.19374v1)** | 2025-12-22 | <details><summary>Show</summary><p>Speech intelligibility assessment is essential for many speech-related applications. However, most objective intelligibility metrics are intrusive, as they require clean reference speech in addition to the degraded or processed signal for evaluation. Furthermore, existing metrics such as STOI are primarily designed for normal hearing listeners, and their predictive accuracy for hearing impaired speech intelligibility remains limited. On the other hand, the GESI (Gammachirp Envelope Similarity Index) can be used to estimate intelligibility for hearing-impaired listeners, but it is also intrusive, as it depends on reference signals. This requirement limits its applicability in real-world scenarios. To overcome this limitation, this study proposes DeepGESI, a non-intrusive deep learning-based model capable of accurately and efficiently predicting the speech intelligibility of hearing-impaired listeners without requiring any clean reference speech. Experimental results demonstrate that, under the test conditions of the 2nd Clarity Prediction Challenge(CPC2) dataset, the GESI scores predicted by DeepGESI exhibit a strong correlation with the actual GESI scores. In addition, the proposed model achieves a substantially faster prediction speed compared to conventional methods.</p></details> |  |
| **[ForeSpeed: A real-world video dataset of CCTV cameras with different settings for vehicle speed estimation](https://arxiv.org/abs/2512.19364v1)** | 2025-12-22 | <details><summary>Show</summary><p>The need to estimate the speed of road vehicles has become increasingly important in the field of video forensics, particularly with the widespread deployment of CCTV cameras worldwide. Despite the development of various approaches, the accuracy of forensic speed estimation from real-world footage remains highly dependent on several factors, including camera specifications, acquisition methods, spatial and temporal resolution, compression methods, and scene perspective, which can significantly influence performance. In this paper, we introduce ForeSpeed, a comprehensive dataset designed to support the evaluation of speed estimation techniques in real-world scenarios using CCTV footage. The dataset includes recordings of a vehicle traveling at known speeds, captured by three digital and three analog cameras from two distinct perspectives. Real-world road metrics are provided to enable the restoration of the scene geometry. Videos were stored with multiple compression factors and settings, to simulate real world scenarios in which export procedures are not always performed according to forensic standards. Overall, ForeSpeed, includes a collection of 322 videos. As a case study, we employed the ForeSpeed dataset to benchmark a speed estimation algorithm available in a commercial product (Amped FIVE). Results demonstrate that while the method reliably estimates average speed across various conditions, its uncertainty range significantly increases when the scene involves strong perspective distortion. The ForeSpeed dataset is publicly available to the forensic community, with the aim of facilitating the evaluation of current methodologies and inspiring the development of new, robust solutions tailored to collision investigation and forensic incident analysis.</p></details> |  |
| **[GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting](https://arxiv.org/abs/2412.14579v2)** | 2025-12-22 | <details><summary>Show</summary><p>Weakly-supervised 3D occupancy perception is crucial for vision-based autonomous driving in outdoor environments. Previous methods based on NeRF often face a challenge in balancing the number of samples used. Too many samples can decrease efficiency, while too few can compromise accuracy, leading to variations in the mean Intersection over Union (mIoU) by 5-10 points. Furthermore, even with surrounding-view image inputs, only a single image is rendered from each viewpoint at any given moment. This limitation leads to duplicated predictions, which significantly impacts the practicality of the approach. However, this issue has largely been overlooked in existing research. To address this, we propose GSRender, which uses 3D Gaussian Splatting for weakly-supervised occupancy estimation, simplifying the sampling process. Additionally, we introduce the Ray Compensation module, which reduces duplicated predictions by compensating for features from adjacent frames. Finally, we redesign the dynamic loss to remove the influence of dynamic objects from adjacent frames. Extensive experiments show that our approach achieves SOTA results in RayIoU (+6.0), while also narrowing the gap with 3D- supervised methods. This work lays a solid foundation for weakly-supervised occupancy perception. The code is available at https://github.com/Jasper-sudo-Sun/GSRender.</p></details> |  |
| **[NA-DiD: Extending Difference-in-Differences with Capabilities](https://arxiv.org/abs/2507.12690v2)** | 2025-12-22 | <details><summary>Show</summary><p>This paper introduces the Non-Additive Difference-in-Differences (NA-DiD) framework, which extends classical DiD by incorporating non-additive measures the Choquet integral for effect aggregation. It serves as a novel econometric tool for impact evaluation, particularly in settings with non-additive treatment effects. First, we introduce the integral representation of the classial DiD model, and then extend it to non-additive measures, therefore deriving the formulae for NA-DiD estimation. Then, we give its theoretical properties. Applying NA-DiD to a simulated hospital hygiene intervention, we find that classical DiD can overestimate treatment effects, f.e. failing to account for compliance erosion. In contrast, NA-DiD provides a more accurate estimate by incorporating non-linear aggregation. The Julia implementation of the techniques used and introduced in this article is provided in the appendices.</p></details> | <details><summary>I hav...</summary><p>I have received a reviews from a journal, and after reading them I came to the conclusion that this research does not meet novelty threshold for a paper I would like to put in public. I missed some references, which are more relevant to the problem I try to solve and offer a better solution</p></details> |
| **[TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking](https://arxiv.org/abs/2512.02789v2)** | 2025-12-22 | <details><summary>Show</summary><p>The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.</p></details> |  |
| **[A hybrid-Hill estimator enabled by heavy-tailed block maxima](https://arxiv.org/abs/2512.19338v1)** | 2025-12-22 | <details><summary>Show</summary><p>When analysing extreme values, two alternative statistical approaches have historically been held in contention: the seminal block maxima method (or annual maxima method, spurred by hydrological applications) and the peaks-over-threshold. Clamoured amongst statisticians as wasteful of potentially informative data, the block maxima method gradually fell into disfavour whilst peaks-over-threshold-based methodologies were ushered to the centre stage of extreme value statistics. This paper proposes a hybrid method which reconciles these two hitherto disconnected approaches. Appealing in its simplicity, our main result introduces a new universal limiting characterisation of extremes that eschews the customary requirement of a sufficiently large block size for the plausible block maxima-fit to an extreme value distribution. We advocate that inference should be drawn solely on larger block maxima, from which practice the mainstream peaks-over-threshold methodology coalesces. The asymptotic properties of the promised hybrid-Hill estimator herald more than its efficiency, but rather that a fully-fledged unified semi-parametric stream of statistics for extreme values is viable. A finite sample simulation study demonstrates that a reduced-bias off-shoot of the hybrid-Hill estimator fares exceptionally well against the incumbent maximum likelihood estimation that relies on a numerical fit to the entire sample of block maxima.</p></details> | 31 pages, 5 figures |
| **[Orthogonal Approximate Message Passing with Optimal Spectral Initializations for Rectangular Spiked Matrix Models](https://arxiv.org/abs/2512.19334v1)** | 2025-12-22 | <details><summary>Show</summary><p>We propose an orthogonal approximate message passing (OAMP) algorithm for signal estimation in the rectangular spiked matrix model with general rotationally invariant (RI) noise. We establish a rigorous state evolution that precisely characterizes the algorithm's high-dimensional dynamics and enables the construction of iteration-wise optimal denoisers. Within this framework, we accommodate spectral initializations under minimal assumptions on the empirical noise spectrum. In the rectangular setting, where a single rank-one component typically generates multiple informative outliers, we further propose a procedure for combining these outliers under mild non-Gaussian signal assumptions. For general RI noise models, the predicted performance of the proposed optimal OAMP algorithm agrees with replica-symmetric predictions for the associated Bayes-optimal estimator, and we conjecture that it is statistically optimal within a broad class of iterative estimation methods.</p></details> |  |
| **[The asymptotic distribution of the likelihood ratio test statistic in two-peak discovery experiments](https://arxiv.org/abs/2512.19333v1)** | 2025-12-22 | <details><summary>Show</summary><p>Likelihood ratio tests are widely used in high-energy physics, where the test statistic is usually assumed to follow a chi-squared distribution with a number of degrees of freedom specified by Wilks' theorem. This assumption breaks down when parameters such as signal or coupling strengths are restricted to be non-negative and their values under the null hypothesis lie on the boundary of the parameter space. Based on a recent clarification concerning the correct asymptotic distribution of the likelihood ratio test statistic for cases where two of the parameters are on the boundary, we revisit the the question of significance estimation for two-peak signal-plus-background counting experiments. In the high-energy physics literature, such experiments are commonly analyzed using Wilks' chi-squared distribution or the one-parameter Chernoff limit. We demonstrate that these approaches can lead to strongly miscalibrated significances, and that the test statistic distribution is instead well described by a chi-squared mixture with weights determined by the Fisher information matrix. Our results highlight the need for boundary-aware asymptotics in the analysis of two-peak counting experiments.</p></details> | 25 pages, 6 figures |
| **[High dimensional matrix estimation through elliptical factor models](https://arxiv.org/abs/2512.19325v1)** | 2025-12-22 | <details><summary>Show</summary><p>Elliptical factor models play a central role in modern high-dimensional data analysis, particularly due to their ability to capture heavy-tailed and heterogeneous dependence structures. Within this framework, Tyler's M-estimator (Tyler, 1987a) enjoys several optimality properties and robustness advantages. In this paper, we develop high-dimensional scatter matrix, covariance matrix and precision matrix estimators grounded in Tyler's M-estimation. We first adapt the Principal Orthogonal complEment Thresholding (POET) framework (Fan et al., 2013) by incorporating the spatial-sign covariance matrix as an effective initial estimator. Building on this idea, we further propose a direct extension of POET tailored for Tyler's M-estimation, referred to as the POET-TME method. We establish the consistency rates for the resulting estimators under elliptical factor models. Comprehensive simulation studies and a real data application illustrate the superior performance of POET-TME, especially in the presence of heavy-tailed distributions, demonstrating the practical value of our methodological contributions.</p></details> |  |
| **[Joint parameter estimation and multidimensional reconciliation for continuous-variable quantum key distribution](https://arxiv.org/abs/2508.05558v2)** | 2025-12-22 | <details><summary>Show</summary><p>Accurate quantum channel parameter estimation is essential for effective information reconciliation in continuous-variable quantum key distribution (CV-QKD). However, conventional maximum likelihood (ML) estimators rely on a large amount of discarded data (or pilot symbols), leading to a significant loss in symbol efficiency. Moreover, the separation between the estimation and reconciliation phases can introduce error propagation. In this paper, we propose a novel joint message-passing scheme that unifies channel parameter estimation and information reconciliation within a Bayesian framework. By leveraging the expectation-maximization (EM) algorithm, the proposed method simultaneously estimates unknown parameters during decoding, eliminating the need for separate ML estimation. Furthermore, we introduce a hybrid multidimensional rotation scheme that removes the requirement for norm feedback, significantly reducing classical channel overhead. To the best of our knowledge, this is the first work to unify multidimensional reconciliation and channel parameter estimation in CV-QKD, providing a practical solution for high-efficiency reconciliation with minimal pilots.</p></details> | 11 pages, 6 figures |
| **[Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market](https://arxiv.org/abs/2503.04521v2)** | 2025-12-22 | <details><summary>Show</summary><p>The convergence of edge computing and Artificial Intelligence (AI) gives rise to Edge-AI, which enables the deployment of real-time AI applications at the network edge. A key research challenge in Edge-AI is edge inference acceleration, which aims to realize low-latency high-accuracy Deep Neural Network (DNN) inference by offloading partitioned inference tasks from end devices to edge servers. However, existing research has yet to adopt a practical Edge-AI market perspective, which would explore the personalized inference needs of AI users (e.g., inference accuracy, latency, and task complexity), the revenue incentives for AI service providers that offer edge inference services, and multi-stakeholder governance within a market-oriented context. To bridge this gap, we propose an Auction-based Edge Inference Pricing Mechanism (AERIA) for revenue maximization to tackle the multi-dimensional optimization problem of DNN model partition, edge inference pricing, and resource allocation. We develop a multi-exit device-edge synergistic inference scheme for on-demand DNN inference acceleration, and theoretically analyze the auction dynamics amongst the AI service providers, AI users and edge infrastructure provider. Owing to the strategic mechanism design via randomized consensus estimate and cost sharing techniques, the Edge-AI market attains several desirable properties. These include competitiveness in revenue maximization, incentive compatibility, and envy-freeness, which are crucial to maintain the effectiveness, truthfulness, and fairness in auction outcomes. Extensive simulations based on four representative DNN inference workloads demonstrate that AERIA significantly outperforms several state-of-the-art approaches in revenue maximization. This validates the efficacy of AERIA for on-demand DNN inference in the Edge-AI market.</p></details> | <details><summary>Accep...</summary><p>Accepted for publication in IEEE Transactions on Mobile Computing. Index Terms: Edge-AI, DNN Inference Offloading, Resource Management, Dynamic Pricing, Auction Mechanism</p></details> |
| **[Local smoothing and maximal estimates for average over surfaces of codimension 2 in $\mathbb R^4$](https://arxiv.org/abs/2507.22695v2)** | 2025-12-22 | <details><summary>Show</summary><p>In this paper, we obtain local smoothing estimates for the averages over nondegenerate surfaces of codimension $2$ in $\mathbb R^4$. We make use of multilinear restriction estimates and decoupling inequalities for a hypersurface in $\mathbb R^5$, a conical extension of a two-dimensional nondegenerate surface along two flat directions. We also establish sharp $L^p$--$L^q$ estimates for maximal averages over nondegenerate surfaces of half the ambient dimension in $\mathbb R^{2n}$ for even $n \ge 2$.</p></details> | <details><summary>32 pa...</summary><p>32 pages, The title was changed, and the introduction was expanded</p></details> |
| **[Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context](https://arxiv.org/abs/2512.19283v1)** | 2025-12-22 | <details><summary>Show</summary><p>Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.</p></details> | <details><summary>Proje...</summary><p>Project Page: https://kyungwoncho.github.io/HaMoS/</p></details> |
| **[Spearman's rho for zero-inflated count data: formulation and attainable bounds](https://arxiv.org/abs/2503.13148v2)** | 2025-12-22 | <details><summary>Show</summary><p>We propose an alternative formulation of Spearman's rho for zero-inflated count data. The formulation yields an estimator with explicitly attainable bounds, facilitating interpretation in settings where the standard range [-1,1] is no longer informative.</p></details> |  |
| **[On the large time behavior of the 2D inhomogeneous incompressible viscous flows](https://arxiv.org/abs/2512.19281v1)** | 2025-12-22 | <details><summary>Show</summary><p>This paper studies the two-dimensional inhomogeneous Navier--Stokes equations governing stratified flows in a bounded domain under a gravitational potential \(f\). Our main results are as follows. First, we provide a rigorous characterization of steady states, proving that under the Dirichlet condition \(\mathbf{u}|_{\partial Ω} = \mathbf{0}\), all admissible equilibria are hydrostatic and satisfy \(\nabla p_s = -ρ_s \nabla f\). Second, through a perturbative analysis around arbitrary hydrostatic profiles, we show that despite possible transient growth induced by the Rayleigh--Taylor mechanism, the system always relaxes to a hydrostatic equilibrium. Third, we identify a necessary and sufficient condition on the initial density perturbation for convergence to a linear hydrostatic density profile of the form \(ρ_s = -γf + β\), with \(γ> 0\) and \(β> 0\). Finally, we establish improved regularity estimates for strong solutions corresponding to initial data in the Sobolev space \(H^3(Ω)\).</p></details> |  |
| **[Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals](https://arxiv.org/abs/2512.19280v1)** | 2025-12-22 | <details><summary>Show</summary><p>Axial piston pumps are crucial components in fluid power systems, where reliable fault diagnosis is essential for ensuring operational safety and efficiency. Traditional data-driven methods require extensive labeled fault data, which is often impractical to obtain, while model-based approaches suffer from parameter uncertainties. This paper proposes a digital twin (DT)-driven zero-shot fault diagnosis framework utilizing fluid-borne noise (FBN) signals. The framework calibrates a high-fidelity DT model using only healthy-state data, generates synthetic fault signals for training deep learning classifiers, and employs a physics-informed neural network (PINN) as a virtual sensor for flow ripple estimation. Gradient-weighted class activation mapping (Grad-CAM) is integrated to visualize the decision-making process of neural networks, revealing that large kernels matching the subsequence length in time-domain inputs and small kernels in time-frequency domain inputs enable higher diagnostic accuracy by focusing on physically meaningful features. Experimental validations demonstrate that training on signals from the calibrated DT model yields diagnostic accuracies exceeding 95\% on real-world benchmarks, while uncalibrated models result in significantly lower performance, highlighting the framework's effectiveness in data-scarce scenarios.</p></details> |  |
| **[Scale-Invariant Robust Estimation of High-Dimensional Kronecker-Structured Matrices](https://arxiv.org/abs/2512.19273v1)** | 2025-12-22 | <details><summary>Show</summary><p>High-dimensional Kronecker-structured estimation faces a conflict between non-convex scaling ambiguities and statistical robustness. The arbitrary factor scaling distorts gradient magnitudes, rendering standard fixed-threshold robust methods ineffective. We resolve this via Scaled Robust Gradient Descent (SRGD), which stabilizes optimization by de-scaling gradients before truncation. To further enforce interpretability, we introduce Scaled Hard Thresholding (SHT) for invariant variable selection. A two-step estimation procedure, built upon robust initialization and SRGD--SHT iterative updates, is proposed for canonical matrix problems, such as trace regression, matrix GLMs, and bilinear models. The convergence rates are established for heavy-tailed predictors and noise, identifying a phase transition where optimal convergence rates recover under finite noise variance and degrade optimally for heavier tails. Experiments on simulated data and two real-world applications confirm superior robustness and efficiency of the proposed procedure.</p></details> | 85 pages, 11 figues |
| **[Precise control of high-frequency ultrasounds in thin crystals for the development of tunable narrowband and directional gamma-ray sources](https://arxiv.org/abs/2512.19268v1)** | 2025-12-22 | <details><summary>Show</summary><p>This work presents a complete methodology for the precise characterization of the acoustic field inside crystal-based devices driven by high-frequency ultrasounds towards the generation of tunable narrowband and directional gamma radiation via undulation of ultra-relativistic charged particles. Such gamma-ray sources have long been anticipated by the scientific community, as they promise new powerful tools for the study of high-energy physical phenomena and the development of novel nuclear technologies. In such devices, a piezoelectric transducer induces tens of MHz harmonic waves inside a silicon monocrystal. Ultra-relativistic charged particles traversing the crystal get trapped within the channels formed by the extremely strong electric fields of the acoustically modulated lattice planes, undergoing undulation and emitting gamma radiation. Precise characterization of the acoustic field in the crystal is crucial for the determination of the expected characteristics of the secondarily generated gamma rays. For this purpose, fast laser refraction imaging is used here to image the acoustic waves by exploiting the spatial redistribution of a laser beam optical intensity caused by the acoustic field. A dedicated computational model is developed for the estimation of the spatial distribution of the pressure and lattice deformation inside the crystal. This methodology provides a framework for future novel gamma-ray sources in high-energy facilities.</p></details> |  |
| **[DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method](https://arxiv.org/abs/2508.17054v3)** | 2025-12-22 | <details><summary>Show</summary><p>Previous dominant methods for scene flow estimation focus mainly on input from two consecutive frames, neglecting valuable information in the temporal domain. While recent trends shift towards multi-frame reasoning, they suffer from rapidly escalating computational costs as the number of frames grows. To leverage temporal information more efficiently, we propose DeltaFlow ($Δ$Flow), a lightweight 3D framework that captures motion cues via a $Δ$ scheme, extracting temporal features with minimal computational cost, regardless of the number of frames. Additionally, scene flow estimation faces challenges such as imbalanced object class distributions and motion inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to enhance learning across underrepresented classes and an Instance Consistency Loss to enforce coherent object motion, improving flow accuracy. Extensive evaluations on the Argoverse 2, Waymo and nuScenes datasets show that $Δ$Flow achieves state-of-the-art performance with up to 22% lower error and $2\times$ faster inference compared to the next-best multi-frame supervised method, while also demonstrating a strong cross-domain generalization ability. The code is open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model weights.</p></details> | <details><summary>NeurI...</summary><p>NeurIPS 2025 Spotlight, 18 pages (10 main pages + 8 supp materail), 11 figures, code at https://github.com/Kin-Zhang/DeltaFlow</p></details> |
| **[Pointwise bounds on Dirichlet Green's functions for a singular drift term](https://arxiv.org/abs/2511.12741v2)** | 2025-12-22 | <details><summary>Show</summary><p>We introduce a technique to obtain pointwise upper and lower bounds for the Green's function of elliptic operators whose principal part is the Laplacian and that include a drift term diverging near the boundary like a power of the inverse distance with exponent less than 1, in the unit ball B(0,1) \subset \mathbb{R}^n, n \ge 3. The constants in the upper estimates are uniform in B(0,r) for each r < 1, with explicit dependence on r. The drift here belongs to C^{1,α}_{\mathrm{loc}} and may, more generally, be majorized by a function radially integrable up to the boundary. These appear to be the first such estimates for non-coercive drifts and remain new even for smooth drifts, suggesting extensions to singular potentials and other settings where energy methods fail.</p></details> | Fixed several typos |
| **[Estimation of Population Linear Spectral Statistics by Marchenko--Pastur Inversion](https://arxiv.org/abs/2504.03390v4)** | 2025-12-22 | <details><summary>Show</summary><p>A new method of estimating population linear spectral statistics from high-dimensional data is introduced. When the dimension $d$ grows with the sample size $n$ such that $\frac{d}{n} \to c>0$, the proposed method is the first with proven convergence rate of $\mathcal{O}(n^{\varepsilon - 1})$ for any $\varepsilon > 0$ in a general nonparametric setting. For Gaussian data, a CLT for the estimation error with normalization factor $n$ is shown.</p></details> | 67 pages, 10 figures |
| **[Identification of Separable OTUs for Multinomial Classification in Compositional Data Analysis](https://arxiv.org/abs/2511.02509v2)** | 2025-12-22 | <details><summary>Show</summary><p>High-throughput sequencing has transformed microbiome research, but it also produces inherently compositional data that challenge standard statistical and machine learning methods. In this work, we propose a multinomial classification framework for compositional microbiome data based on penalized log-ratio regression and pairwise separability screening. The method quantifies the discriminative ability of each OTU through the area under the receiver operating characteristic curve ($AUC$) for all pairwise log-ratios and aggregates these values into a global separability index $S_k$, yielding interpretable rankings of taxa together with confidence intervals. We illustrate the approach by reanalyzing the Baxter colorectal adenoma dataset and comparing our results with Greenacre's ordination-based analysis using Correspondence Analysis and Canonical Correspondence Analysis. Our models consistently recover a core subset of taxa previously identified as discriminant, thereby corroborating Greenacre's main findings, while also revealing additional OTUs that become important once demographic covariates are taken into account. In particular, adjustment for age, gender, and diabetes medication improves the precision of the separation index and highlights new, potentially relevant taxa, suggesting that part of the original signal may have been influenced by confounding. Overall, the integration of log-ratio modeling, covariate adjustment, and uncertainty estimation provides a robust and interpretable framework for OTU selection in compositional microbiome data. The proposed method complements existing ordination-based approaches by adding a probabilistic and inferential perspective, strengthening the identification of biologically meaningful microbial signatures.</p></details> |  |
| **[Vision-Aided Relative State Estimation for Approach and Landing on a Moving Platform with Inertial Measurements](https://arxiv.org/abs/2512.19245v1)** | 2025-12-22 | <details><summary>Show</summary><p>This paper tackles the problem of estimating the relative position, orientation, and velocity between a UAV and a planar platform undergoing arbitrary 3D motion during approach and landing. The estimation relies on measurements from Inertial Measurement Units (IMUs) mounted on both systems, assuming there is a suitable communication channel to exchange data, together with visual information provided by an onboard monocular camera, from which the bearing (line-of-sight direction) to the platform's center and the normal vector of its planar surface are extracted. We propose a cascade observer with a complementary filter on SO(3) to reconstruct the relative attitude, followed by a linear Riccati observer for relative position and velocity estimation. Convergence of both observers is established under persistently exciting conditions, and the cascade is shown to be almost globally asymptotically and locally exponentially stable. We further extend the design to the case where the platform's rotation is restricted to its normal axis and show that its measured linear acceleration can be exploited to recover the remaining unobservable rotation angle. A sufficient condition to ensure local exponential convergence in this setting is provided. The performance of the proposed observers is validated through extensive simulations.</p></details> | <details><summary>13 pa...</summary><p>13 pages, 4 figures. Submitted to IFAC World Congress 2026</p></details> |
| **[A Reverse Reachable Set Based Approach for Motif Oriented Profit maximization in Social Networks](https://arxiv.org/abs/2512.19237v1)** | 2025-12-22 | <details><summary>Show</summary><p>Profit Maximization is one of the key objectives for social media marketing, where the task is to choose a limited number of highly influential nodes such that their initial activation leads to maximum profit. In this paper, we introduce a variant of the Profit Maximization Problem where we consider that instead of nodes, benefits are assigned to some of the motifs of the graph, and these benefit values can be earned once a given threshold count of nodes from the motifs is influenced. The goal here is to choose a limited number of nodes for initial activation called seed nodes such that the motif-oriented profit gets maximized. Formally, we call our problem the Motif Oriented Profit Maximization Problem. We show that the problem is NP-hard to solve optimally. We propose a Reverse Reachable Set-based framework to solve our problem. The proposed methodology broadly divides into three steps: KPT Estimation and RR Set generation, Seed Set Selection, and Motif Oriented Profit Estimation. The proposed methodology has been analyzed to understand its time and space requirements. It has been implemented with real-world social network datasets, and the results are reported. We observe that the seed set selected by the proposed solution approaches leads to more profit compared to the seed sets selected by the existing methods. The whole implementation and data are available at: https://github.com/PoonamSharma-PY/MotifProfit.</p></details> | <details><summary>This ...</summary><p>This paper is accepted in the prestigious 22nd International Conference on Distributed Computing and Intelligent Technology - ICDCIT 2026</p></details> |
| **[Multi-modal On-Device Learning for Monocular Depth Estimation on Ultra-low-power MCUs](https://arxiv.org/abs/2512.00086v2)** | 2025-12-22 | <details><summary>Show</summary><p>Monocular depth estimation (MDE) plays a crucial role in enabling spatially-aware applications in Ultra-low-power (ULP) Internet-of-Things (IoT) platforms. However, the limited number of parameters of Deep Neural Networks for the MDE task, designed for IoT nodes, results in severe accuracy drops when the sensor data observed in the field shifts significantly from the training dataset. To address this domain shift problem, we present a multi-modal On-Device Learning (ODL) technique, deployed on an IoT device integrating a Greenwaves GAP9 MicroController Unit (MCU), a 80 mW monocular camera and a 8 x 8 pixel depth sensor, consuming $\approx$300mW. In its normal operation, this setup feeds a tiny 107 k-parameter $μ$PyD-Net model with monocular images for inference. The depth sensor, usually deactivated to minimize energy consumption, is only activated alongside the camera to collect pseudo-labels when the system is placed in a new environment. Then, the fine-tuning task is performed entirely on the MCU, using the new data. To optimize our backpropagation-based on-device training, we introduce a novel memory-driven sparse update scheme, which minimizes the fine-tuning memory to 1.2 MB, 2.2x less than a full update, while preserving accuracy (i.e., only 2% and 1.5% drops on the KITTI and NYUv2 datasets). Our in-field tests demonstrate, for the first time, that ODL for MDE can be performed in 17.8 minutes on the IoT node, reducing the root mean squared error from 4.9 to 0.6m with only 3 k self-labeled samples, collected in a real-life deployment scenario.</p></details> | <details><summary>14 pa...</summary><p>14 pages, 9 figures, 3 tables. Associated open-source release available at: https://github.com/idsia-robotics/ultralow-power-monocular-depth-ondevice-learning</p></details> |

